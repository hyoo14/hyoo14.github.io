---
layout: post
title:  "[2019]Towards Deep Learning Models Resistant to Adversarial Attacks"  
date:   2024-04-19 17:21:29 -0400
categories: study
---

{% highlight ruby %}


한줄 요약: 

짧은 요약(Abstract) :    
* 최근 연구에 따르면 심층 신경망은 적대적 예시, 즉 자연 데이터와 거의 구별할 수 없지만 네트워크에 의해 잘못 분류되는 입력에 취약하다고 합니다
* 실제로 최신 연구 결과에 따르면 적대적 공격의 존재는 심층 학습 모델의 본질적인 약점일 수 있다고 제안됩니다
* 이 문제를 해결하기 위해 저자들은 견고한 최적화의 관점을 통해 신경망의 적대적 견고성을 연구합니다
* 이 접근법은 이 주제에 대한 이전 연구를 광범위하게 통합하는 관점을 제공합니다
* 그 원칙적인 성격은 신뢰할 수 있고 어떤 의미에서 보편적인 신경망 훈련 및 공격 방법을 식별할 수 있도록 해줍니다
* 특히, 저자들은 모든 적을 막을 수 있는 구체적인 보안 보장을 명시합니다
* 이러한 방법들은 광범위한 적대적 공격에 대해 상당히 개선된 저항력을 가진 네트워크를 훈련할 수 있게 합니다
* 또한, 1차 적에 대한 보안을 자연스럽고 넓은 보안 보장으로 제안합니다
* 저자들은 이러한 잘 정의된 적의 클래스에 대한 견고성이 완전히 저항할 수 있는 심층 학습 모델로 가는 중요한 디딤돌임을 믿습니다

Useful sentences :  
*   


{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1_IubWvPAcy5m153Sg7ZClbzietjY_IYy?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 
<br/>
# 1 Introduction  
* 컴퓨터 비전과 자연어 처리 분야에서의 최근 돌파구는 보안이 중요한 시스템 중심에 분류기를 가져오고 있습니다  
* 특히 자율 주행차, 얼굴 인식 및 멀웨어 탐지와 같은 중요한 예가 있습니다  
* 이러한 발전은 기계 학습의 보안 측면을 점점 중요하게 만듭니다  
* 특히 적대적으로 선택된 입력에 대한 저항력은 중요한 설계 목표가 되고 있습니다  
* 훈련된 모델은 일반적으로 양성 입력을 분류하는 데 매우 효과적이지만 최근 연구는 종종 적* 이 입력을 조작하여 모델이 잘못된 출력을 내도록 만들 수 있음을 보여줍니다  
* 이 현상은 특히 심층 신경망의 맥락에서 주목받고 있으며 이 주제에 대한 연구가 빠르게 증가하고 있습니다  
* 컴퓨터 비전은 특히 두드러진 도전을 제시합니다  
* 매우 작은 입력 이미지의 변화가 최신 상태의 신경망을 속일 수 있습니다  
* 이는 친숙한 예가 제대로 분류되었음에도 불구하고 발생하며, 변화는 인간에게 눈에 띄지 않습니다  
* 보안 함의를 제외하고, 이 현상은 또한 저자들의 현재 모델이 강건한 방식으로 기본 개념을 학습하지 않고 있음을 보여줍니다  
* 이 모든 발견은 근본적인 질문을 제기합니다    
* 저자들은 어떻게 심층 신경망을 훈련시켜 적대적 입력에 견고하게 만들 수 있을까요?  

<br/>
# 2 An Optimization View on Adversarial Robustness
* 저자들의 토론은 대부분 적대적 견고성에 대한 최적화 관점을 중심으로 전개될 것입니다  
* 이 관점은 저자들이 연구하고자 하는 현상을 정확하게 포착할 뿐만 아니라 저자들의 조사를 안내할 것입니다  
* 이를 위해 저자들은 표준 분류 작업을 고려하며, 이 작업에서는 예제 x와 해당 레이블 y가 포함된 쌍에 대한 기본 데이터 분포 D를 가정합니다  
* 저자들은 또한 적절한 손실 함수 L(θ, x, y)가 주어졌다고 가정하는데, 예를 들어 신경망에는 교차 엔트로피 손실이 사용됩니다  
* 통상적으로 θ ∈ Rp는 모델 매개변수의 집합입니다  
* 저자들의 목표는 데이터 D에서 샘플링된 E(x, y)의 리스크 L(x, y, θ)를 최소화하는 모델 매개변수 θ를 찾는 것입니다  

* 실증적 리스크 최소화(ERM)는 작은 인구 리스크를 가진 분류기를 찾는 데 매우 성공적인 방법으로 입증되었습니다  
* 불행히도 ERM은 종종 적대적으로 제작된 예시에 견고한 모델을 생성하지 못합니다  
* 공식적으로, 적은 적대적 예시 xadv를 찾는 효율적인 알고리즘(“적”)이 있는데, 이는 xadv가 x에 매우 가깝지만 모델이 xadv를 잘못된 클래스 c2에 속한다고 잘못 분류합니다  
* 저자들은 적대적 공격에 견고한 모델을 안정적으로 훈련시키기 위해 ERM 패러다임을 적절히 보완할 필요가 있습니다  
* 특정 공격에 대한 견고성을 직접 향상시키는 방법에 의존하는 대신, 저자들은 적대적으로 견고한 모델이 충족해야 하는 구체적인 보장을 먼저 제안합니다  
* 그런 다음 저자들은 이 보장을 달성하기 위해 훈련 방법을 조정합니다  

* 첫 번째 단계로서 공격 모델을 명시하고자 합니다  
* 즉, 저자들의 모델이 견딜 수 있어야 하는 공격의 정확한 정의입니다  
* 각 데이터 포인트 x에 대해, 저자들은 적의 조작 능력을 형식화하는 허용된 변조의 집합 S ⊆ Rd를 도입합니다  
* 예를 들어 이미지 분류에서 저자들은 S를 선택하여 이미지 간의 지각적 유사성을 포착합니다  
* 최근에는 x 주변의 `∞-공이 적대적 변조에 대한 자연스러운 개념으로 연구되었습니다  
* 이 논문에서 저자들은 `∞-제한 공격에 대한 견고성에 중점을 둘 것입니다  
* 그러나 저자들은 보다 포괄적인 지각적 유사성 개념이 향후 연구의 중요한 방향임을 언급합니다  

* 다음으로, 저자들은 인구 위험 ED[L]의 정의를 수정하여 위의 적을 포함시킵니다  
D 에서의 샘플을 직접  손실 L에 공급하는 대신, 저자들은 적이 입력을 먼저 변조할 수 있도록 허용합니다  
* 이것은 저자들의 주요 연구 대상인 다음 안장점 문제를 낳습니다  
min θ ρ(θ), 여기서 ρ(θ) = E(x, y)∼D [max δ∈S L(θ, x + δ, y)]      


## 2.1 A Unified View on Attacks and Defenses
* 적대적 예시에 대한 이전 연구는 주로 두 가지 주요 질문에 초점을 맞추었습니다  
* 첫 번째는, 저자들은 어떻게 강력한 적대적 예시를 생성할 수 있을까요, 즉 모델을 속이면서도 작은 변조만을 요구하는 적대적 예시입니다  
* 두 번째는, 저자들은 어떻게 모델을 훈련시켜 적대적 예시가 없거나 적어도 쉽게 찾을 수 없도록 할 수 있을까요  

* 저자들의 안장점 문제(2.1)에 대한 관점은 이러한 두 질문에 대한 답을 제공합니다  
* 공격 측면에서 이전 연구는 고속 기울기 부호 방법(FGSM)과 그 여러 변형을 제안했습니다  
* FGSM은 `∞-제한적인 적을 위한 공격으로, 적대적 예시를 x + ε sgn(∇xL(θ, x, y))로 계산합니다  

* 이 공격은 안장점 공식의 내부 부분을 최대화하는 단순한 단계 스키마로 해석할 수 있습니다  
* 보다 강력한 적은 다중 단계 변형으로, 이는 본질적으로 음의 손실 함수에 대한 투영 기울기 하강(PGD)입니다  

* xt+1 = Πx+S ( xt + α sgn(∇xL(θ, x, y)) )  

* 다른 방법으로는 무작위 변조와 함께 FGSM이 제안되었습니다  
* 분명히 이러한 접근법은 모두 (2.1)의 내부 최대화 문제를 해결하는 구체적인 시도로 볼 수 있습니다  

* 방어 측면에서는 훈련 데이터 세트가 종종 FGSM으로 생성된 적대적 예시로 보강됩니다  
* 이 접근법은 또한 (2.1)에서 내부 최대화 문제를 선형화할 때 직접 따릅니다  
* 간소화된 견고한 최적화 문제를 해결하기 위해, 저자들은 각 훈련 예시를 FGSM으로 변조된 대응물로 대체합니다  
* 보다 정교한 방어 메커니즘으로는 여러 적에 대항하여 훈련하는 것과 같은 더 나은, 더 포괄적인 내부 최대화 문제의 근사치로 볼 수 있습니다

<br/>
# 3 Towards Universally Robust Networks
* 적대적 예시에 대한 현재 연구는 특정 방어 메커니즘에 집중하거나 그러한 방어에 대한 공격을 조사하는 데 주로 초점을 맞추고 있습니다  
* 식 (2.1)의 안장점 문제에서 얻은 작은 적대적 손실은 허용된 공격으로 네트워크를 속일 수 없다는 보장을 제공합니다  
* 적대적 변조가 가능하지 않다는 것은 즉, 손실이 우리 공격 모델에 의해 허용되는 모든 변조에 대해 작기 때문입니다  
* 따라서 이제 저자들은 (2.1)의 좋은 해결책을 얻는 데 집중합니다  

* 불행하게도 안장점 문제에서 제공되는 전체적인 보장은 유용하게 보이지만, 실제로 좋은 해결책을 합리적인 시간 내에 찾을 수 있는지는 분명하지 않습니다  
* (2.1)의 안장점 문제를 해결하는 것은 비볼록 외부 최소화 문제와 비오목 내부 최대화 문제 모두를 다루어야 합니다  
* 저자들의 주요 기여 중 하나는 실제로는 안장점 문제를 해결할 수 있다는 것을 보여주는 것입니다  
* 특히 이제 비오목 내부 문제의 구조를 실험적으로 탐구하면서 이 문제를 논의합니다  
* 저자들은 이 문제의 손실 풍경이 예상외로 다루기 쉬운 국소 최대의 구조를 가지고 있음을 주장합니다  
* 이 구조는 또한 투영 기울기 하강을 “최종” 1차 적으로 가리키게 됩니다  
* 4장과 5장에서는 결과적으로 훈련된 네트워크가 충분히 큰 경우 다양한 공격에 대해 실제로 견고함을 보여줍니다

## 3.1 The Landscape of Adversarial Examples  
* 적대적 예시에 대한 내부 문제는 주어진 네트워크와 데이터 포인트(저자들의 공격 모델의 적용을 받음)에 대한 적대적 예시를 찾는 것입니다  
* 이 문제는 매우 비오목 함수를 최대화해야 하므로 해결하기 어려울 것으로 예상됩니다  
* 실제로 이전 연구에서는 내부 최대화 문제를 선형화하여 이 문제에 접근했습니다  
* 위에서 언급한 바와 같이, 이 선형화 접근 방식은 FGSM과 같은 잘 알려진 방법을 낳습니다  
* FGSM에 대한 훈련은 일부 성공을 보여 주었지만, 최근 연구는 이 한 단계 접근법의 중요한 단점을 강조합니다  
* 약간 더 정교한 적이 여전히 높은 손실을 찾을 수 있습니다  

* 내부 문제에 대한 국소 최대의 풍경을 좀 더 자세히 이해하기 위해 저자들은 MNIST와 CIFAR10 모델에서 다양한 모델에 대한 국소 최대를 조사합니다  
* 주요 도구는 투영 기울기 하강(PGD)인데, 이는 대규모 제약 최적화에 대한 표준 방법입니다  
* 손실 풍경의 큰 부분을 탐색하기 위해 저자들은 평가 세트의 데이터 포인트 주위의 `∞ 공에서 PGD를 여러 번 재시작합니다  

* 놀랍게도 저자들의 실험은 내부 문제가 결국에는 1차 방법의 관점에서 다룰 수 있음을 보여줍니다  
* 많은 국소 최대가 xi + S 내에서 널리 퍼져 있지만, 이들은 매우 집중된 손실 값을 가지고 있습니다  
* 이것은 신경망 훈련이 가능한 이유에 대한 일반적인 믿음을 반영합니다  
* 즉, 모델 매개 변수의 함수로서의 손실은 일반적으로 매우 유사한 값을 가진 많은 국소 최소를 가지고 있습니다  


## 3.2 First-Order Adversaries  
* 저자들의 실험은 PGD로 발견된 국소 최대가 비슷한 손실 값을 가지고 있음을 보여줍니다  
* 이 집중 현상은 1차 적에 대한 견고함이 모든 1차 적에 대한 견고함을 제공한다는 흥미로운 관점을 제시합니다  
* 적이 입력에 대한 손실 함수의 기울기만을 사용하는 한, PGD보다 현저히 나은 국소 최대를 찾지 못할 것이라고 저자들은 추측합니다  
* 5장에서는 PGD 적에 대해 견고하게 훈련된 네트워크가 다양한 다른 공격에도 견고함을 보여주는 더 많은 실험적 증거를 제공합니다  

* 물론 저자들의 탐험으로 인해 몇몇 고립된 최대값이 훨씬 더 큰 함수 값을 가질 수 있는 가능성을 배제할 수는 없습니다  
* 그러나 저자들의 실험은 이러한 더 나은 국소 최대를 1차 방법으로 찾기가 어렵다는 것을 시사합니다  
* 대규모의 무작위 재시작조차도 현저히 다른 손실 값을 가진 기능 값을 찾지 못했습니다  
* 공격 모델에 적의 계산 능력을 포함시키는 것은 현대 암호학의 근간이 되는 다항식으로 제한된 적의 개념을 상기시켜야 합니다  
* 여기서 이 고전적인 공격 모델은 적이 최대 다항식 계산 시간을 요구하는 문제만을 해결할 수 있도록 허용합니다  
* 여기서 저자들은 기계 학습의 맥락에서 적의 능력에 대한 최적화 기반 관점을 사용하는 것이 더 적합하다고 믿습니다  
* 결국, 많은 최근 기계 학습 문제의 계산 복잡성에 대한 철저한 이해는 아직 개발되지 않았습니다  
* 그러나 기계 학습에서 해결되는 대부분의 최적화 문제는 1차 방법으로 해결되며, SGD 변형이 특히 심층 학습 모델을 훈련시키는 가장 효과적인 방법입니다  
* 따라서 저자들은 현재의 심층 학습 실천에 대해 1차 정보에 의존하는 공격이 어떤 의미에서 보편적이라고 믿습니다  
* 이 두 가지 아이디어를 결합하면 보장된 견고함을 가진 기계 학습 모델로 가는 길을 제시합니다  
* 저자들이 PGD 적에 대해 네트워크를 훈련시키면 현재 접근 방식을 포괄하는 다양한 공격에 대해 견고할 것입니다  
* 실제로 이 견고성 보장은 블랙박스 공격의 맥락에서 더욱 강해질 것입니다  
* 즉, 적이 대상 네트워크에 직접 접근할 수 없는 공격입니다  
* 대신 적은 (대략적인) 모델 아키텍처와 훈련 데이터 세트와 같은 덜 구체적인 정보만 가지고 있습니다  
이 공격 모델을 '제로 오더' 공격의 예로 볼 수 있습니다  
* 즉, 적은 분류기에 직접 접근할 수 없으며 선택한 예제에서만 평가할 수 있지만 기울기 피드백은 받을 수 없습니다  

## 3.3 Descent Directions for Adversarial Training  
* 앞서 논의한 내용에 따르면 내부 최적화 문제를 PGD를 적용하여 성공적으로 해결할 수 있습니다  
* 적대적으로 견고한 네트워크를 훈련시키기 위해서는 안장점 공식(2.1)의 외부 최적화 문제, 즉 내부 최대화 문제의 값을 최소화하는 모델 매개변수를 찾아야 합니다  

* 신경망을 훈련시키는 주된 방법은 확률적 경사 하강법(SGD)을 사용하는 것입니다  
* 외부 문제의 기울기, ∇θρ(θ),를 계산하는 자연스러운 방법은 내부 문제의 최대화자에서 손실 함수의 기울기를 계산하는 것입니다  
* 이는 입력 점을 해당하는 적대적 변조로 대체하고 네트워크를 변조된 입력으로 정상적으로 훈련시키는 것과 동일합니다  
* 처음에는 이것이 안장점 문제에 대한 유효한 하강 방향인지 명확하지 않습니다  
* 그러나 연속적으로 미분 가능한 함수의 경우, 최적화의 고전 정리인 단스킨의 정리는 이것이 사실임을 밝히고, 내부 최대화자에서의 기울기가 안장점 문제에 대한 하강 방향에 해당함을 나타냅니다  

* 단스킨의 정리가 우리 문제에 대해 정확하게 적용되지 않는다는 점(함수가 ReLU 및 최대 풀링 유닛으로 인해 연속적으로 미분 가능하지 않으며, 내부 최대화자를 대략적으로만 계산하기 때문)에도 불구하고, 저자들의 실험은 이 기울기를 사용하여 문제를 최적화할 수 있음을 시사합니다  
* 적대적 예시에서 손실의 기울기를 사용하여 SGD를 적용함으로써, 훈련하는 동안 안장점 문제의 손실을 지속적으로 줄일 수 있습니다  
* 이러한 관찰은 저자들이 안장점 공식(2.1)을 신뢰성 있게 최적화하고 견고한 분류기를 훈련시킬 수 있음을 시사합니다  
* 저자들은 단스킨의 정리를 공식적으로 진술하고 이 문제에 어떻게 적용되는지 설명합니다

<br/>
# 4 Network Capacity and Adversarial Robustness  
* 식 (2.1)의 문제를 성공적으로 해결하는 것만으로는 견고하고 정확한 분류를 보장할 수 없습니다  
* 문제의 값(즉, 적대적 예시에 대한 최종 손실)이 작아야 저자들의 분류기 성능에 대한 보장이 제공됩니다  
* 특히 매우 작은 값은 적대적 입력에 견고한 완벽한 분류기에 해당합니다  

* 가능한 변조의 고정된 집합 S에 대해, 문제의 값은 저자들이 학습하는 분류기의 아키텍처에 전적으로 의존합니다  
* 결과적으로 모델의 아키텍처 용량은 전체 성능에 중대한 영향을 미치는 주요 요소가 됩니다  
* 고차원에서 견고한 방식으로 예제를 분류하려면 더 강력한 분류기가 필요합니다  
* 적대적 예시의 존재는 문제의 결정 경계를 더 복잡한 것으로 변경합니다  

* 저자들의 실험은 용량이 견고성뿐만 아니라 강력한 적에 대항하여 성공적으로 훈련할 수 있는 능력에도 중요하다는 것을 확인합니다  
* MNIST 데이터 세트의 경우, 저자들은 간단한 컨볼루션 네트워크를 고려하고 네트워크 크기가 다양한 적에 대한 행동이 어떻게 변하는지 연구합니다  
* 네트워크를 계속 두 배로 늘리면서(즉, 컨볼루션 필터의 수와 완전 연결 계층의 크기를 두 배로 늘림) 저자들은 결과를 확인합니다  

* CIFAR10 데이터 세트의 경우, 저자들은 레즈넷 모델을 사용했습니다  
* 데이터 증대를 위해 무작위 크롭과 뒤집기를 사용했으며, 이미지별 표준화를 수행했습니다  
* 용량을 늘리기 위해 저자들은 네트워크에 더 넓은 계층을 10배로 증가시켰습니다  
* 이로 인해 (16, 160, 320, 640) 필터를 가진 5개의 잔여 유닛이 있는 네트워크가 생성되었습니다  
* 이 네트워크는 자연 예제로 훈련했을 때 95.2%의 정확도를 달성했습니다  
* 적대적 예시는 ε = 8로 생성되었습니다  
* 용량 실험 결과는 다음과 같습니다  

<br/>
# 5 Experiments: Adversarially Robust Deep Learning Models  
* 이전 섹션에서 개발한 문제에 대한 이해를 바탕으로 이제 저자들은 견고한 분류기를 훈련시키는 제안된 접근 방식을 적용할 수 있습니다  
* 지금까지의 실험에서 보여준 것처럼, 저자들은 두 가지 주요 요소에 초점을 맞춰야 합니다: a) 충분히 높은 용량의 네트워크를 훈련시키고 b) 가능한 가장 강력한 적을 사용합니다  

* MNIST와 CIFAR10에 대해, 선택된 적은 자연 예시 주변에서 무작위 변조로 시작하는 투영 기울기 하강(PGD)입니다  
* 이것은 오직 1차 정보를 사용하여 예시의 손실을 효율적으로 최대화할 수 있는 알고리즘으로, 우리의 * '완전한' 1차 적 개념에 해당합니다  
* 모델을 여러 에포크 동안 훈련시키므로 배치당 PGD를 여러 번 재시작할 필요가 없습니다 — 다음에 각 예시를 만날 때 새로운 시작이 선택될 것입니다  

* 적대적 예시에 대한 훈련 손실이 지속적으로 감소하는 것을 관찰함으로써, 저자들은 실제로 원래 최적화 문제를 훈련 중에 성공적으로 해결하고 있음을 알 수 있습니다  
* 이 행동은 저자들이 안장점 공식의 내부 문제의 값을 지속적으로 줄일 수 있다는 것을 시사하며, 점점 더 견고한 분류기를 생산하고 있습니다  

<br/>
# 6 Related Work  
* 적대적 예시에 대한 연구 분야는 매우 광범위하므로 저자들은 여기에서 가장 관련이 깊은 논문에만 초점을 맞춥니다  
* 먼저 저자들은 견고한 최적화가 심층 학습 이전에 수십 년 동안 연구되어 왔음을 지적하고 싶습니다  
* 저자들은 또한 적대적 기계 학습이 심층 신경망의 널리 퍼진 사용 이전부터 존재했다는 점을 지적합니다  

* 적대적 훈련은 이전에 도입되었지만 사용된 적은 매우 약했습니다  
* 이는 데이터 포인트 주변의 손실을 선형화하는 데 의존했기 때문입니다  
* 결과적으로 이 모델들은 이 특정 적에 대해서는 견고했지만, 반복적 공격을 사용하는 약간 더 정교한 적에게는 완전히 취약했습니다  

* 이미지넷에서의 적대적 훈련에 대한 최근 연구도 모델 용량이 적대적 훈련에 중요하다는 것을 관찰했습니다  
* 이 논문과는 대조적으로, 저자들은 다단계 방법(PGD)에 대한 훈련이 이러한 적에 대한 저항력을 이끌어낸다는 것을 발견했습니다  

* 이전에 적대적 훈련을 위한 최소-최대 최적화 문제의 버전도 고려되었습니다  
* 그러나 이전 언급된 결과와 현재 논문 사이에는 세 가지 중요한 차이점이 있습니다  
* 첫째, 저자들은 내부 최대화 문제를 해결하기 어렵다고 주장하는 반면, 저자들은 손실 표면을 더 자세히 조사하고 무작위로 재시작된 투영 기울기 하강이 비슷한 품질의 해결책으로 수렴하는 것을 발견했습니다  
* 이는 심층 신경망이 적대적 예시에 대해 면역될 수 있음을 제공하는 좋은 증거입니다  
* 둘째, 이들은 1단계 적만을 고려하는 반면, 저자들은 다단계 방법을 사용합니다  
* 셋째, 이전의 실험에서는 유망한 결과를 보였지만 FGSM에 대해서만 평가되었습니다  
* 그러나 FGSM만으로 평가하는 것은 완전히 신뢰할 수 없습니다  
* 그 증거로, ε = 0.7에 대해 70%의 정확도를 보고했지만, 0.5 이상으로 각 픽셀을 변조할 수 있는 어떤 적도 일관된 회색 이미지를 만들어 분류기를 속일 수 있습니다  

<br/>
# 7 Conclusion  
* 저자들의 발견은 심층 신경망이 적대적 공격에 저항할 수 있도록 만들 수 있음을 보여줍니다  
* 이론과 실험이 가리키는 바와 같이, 저자들은 신뢰할 수 있는 적대적 훈련 방법을 설계할 수 있습니다  
* 이는 근본적인 최적화 과제의 예상치 못한 규칙적 구조 덕분입니다  
* 즉, 관련 문제가 많은 서로 다른 국소 최대를 가지고 있지만, 그 값들은 매우 집중되어 있습니다  
* 전반적으로, 저자들의 발견은 적대적으로 견고한 심층 학습 모델이 현재 도달 가능하다는 희망을 제공합니다  

* MNIST 데이터 세트의 경우, 저자들의 네트워크는 강력한 `∞-제한 적에 대해 높은 정확도를 달성하며, 큰 변조에도 견고합니다  
* 저자들의 CIFAR10 실험은 아직 같은 수준의 성능에 도달하지 못했습니다  
* 그러나 저자들의 결과는 이미 저자들의 기술이 네트워크의 견고성을 크게 향상시킨다는 것을 보여줍니다  
* 저자들은 이 방향을 더 탐구함으로써 이 데이터 세트에 대한 적대적으로 견고한 네트워크를 이끌어 낼 것이라고 믿습니다

<br/>  
# 요약  
* 
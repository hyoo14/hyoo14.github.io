---
layout: post
title:  "[2025]DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
date:   2026-01-07 17:14:31 -0000
categories: study
---

{% highlight ruby %}

한줄 요약: DeepSeek-V3.2는 DeepSeek Sparse Attention(DSA) 메커니즘을 도입하여 긴 문맥에서의 계산 효율성을 극대화하고, 대규모 강화 학습 프레임워크를 통해 GPT-5와 유사한 성능을 달성.


짧은 요약(Abstract) :

DeepSeek-V3.2는 높은 계산 효율성과 뛰어난 추론 및 에이전트 성능을 조화롭게 결합한 모델입니다. 이 모델의 주요 기술 혁신은 다음과 같습니다: 
1. **DeepSeek Sparse Attention (DSA)**: DSA는 긴 문맥 시나리오에서 모델 성능을 유지하면서 계산 복잡성을 크게 줄이는 효율적인 주의 메커니즘입니다.
2. **확장 가능한 강화 학습 프레임워크**: 강력한 강화 학습 프로토콜을 구현하고 후속 훈련 컴퓨팅을 확장함으로써 DeepSeek-V3.2는 GPT-5와 유사한 성능을 발휘합니다. 특히, 고성능 변형인 DeepSeek-V3.2-Speciale는 GPT-5를 초월하며 Gemini-3.0-Pro와 동등한 추론 능력을 보여줍니다. 이 모델은 2025년 국제 수학 올림피아드(IMO)와 국제 정보 올림피아드(IOI)에서 금메달 성과를 달성했습니다.
3. **대규모 에이전트 작업 합성 파이프라인**: 도구 사용 시나리오에 추론을 통합하기 위해, 대규모로 훈련 데이터를 체계적으로 생성하는 새로운 합성 파이프라인을 개발했습니다. 이 방법론은 복잡하고 상호작용적인 환경 내에서 일반화 및 지시 따르기 강인성을 크게 향상시킵니다.


DeepSeek-V3.2 is a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of this model are as follows:
1. **DeepSeek Sparse Attention (DSA)**: DSA is an efficient attention mechanism that significantly reduces computational complexity while preserving model performance in long-context scenarios.
2. **Scalable Reinforcement Learning Framework**: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).
3. **Large-Scale Agentic Task Synthesis Pipeline**: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.


* Useful sentences :


{% endhighlight %}

<br/>

[Paper link]()
[~~Lecture link~~]()

<br/>

# 단어정리
*


<br/>
# Methodology


DeepSeek-V3.2는 고성능의 대규모 언어 모델로, 다음과 같은 주요 방법론을 통해 개발되었습니다.

1. **DeepSeek Sparse Attention (DSA)**: DSA는 효율적인 주의 메커니즘으로, 긴 문맥에서의 계산 복잡성을 크게 줄이면서도 모델 성능을 유지합니다. DSA는 두 가지 주요 구성 요소로 이루어져 있습니다: 
   - **라이트닝 인덱서**: 쿼리 토큰과 이전 토큰 간의 인덱스 점수를 계산하여 선택할 토큰을 결정합니다.
   - **세밀한 토큰 선택 메커니즘**: 인덱스 점수에 따라 상위 k개의 키-값 항목만을 선택하여 주의 출력을 계산합니다. 이로 인해 DSA는 O(L^2)에서 O(Lk)로 계산 복잡성을 줄여, 긴 문맥에서도 효율성을 높입니다.

2. **확장 가능한 강화 학습 프레임워크**: DeepSeek-V3.2는 안정적이고 확장 가능한 강화 학습 프로토콜을 구현하여, 사후 훈련 단계에서 10% 이상의 계산 예산을 할당합니다. 이를 통해 모델은 복잡한 작업에서 뛰어난 성능을 발휘할 수 있습니다.

3. **대규모 에이전틱 작업 합성 파이프라인**: 이 파이프라인은 도구 사용 시나리오에서의 추론을 통합하기 위해 설계되었습니다. 1,800개 이상의 다양한 환경과 85,000개의 복잡한 프롬프트를 생성하여, 모델의 일반화 및 지시 따르기 능력을 크게 향상시킵니다.

4. **전문가 증류 및 혼합 강화 학습**: 각 작업에 대해 전문화된 모델을 개발하고, 이를 통해 도메인별 데이터를 생성하여 최종 체크포인트를 만듭니다. 혼합 강화 학습을 통해 다양한 도메인에서의 성능을 균형 있게 조정합니다.

5. **생각 통합**: DeepSeek-V3.2는 도구 호출 시나리오에서의 사고 과정을 통합하여 복잡한 문제를 해결하는 능력을 향상시킵니다. 이를 위해 역사적 추론 내용을 보존하고, 새로운 사용자 메시지가 도입될 때만 이를 삭제하는 방식으로 컨텍스트 관리를 수행합니다.

이러한 방법론을 통해 DeepSeek-V3.2는 고성능의 대규모 언어 모델로 자리매김하며, 복잡한 작업에서의 성능을 크게 향상시켰습니다.

---




DeepSeek-V3.2 is a high-performance large language model developed through the following key methodologies:

1. **DeepSeek Sparse Attention (DSA)**: DSA is an efficient attention mechanism that significantly reduces computational complexity while maintaining model performance in long-context scenarios. DSA consists of two main components:
   - **Lightning Indexer**: This component computes the index score between the query token and preceding tokens to determine which tokens to select.
   - **Fine-Grained Token Selection Mechanism**: It retrieves only the top-k key-value entries based on the index scores to compute the attention output. This reduces the computational complexity from O(L^2) to O(Lk), enhancing efficiency even in long contexts.

2. **Scalable Reinforcement Learning Framework**: DeepSeek-V3.2 implements a robust and scalable reinforcement learning protocol that allocates more than 10% of the computational budget during the post-training phase. This allows the model to perform exceptionally well on complex tasks.

3. **Large-Scale Agentic Task Synthesis Pipeline**: This pipeline is designed to integrate reasoning into tool-use scenarios. It systematically generates over 1,800 distinct environments and 85,000 complex prompts, significantly improving the model's generalization and instruction-following capabilities.

4. **Specialist Distillation and Mixed Reinforcement Learning**: For each task, specialized models are developed to produce domain-specific data for the final checkpoint. Mixed reinforcement learning balances performance across diverse domains.

5. **Thinking Integration**: DeepSeek-V3.2 integrates a thinking process into tool-calling scenarios to enhance its ability to solve complex problems. It manages context by retaining historical reasoning content and only discarding it when a new user message is introduced.

Through these methodologies, DeepSeek-V3.2 establishes itself as a high-performance large language model, significantly enhancing performance on complex tasks.


<br/>
# Results



DeepSeek-V3.2는 여러 경쟁 모델과 비교하여 다양한 벤치마크에서 성능을 평가받았습니다. 이 모델은 GPT-5, Gemini-3.0-Pro, Kimi-K2, MiniMax와 같은 최신 모델들과 비교되었습니다. 평가에 사용된 주요 벤치마크는 다음과 같습니다:

1. **MMLU-Pro**: 이 벤치마크는 언어 이해 능력을 평가하는 데 사용되며, DeepSeek-V3.2는 85.0%의 정확도를 기록했습니다. 반면, GPT-5는 87.5%, Gemini-3.0-Pro는 90.1%의 정확도를 보였습니다.

2. **GPQA Diamond**: 이 벤치마크에서 DeepSeek-V3.2는 82.4%의 Pass@1을 기록했으며, GPT-5는 85.7%, Gemini-3.0-Pro는 91.9%로 더 높은 성과를 보였습니다.

3. **HLE (Human Last Exam)**: DeepSeek-V3.2는 25.1%의 Pass@1을 기록했으며, GPT-5는 26.3%, Gemini-3.0-Pro는 37.7%로 더 높은 점수를 얻었습니다.

4. **CodeLiveCodeBench**: DeepSeek-V3.2는 83.3%의 Pass@1을 기록하여, GPT-5의 84.5%와 Gemini-3.0-Pro의 90.7%에 비해 다소 낮은 성과를 보였습니다.

5. **AIME 2025**: DeepSeek-V3.2는 93.1%의 Pass@1을 기록했으며, GPT-5는 94.6%, Gemini-3.0-Pro는 95.0%로 더 높은 성과를 보였습니다.

6. **HMMT (Harvard-MIT Mathematics Tournament)**: DeepSeek-V3.2는 92.5%의 Pass@1을 기록했으며, Gemini-3.0-Pro는 97.5%로 더 높은 성과를 보였습니다.

7. **Terminal Bench 2.0**: DeepSeek-V3.2는 46.4%의 정확도를 기록했으며, GPT-5는 42.8%, Gemini-3.0-Pro는 54.2%로 더 높은 성과를 보였습니다.

이러한 결과들은 DeepSeek-V3.2가 여러 벤치마크에서 경쟁 모델에 비해 다소 낮은 성과를 보였음을 나타냅니다. 그러나 DeepSeek-V3.2는 여전히 오픈 소스 모델로서 상당한 성능을 발휘하며, 특히 코드 관련 작업에서 강력한 성능을 보여주었습니다. 




DeepSeek-V3.2 was evaluated against several competitive models across various benchmarks. This model was compared with state-of-the-art models such as GPT-5, Gemini-3.0-Pro, Kimi-K2, and MiniMax. The key benchmarks used for evaluation include:

1. **MMLU-Pro**: This benchmark assesses language understanding capabilities, where DeepSeek-V3.2 achieved an accuracy of 85.0%. In comparison, GPT-5 scored 87.5%, and Gemini-3.0-Pro achieved 90.1%.

2. **GPQA Diamond**: In this benchmark, DeepSeek-V3.2 recorded a Pass@1 of 82.4%, while GPT-5 scored 85.7%, and Gemini-3.0-Pro reached 91.9%.

3. **HLE (Human Last Exam)**: DeepSeek-V3.2 achieved a Pass@1 of 25.1%, with GPT-5 at 26.3% and Gemini-3.0-Pro at 37.7%.

4. **CodeLiveCodeBench**: DeepSeek-V3.2 recorded a Pass@1 of 83.3%, compared to GPT-5's 84.5% and Gemini-3.0-Pro's 90.7%.

5. **AIME 2025**: DeepSeek-V3.2 achieved a Pass@1 of 93.1%, while GPT-5 scored 94.6% and Gemini-3.0-Pro reached 95.0%.

6. **HMMT (Harvard-MIT Mathematics Tournament)**: DeepSeek-V3.2 recorded a Pass@1 of 92.5%, with Gemini-3.0-Pro achieving 97.5%.

7. **Terminal Bench 2.0**: DeepSeek-V3.2 achieved an accuracy of 46.4%, while GPT-5 scored 42.8% and Gemini-3.0-Pro reached 54.2%.

These results indicate that DeepSeek-V3.2 performed somewhat lower than competitive models across various benchmarks. However, it still demonstrates significant performance as an open-source model, particularly excelling in code-related tasks.


<br/>
# 예제



DeepSeek-V3.2 모델은 다양한 에이전트 작업을 수행하기 위해 대규모로 합성된 훈련 데이터를 사용합니다. 이 데이터는 여러 가지 복잡한 작업을 해결하기 위해 설계되었습니다. 예를 들어, 여행 계획을 세우는 작업을 살펴보겠습니다.

#### 예시: 여행 계획 세우기

**작업 설명**: 사용자는 특정 날짜에 특정 도시에서 시작하여 여행 일정을 계획해야 합니다. 사용자는 다음과 같은 요구 사항을 제시합니다:
- 여행 기간: 2025년 10월 1일부터 10월 3일까지
- 각 도시, 호텔, 관광지, 레스토랑은 중복되지 않아야 함
- 두 번째 날의 호텔 가격에 따라 레스토랑과 관광지의 예산이 달라져야 함

**훈련 데이터 인풋**:
- 시스템 프롬프트: "여행 계획을 세우는 데 도움을 주세요. 다음 요구 사항을 충족해야 합니다: [요구 사항 나열]"
- 예시 입력: "2025년 10월 1일, 항저우에서 시작하는 3일 여행 계획을 세워주세요."

**아웃풋**:
- 모델은 다음과 같은 형식으로 결과를 반환합니다:
```json
[
    { "time": "2025-10-01", "city": "항저우", "hotel": "호텔 A", "afternoon_restaurant": "레스토랑 A", "afternoon_attraction": "관광지 A", "evening_restaurant": "레스토랑 B" },
    { "time": "2025-10-02", "city": "상하이", "hotel": "호텔 B", "afternoon_restaurant": "레스토랑 C", "afternoon_attraction": "관광지 B", "evening_restaurant": "레스토랑 D" },
    { "time": "2025-10-03", "city": "베이징", "hotel": "호텔 C", "afternoon_restaurant": "레스토랑 E", "afternoon_attraction": "관광지 C", "evening_restaurant": "레스토랑 F" }
]
```

이와 같은 방식으로 모델은 사용자의 요구 사항을 충족하는 여행 일정을 생성합니다. 각 항목은 날짜, 도시, 호텔, 레스토랑, 관광지 정보를 포함하여 사용자가 요구한 조건을 모두 반영합니다.




DeepSeek-V3.2 model utilizes a large-scale synthesized training dataset to perform various agentic tasks. This data is designed to solve a range of complex tasks. For example, let's look at the task of planning a trip.

#### Example: Trip Planning

**Task Description**: The user needs to plan a travel itinerary starting from a specific city on specific dates. The user provides the following requirements:
- Travel Duration: From October 1, 2025, to October 3, 2025
- No repetition of cities, hotels, attractions, or restaurants
- Budget for restaurants and attractions varies based on the hotel price on the second day

**Training Data Input**:
- System Prompt: "Please help me plan a trip. The following requirements must be met: [list of requirements]"
- Example Input: "Plan a 3-day trip starting from Hangzhou on October 1, 2025."

**Output**:
- The model returns results in the following format:
```json
[
    { "time": "2025-10-01", "city": "Hangzhou", "hotel": "Hotel A", "afternoon_restaurant": "Restaurant A", "afternoon_attraction": "Attraction A", "evening_restaurant": "Restaurant B" },
    { "time": "2025-10-02", "city": "Shanghai", "hotel": "Hotel B", "afternoon_restaurant": "Restaurant C", "afternoon_attraction": "Attraction B", "evening_restaurant": "Restaurant D" },
    { "time": "2025-10-03", "city": "Beijing", "hotel": "Hotel C", "afternoon_restaurant": "Restaurant E", "afternoon_attraction": "Attraction C", "evening_restaurant": "Restaurant F" }
]
```

In this way, the model generates a travel itinerary that meets the user's requirements. Each entry includes the date, city, hotel, restaurant, and attraction information, reflecting all the conditions specified by the user.

<br/>
# 요약
DeepSeek-V3.2는 DeepSeek Sparse Attention(DSA) 메커니즘을 도입하여 긴 문맥에서의 계산 효율성을 극대화하고, 대규모 강화 학습 프레임워크를 통해 GPT-5와 유사한 성능을 달성하였다. 이 모델은 2025년 국제 수학 올림피아드와 정보 올림피아드에서 금메달 성과를 기록하며, 복잡한 상호작용 환경에서의 일반화 및 지시 따르기 능력을 크게 향상시켰다. 예를 들어, DeepSeek-V3.2-Speciale는 Gemini-3.0-Pro를 초월하는 성능을 보여주었다.

---

DeepSeek-V3.2 introduces the DeepSeek Sparse Attention (DSA) mechanism to maximize computational efficiency in long contexts and achieves performance comparable to GPT-5 through a scalable reinforcement learning framework. The model has demonstrated significant improvements in generalization and instruction-following capabilities, achieving gold-medal performance in the 2025 International Mathematical Olympiad and the International Olympiad in Informatics. For instance, DeepSeek-V3.2-Speciale surpasses the performance of Gemini-3.0-Pro.

<br/>
# 기타



1. **다이어그램 및 피규어**
   - **Figure 1**: DeepSeek-V3.2의 성능을 다른 모델들과 비교한 벤치마크 결과를 보여줍니다. DeepSeek-V3.2-Speciale는 GPT-5 및 Gemini-3.0-Pro와 유사한 성능을 보이며, 특히 수학 및 프로그래밍 대회에서 금메달 성과를 달성했습니다. 이는 DeepSeek-V3.2의 높은 추론 능력과 에이전트 성능을 강조합니다.
   - **Figure 2**: DeepSeek Sparse Attention(DSA)의 아키텍처를 설명합니다. DSA는 효율적인 주의 메커니즘을 통해 긴 컨텍스트에서의 계산 복잡성을 줄이고 성능을 유지하는 방법을 보여줍니다.
   - **Figure 3**: DeepSeek-V3.1-Terminus와 DeepSeek-V3.2의 추론 비용을 비교합니다. DSA를 통해 DeepSeek-V3.2는 긴 컨텍스트에서의 속도를 크게 향상시켰습니다.

2. **테이블**
   - **Table 2**: DeepSeek-V3.2와 다른 모델들 간의 성능 비교를 보여줍니다. DeepSeek-V3.2는 여러 벤치마크에서 경쟁력 있는 성과를 보이며, 특히 코드 에이전트 평가에서 우수한 성능을 나타냅니다.
   - **Table 3**: DeepSeek-V3.2-Speciale의 성능을 다양한 벤치마크에서 비교합니다. 이 모델은 Gemini-3.0-Pro를 초과하는 성능을 보여주며, 특히 수학 및 코딩 대회에서 금메달 수준의 성과를 달성했습니다.
   - **Table 5**: 합성 에이전트 작업의 정확도를 비교합니다. DeepSeek-V3.2-Exp는 합성 데이터에서 낮은 성과를 보였지만, 이는 합성 데이터가 도전적임을 나타냅니다.

3. **어펜딕스**
   - **Appendix A**: MHA와 MQA 모드의 차이를 설명합니다. MQA 모드는 계산 효율성을 높이기 위해 여러 쿼리에서 키-값 항목을 공유하는 방식을 사용합니다.
   - **Appendix D**: IOI, ICPC, IMO, CMO의 평가 방법을 설명합니다. 각 대회에서 모델의 최대 생성 길이를 설정하고, 제출 전략을 통해 최적의 성과를 달성하는 방법을 제시합니다.

### Insights and Results from Other Sections (Diagrams, Figures, Tables, Appendices)

1. **Diagrams and Figures**
   - **Figure 1**: This figure presents benchmark results comparing the performance of DeepSeek-V3.2 with other models. DeepSeek-V3.2-Speciale shows comparable performance to GPT-5 and Gemini-3.0-Pro, particularly achieving gold medal results in mathematics and programming competitions, highlighting its superior reasoning and agent performance.
   - **Figure 2**: This figure illustrates the architecture of DeepSeek Sparse Attention (DSA). DSA effectively reduces computational complexity while maintaining performance in long-context scenarios, showcasing an efficient attention mechanism.
   - **Figure 3**: This figure compares the inference costs of DeepSeek-V3.1-Terminus and DeepSeek-V3.2. The implementation of DSA significantly enhances the speed of DeepSeek-V3.2 in long-context scenarios.

2. **Tables**
   - **Table 2**: This table compares the performance of DeepSeek-V3.2 with other models across various benchmarks. DeepSeek-V3.2 demonstrates competitive performance, particularly excelling in code agent evaluations.
   - **Table 3**: This table compares the performance of DeepSeek-V3.2-Speciale across multiple benchmarks. The model surpasses Gemini-3.0-Pro, achieving gold medal-level performance in mathematics and coding competitions.
   - **Table 5**: This table shows the accuracy of synthetic agentic tasks across different models. DeepSeek-V3.2-Exp achieves low accuracy, indicating that the synthetic data includes challenging tasks.

3. **Appendices**
   - **Appendix A**: This section explains the differences between MHA and MQA modes. The MQA mode enhances computational efficiency by sharing key-value entries across multiple queries.
   - **Appendix D**: This section outlines the evaluation methods for IOI, ICPC, IMO, and CMO competitions. It describes how the model's maximum generation length is set and the submission strategies used to achieve optimal performance.

<br/>
# refer format:
### BibTeX 형식

```bibtex
@article{DeepSeek2025,
  title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models},
  author={DeepSeek-AI},
  year={2025},
  journal={arXiv preprint arXiv:2512.02556},
  url={https://arxiv.org/abs/2512.02556}
}
```

### 시카고 스타일 인용

DeepSeek-AI. 2025. "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models." arXiv preprint arXiv:2512.02556. https://arxiv.org/abs/2512.02556.

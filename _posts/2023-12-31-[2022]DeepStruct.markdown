---
layout: post
title:  "[2022]DeepStruct: Pretraining of Language Models for Structure Prediction"
date:   2023-12-31 12:57:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    
* 이 논문은 언어 모델의 구조적 이해 능력을 향상시키는 방법을 소개  
* 파인튜닝 통해 모델을 향상시키는 것이 아니라, 텍스트에서 구조를 생성하기 위해 작업과 무관한 코퍼스의 집합에 언어 모델을 사전 훈련하는 것을 포함
* 이 구조 사전 훈련은 모델이 구조적 작업에 대해 가진 지식의 제로샷 전이를 가능하게 함  
* 이 방법은 open information extraction, joint entity and relation extraction, named entity(명명된 실체) recognition, relation classification, semantic role labeling, event extraction, coreference resolution(공동 참조 해결), factual probe(사실적 탐사), intent detection(의도 탐지), 대화 상태 추적을 포함한 10가지 구조 예측 작업에 대한 28개 데이터셋에서 평가
* 이 접근법은 작업별 특정 훈련 세트로 더욱 강화, 이는 10B 파라미터 언어 모델이 대부분의 작업에 비중이 있는 지식을 전달할 수 있으며, 평가된 28개 데이터셋 중 21개에서 SOTA 달성  

Useful sentences :  
*   

{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1hdW96_pkwdPshi-MrpA-oZkN6fjDwQ_U?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 



<br/>

# 1 Introduction  
*   pretrained Language Models이 최근 몇 년 동안 자연어 처리(NLP) 분야에 혁명을 가져옴  
* 이러한 모델들은 점점 더 유연하고 과제에 구애받지 않는 downstream task에 능숙  
* 그러나 이러한 모델들의 transfer 성능은 구조 예측 작업에서는 충분히 연구되지 않음  
* 대부분의 연구는 텍스트의 특정 측면, 예를 들어 언어 모델링에서 다음 단어를 예측하는 것과 같은 것을 이해하는 데 중점  
* 그러나 이러한 downstream task와는 달리, 구조 예측은 텍스트의 구조적 이해가 필요하며, 이는 텍스트의 여러 관련 측면을 하나의 구조로 통합하는 데 더 나아가야 함   

# 2 Structure Pretraining  
* 'DeepStruct'라는 구조 사전 훈련 방법을 제안  
* 이 방법은 과제에 구애받지 않는 다양한 코퍼스를 사용하여, 입력된 텍트에서 다양한 관계와 엔티티를 추출하는 구조를 예측하는 데 초점  
* 예를 들어, "The couple have a daughter"라는 문장에서 "(couple; have; a daughter)"라는 구조를 추출하는 것과 같음  

## 2.1 Generative Pretraining  
* 언어 모델들의 구조적 이해 능력을 개선하는 방법 제안  
* 이 연구에서는 기존의 작업 증강 방식에 의존하는 대신, 사전 훈련 단계에서 텍스트 구조를 더 잘 이해할 수 있도록 언어 모델들을 체계적으로 교육하는 '구조 사전 훈련'을 도입  
* 이를 통해 언어 모델들이 사전 훈련 동안 구조에 대해 배운 지식을 하류 구조 예측 작업에 제로샷(즉, 추가적인 학습 없이)으로 전달할 수 있게함  
* 예를 들어, 이 연구의 제로샷 10B 매개변수 언어 모델은 구조 예측 벤치마크 데이터셋에서 175B GPT-3 모델의 제로샷 성능을 크게 능가  
* 이러한 성과는 구조 예측을 일련의 단위 작업, 즉 삼중 항목 예측 작업으로 재구성함으로써 달성  
* 연구팀은 작업에 구애받지 않는 구조적 코퍼스 모음에서 언어 모델을 훈련하여 텍스트에서 삼중 항목을 생성  
** 삼중 항목 표현의 설계는 중요한데, 이는 다양한 표준 구조 예측 작업을 동일한 작업 형식으로 통합  
* 이 사전 훈련된 모델인 DEEPSTRUCT는 개방형 정보 추출, 공동 엔티티 및 관계 추출, 명명된 엔티티 인식, 관계 분류, 의미 역할 라벨링, 이벤트 추출, 핵심어 결정, 사실 조사, 의도 탐지 및 대화 상태 추적을 포함한 10개의 구조 예측 작업에 걸쳐 28개 데이터셋에 적용  
** 연구팀은 여러 하류 구조 예측 훈련 세트로 사전 훈련을 더욱 강화하여 28개 데이터셋 중 21개에서 최고의 성능을 달성

### Pretraining Data  
* 제안하는 언어 모델은 텍스트의 단일 측면만 이해하는 것이 아니라, 텍스트의 전체 구조를 이해하는 데 중점  
* 구조 사전 훈련은 언어 모델을 지도하여 텍스트에서 구조를 생성하도록 설계  
* 이는 필요에 따라 임의의 구조를 생성할 수 있도록 하는 것이 이상적이지만, 그러한 구조의 복잡한 성격 때문에 실현하기 어렵  
* 대안으로, 구조 예측을 삼중 항목 생성 작업의 조합으로 재구성  
* 삼중 항목은 (head entity; relation; tail entity)로, 엔티티 간의 관계를 설명  
* 연구팀은 엔티티, 관계, 삼중 항목을 각각 예측하는 세 가지 사전 훈련 작업을 설계  
* 예를 들어, 엔티티 예측은 입력 문장에서 엔티티와 그 타입에 관한 삼중 항목을 출력하는 것을 목표로 하며, 이를 위해 입력 문장 앞에 "entity:"라는 접두사를 추가하는 방식으로 구현  
* 이러한 접근 방식은 언어 모델이 텍스트에서 다양한 구조를 생성하고, 이를 통해 보다 정교한 구조적 이해 능력을 개발할 수 있도록 지원  
* 구조 사전 훈련은 언어 모델에게 다양한 구조 예측 작업에 적용할 수 있는 능력을 제공함으로써, 텍스트의 구조적 이해를 한 단계 더 발전시키는 것을 목표로 함  


## 2.2 Tasks  
* pretrain을 조건부 생성 작업으로 구성하며, 입력 텍스트(x)에 해당하는 출력(y)이 삼중 항목의 시퀀스로 나타나는 구조로 함  
*  pretraib의 목적은 확률적 프레임워크에서 조건부 분포 p(y|x)를 추정하는 것  
*  이를 위해 연구팀은 자기 회귀(auto-regressive)언어 모델을 사용하여 p(y|x)를 모델링  
  
* pretrain 데이터로는, T-REx, TEKGEN, KELM, WebNLG, ConceptNet 등의 여러 대규모 코퍼스를 사용  
* * 이들 코퍼스는 텍스트를 지식 그래프(예: 위키데이터)의 고품질 엔티티 및 관계와 연결하는 삼중 항목으로 구성  
* * 이러한 데이터는 엔티티 및 관계 예측 작업에 사용    
* * 추가적으로, OPIEC 코퍼스는 삼중 항목 예측 작업을 위해 오픈 스키마 삼중 항목을 제공  
** 사전 훈련 데이터의 통계와 해당 사전 훈련 작업을 표 1에서 보여줌  
*** 예를 들어, 엔티티 예측 작업의 훈련 절차는 다음과 같은 입력 및 출력 샘플을 기반: "Input entity: Iago is born in 1951" 및 "Output (Iago; instance of; person)". 여기서 입력 텍스트와 출력 삼중 항목이 정렬되며, 이 정렬은 사전 훈련 데이터에 의해 제공. 토큰은 <s> 토큰으로 시작하여 <e> 토큰으로 끝나는 자기 회귀적 방식으로 예측  ​​​​​​  ​

## 2.3 Zero-Shot  
* 제로샷 DEEPSTRUCT는 사전 훈련된 모델을 특정 작업에 대한 추가적인 훈련 없이 추론 사용  
*  예를 들어, 명명된 엔티티 인식의 경우, 인간은 지시를 이해하고 따를 수 있음(생성형 모델이므로)  
* 제로샷 transfer는 하류 작업을 사전 훈련 작업 중 하나 또는 그 조합으로 변환함으로써 가능.  
* * 추론 시간에 일곱 가지 구조 예측 작업은 "entity:" 접두사가 붙은 입력 예시(파란색)로 엔티티 예측으로 구성.  
* * 한 가지 작업은 "relation:" 접두사(빨간색)가 붙어 관계 예측으로 설정.   
* * 또한, 개방형 정보 추출은 "triple:" 접두사(노란색)가 있는 삼중 항목 예측 작업. 공동 엔티티 및 관계 추출(JER)은 엔티티 및 관계 예측(보라색)의 조합임.  
** 프리트레인 데이터셋과 다운스트림 데이터셋 간의 스키마 정렬을 구축하여, 출력 삼중 항목을 해당 구조 예측으로 해석.   
** 그러나 사전 훈련 데이터의 분포는 하류 데이터셋의 분포와 완벽하게 일치하지 않아, 사전 훈련된 모델의 출력 분포에 변화가 생김.  
** 제로샷 설정은 대화 상태 추적과 같은 일부 작업에서 최상의 성능을 발휘하지 못함  
** 이를 해결하기 위해 다양한 구조 예측 데이터셋을 사전 훈련 코퍼스에 통합하고, 데이터셋 혼합에 대한 방법론을 훈련.   
** 또한, 다중 작업 훈련 후에는 각 작업별 특정 데이터셋에 대해 방법론을 추가로 미세 조정.  
*** 이는 사전 훈련된 언어 모델을 하류 작업에 활용하는 방법​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​.  


## 2.4 Multi-Task  
섹션에서는 DEEPSTRUCT의 다중 작업 설정에 대해 설명합니다. 이 설정에서는 단일 모델이 여러 하류 작업을 수행할 수 있으며, 이는 각 작업에 대해 별도의 모델과 매개변수를 사용하는 지도 학습 모델과 다릅니다. 연구팀은 세 가지 데이터셋에서 최고의 성능을 달성하고, 다른 데이터셋에서도 경쟁력 있는 성능을 보여줍니다. 특히, 제로샷에서 다중 작업 설정으로 전환할 때 대부분의 구조 예측 작업에서 큰 성능 향상이 관찰됩니다. 이는 대부분의 작업이 사전 훈련된 모델의 분포에서 벗어나 있음을 시사합니다. 그럼에도 불구하고, 연구팀의 방법은 다중 작업 학습을 통해 하류 분포에 적응할 수 있는 강력하고 공정한 성능을 보여줍니다. FewRel과 같은 저자원 구조 예측 벤치마크에서도 강력한 다중 작업 성능을 보여줍니다. 이는 다중 작업 설정이 비슷한 작업에서 지식을 전달함으로써 저자원 상황에서 유리할 수 있음을 나타냅니다.

ACE2005 명명된 엔티티 인식 데이터셋은 훈련 및 테스트 분할 간의 중복으로 인해 다중 작업 훈련에서 제외되었습니다. 미세 조정 후에는 21개 데이터셋에서 최고의 성능을 달성합니다. 예를 들어, CoNLL05 Brown (의미 역할 라벨링)에서 +8.0점, TACRED (관계 분류)에서 +2.9점의 절대적인 성능 향상을 보여줍니다.

이러한 결과는 모두 10B 매개변수의 사전 훈련된 GLM 언어 모델을 기반으로 합니다. GLM은 자기 회귀 언어 모델이며, 입력되는 컨텍스트 x는 특정 작업의 이름과 데이터셋 이름이 접두사로 붙은 형태로 인코딩됩니다​​​​​​​​​​​​​​​​​​​​​​​​​​.









# 3 Experiments  
DEEPSTRUCT 모델의 성능을 다양한 구조 예측 작업에서 평가한 결과를 제시합니다.

실험은 DEEPSTRUCT 모델을 사용하여 28개의 데이터셋에 걸친 10개의 구조 예측 작업에 적용합니다. 이 작업들에는 명명된 엔티티 인식, 관계 분류, 의미 역할 라벨링, 사건 추출, 코어퍼런스 해결, 의도 감지 등이 포함됩니다.


## 3.1 Main Results  
1. DEEPSTRUCT 모델은 제로샷 및 다중 작업 설정에서 모두 강력한 성능을 보여줍니다. 특히 다중 작업 설정에서는 여러 데이터셋에서 최고의 성능을 달성하며, 일부 데이터셋에서는 경쟁 모델들에 비해 몇 점 차이로 우수한 성능을 보여줍니다.
2. 몇몇 작업들, 특히 사실 확인 프로브에서는 제로샷 설정이 최신의 최고 성능을 달성합니다.
3. 다중 작업 학습은 대부분의 구조 예측 작업에서 제로샷 설정보다 큰 성능 향상을 보여주며, 이는 모델이 다양한 작업의 분포에 적응할 수 있음을 나타냅니다.
4. 다중 작업 학습 후에, 추가적인 작업별 미세 조정을 통해 21개 데이터셋에서 최고의 성능을 달성합니다. 예를 들어, CoNLL05 Brown (의미 역할 라벨링)과 TACRED (관계 분류)에서 눈에 띄는 성능 향상을 보여줍니다.

이 실험 결과들은 DEEPSTRUCT가 다양한 구조 예측 작업에 대해 강력하고 유연한 성능을 제공할 수 있음을 보여줍니다. 이는 구조 예측 작업에 대한 언어 모델의 이해를 개선하는 데 중요한 기여로 간주될 수 있습니다​​​​​​​​​​​​​​​​.


## 3.2 Ablation Studies  
다양한 사전 훈련 전략이 모델의 하류 성능에 어떤 영향을 미치는지 조사합니다. 연구팀은 CoNLL04 데이터셋(JER)에 대해 다음 설정을 평가합니다:

### Pretraining Strategies  
예제 비례 혼합: 다양한 크기의 데이터셋을 균형 있게 조합합니다.
엔티티 및 관계 증강: 특수 토큰 “[]”을 사용하여 문장에서 엔티티와 관계의 위치를 표시합니다.
사전 훈련 없이 미세 조정: 구조 사전 훈련을 제거하고 CoNLL04에서만 미세 조정합니다.
제로샷: 작업별 데이터셋을 제외한 태스크 불변 데이터셋만 사용합니다.
다중 작업: 미세 조정 없이 다중 작업 모델을 사용합니다.
미세 조정: 하류 작업별 데이터셋을 제외한 구조 사전 훈련을 사용하고 CoNLL04에서 미세 조정합니다.
결과는 구조 예측의 핵심이 복잡한 구조를 삼중 항목 예측 작업으로 간소화하는 데 있다는 것을 확인합니다. 구조 사전 훈련은 언어 모델의 구조 예측 능력을 크게 향상시키며, 이는 언어 모델의 사전 훈련과 하류 작업 간의 격차를 줄입니다.


### Scaling Laws  
"Scaling Laws (확장 법칙)" 섹션은 모델 크기가 구조 사전 훈련에 어떻게 이득을 주는지 탐구합니다. 110M, 220M, 2B, 10B 매개변수를 가진 모델들을 평가합니다. 모델이 커질수록 평균 성능이 향상되며, 특히 10B 매개변수 모델은 가장 좋은 성능을 보여줍니다. 큰 모델은 구조 사전 훈련을 완전히 활용하고 하류 작업에 대해 적절한 삼중 항목을 생성할 수 있습니다. 또한, 모델 크기가 성능 향상에 더 중요한 역할을 하며, 미세 조정은 특정 작업에 모델을 특화시키는 데 도움이 됩니다​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​.






# 4 Related Work  
사전 훈련된 언어 모델 (Pretrained Language Models, LMs): Devlin et al. (2019), Radford et al. (2019b), Yang et al. (2019) 등의 연구에서 소개된 사전 훈련된 언어 모델들은 현대 자연어 처리(NLP)의 핵심 구성 요소로 강조됩니다. 이러한 모델들은 다양한 NLP 작업에 효과적임이 입증되었습니다.

시퀀스-투-시퀀스 (Sequence-to-Sequence, seq2seq) 언어 모델: T5 (Raffel et al. 2019), BART (Lewis et al. 2020), GLM (Du et al. 2021)과 같은 seq2seq 언어 모델은 조건부 생성을 목표로 합니다. 이 모델들은 요약, 텍스트 채우기 등 다양한 자연 언어 생성 작업에 유용하게 적용되었습니다.

생성적 예측 (Generative Prediction): 최근의 연구들(Paolini et al. 2021; Schick and Schütze 2021; Lester et al. 2021)은 seq2seq 모델이 다양한 NLP 작업을 위한 통합적인 해결책을 제공할 수 있음을 발견했습니다. 이러한 모델들은 다양한 작업을 모델링하는 데 활용될 수 있습니다.

# 5 Discussion  
## Related Models  
최근 연구들은 구조 예측 작업에 대해 통합된 해결책을 제공했습니다. DEEPSTRUCT는 TANL(Paolini et al. 2021) 및 DeepEx(Wang et al. 2021)와 같은 최신 모델과 비교됩니다. TANL은 작업별 데이터 증강을 제안하여 각 구조 예측 작업에 대해 입력과 출력에 작업 정보와 예측을 각각 주석으로 달고, DEEPSTRUCT는 구조 예측 작업을 삼중 항목 생성 작업의 집합으로 분해하여 단일 표현 형식으로 통합합니다. 반면 DeepEx는 정보 추출 작업에 대한 삼중 항목을 생성하기 위해 빔 탐색을 통해 사전 훈련된 언어 모델의 주의 행렬을 탐구합니다​​​​​​.


## Zero-Shot Setup  
제로샷 설정에서는 사전 훈련된 모델을 하류 작업에 직접 적용하는 것을 의미합니다. 이는 사전 훈련 데이터에 하류 훈련 세트를 포함하지 않는 설정입니다. DEEPSTRUCT의 경우, 사전 훈련 데이터는 작업에 구애받지 않으며, 각 작업에 대해 사전 훈련 데이터와 작업 데이터셋 간의 오프라인 정렬을 기반으로 합니다. GPT-3의 제로샷 설정은 GPT-3가 제안한 프롬프트 방식을 따르며, 실제 진리를 기반으로 "참/거짓" 질문을 위한 프롬프트를 디자인합니다. GPT-3는 이러한 질문에 대해 "예" 또는 "아니오"로 답변하여 작업 예측을 생성합니다​​​​​​.


# 6 Conclusion  
연구팀은 사전 훈련된 언어 모델을 구조 사전 훈련을 통해 향상시켰으며, 이를 통해 언어 모델이 텍스트에서 삼중 항목을 출력하도록 가르칩니다.
DEEPSTRUCT는 제로샷과 다중 작업 전송 학습을 모두 지원합니다.
DEEPSTRUCT는 평가한 28개 데이터셋 중 21개에서 최신의 최고 성능을 달성했습니다.
이 결과는 사전 훈련된 언어 모델이 보다 고차원적인 이해(예: 구조적 이해)를 처리할 수 있음을 보여주며, 이는 더 많은 NLP 작업에 도움이 될 수 있습니다.
연구팀은 이 연구가 언어의 구조적 이해에 대한 향후 연구를 촉진할 것이라고 기대합니다​​.

# 7 Ethical Considerations  
연구팀은 ACM 윤리 강령을 인지하고 준수하며, 이 연구가 주로 언어 모델의 구조적 예측을 위한 사전 훈련 및 다중 작업 학습에 관한 것임을 언급합니다.
연구에서 사용된 언어 모델들은 기존의 위험과 잠재적 해악에 대해 (Brown et al. 2020)에서 논의된 바 있으며, 특히 작업 불변 데이터(예: 위키피디아)와 다중 작업 하류 데이터셋(대부분 뉴스 기사에서 생성)에서 존재할 수 있는 잠재적 바이어스가 있음을 인정합니다.
연구팀은 모델 사용 또는 NLP 모델을 자신들의 데이터셋으로 훈련시킬 때 취약한 인구 집단에 특히 해로운 출력물이 생산될 것으로 예상하지 않는다고 밝힙니다​​​​.
"8 Environmental Considerations (환경적 고려사항)" 부분에서는 이 연구가 환경에 미치는 영향을 다룹니다. 이 부분의 주요 내용은 다음과 같습니다:

연구팀은 (Du et al. 2021)에서 언급된 사전 훈련된 언어 모델을 사용했으며, 이 모델의 에너지 비용과 탄소 발자국은 사전 훈련 동안 각각 80.6 MWh와 4.6 tCO2e였음을 언급합니다.
구조 사전 훈련은 언어 모델의 사전 훈련 단계의 5% 미만의 그라디언트 스텝을 사용하므로, 추가적인 에너지 비용은 상대적으로 적습니다.
다양한 작업 및 데이터셋에서 사전 훈련된 언어 모델을 훈련하고 조정하는 것은 많은 에너지를 소비하고 이산화탄소 배출량을 증가시킵니다. 이러한 문제를 완화하기 위해, 연구팀은 모든 데이터셋에서 한 번에 훈련하는 다중 작업 훈련을 연구합니다.
특히, 모델 크기가 10억 매개변수로 확장되면 성능 격차가 줄어듦을 보여주며, 이는 대규모 사전 훈련된 모델을 훈련시킬 때 다중 작업 훈련을 사용함으로써 에너지 소비를 줄일 수 있음을 나타냅니다​​​​.



# 8 Environmental Considerations  
사전 훈련된 언어 모델을 다양한 작업과 데이터셋에서 훈련하고 조정하는 과정은 많은 에너지를 소비하고 이산화탄소 배출을 증가시킵니다.
이러한 문제를 완화하기 위해, 연구팀은 한 번에 모든 데이터셋에 대한 훈련을 포함하는 다중 작업 훈련을 연구합니다.
그들의 연구 결과는 모델 크기가 10억 매개변수로 확장될 때, 작은 모델에서의 다중 작업과 다중 작업 미세 조정 간의 성능 격차가 줄어든다는 것을 보여줍니다. 이는 대규모 사전 훈련된 모델을 훈련할 때 다중 작업 훈련을 사용함으로써 에너지 소비를 줄일 수 있음을 나타냅니다​​.  
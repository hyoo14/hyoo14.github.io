---
layout: post
title:  "[2025]Energy-Based Transformers Are Scalable Learners and Thinkers"  
date:   2025-07-12 06:37:40 -0400
categories: study
---

{% highlight ruby %}


í•œì¤„ ìš”ì•½: 



ì§§ì€ ìš”ì•½(Abstract) :    

ì´ ë…¼ë¬¸ì€ ì¸ê°„ì˜ ëŠë¦¬ê³  ì‹ ì¤‘í•œ ì‚¬ê³  ë°©ì‹ì¸ "ì‹œìŠ¤í…œ 2 ì‚¬ê³ (System 2 Thinking)"ë¥¼ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì— ì ìš©í•˜ë ¤ëŠ” ì‹œë„ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì ‘ê·¼ ë°©ì‹ë“¤ì€ í…ìŠ¤íŠ¸ë‚˜ ìˆ˜í•™, ì½”ë”© ë“± íŠ¹ì • ë¶„ì•¼ì—ë§Œ ì œí•œë˜ê±°ë‚˜, ì¶”ê°€ í•™ìŠµì´ë‚˜ ë³´ì¡° ëª¨ë¸ì„ ìš”êµ¬í•˜ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì €ìë“¤ì€ ì™„ì „íˆ ë¹„ì§€ë„ í•™ìŠµë§Œìœ¼ë¡œ ì‹œìŠ¤í…œ 2 ì‚¬ê³ ê°€ ê°€ëŠ¥í•œì§€ë¥¼ ì§ˆë¬¸í•˜ê³ , ì´ì— ëŒ€í•œ í•´ë‹µìœ¼ë¡œ **Energy-Based Transformers (EBTs)**ë¼ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.

EBTëŠ” ì…ë ¥ê³¼ í›„ë³´ ì˜ˆì¸¡ ìŒì— ëŒ€í•´ ì—ë„ˆì§€(ì •ê·œí™”ë˜ì§€ ì•Šì€ í™•ë¥ ê°’)ë¥¼ ë¶€ì—¬í•˜ê³ , ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°™ì€ ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì—ì„œë„ ë¬¸ì œì— ë§ì¶° ê³„ì‚° ìì›ì„ ìœ ì—°í•˜ê²Œ í• ë‹¹í•˜ë©°, ë” ì–´ë ¤ìš´ ë¬¸ì œì— ëŒ€í•´ ë” ë§ì´ "ìƒê°"í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

ì‹¤í—˜ ê²°ê³¼, EBTëŠ” ê¸°ì¡´ Transformer++ ëª¨ë¸ë³´ë‹¤ ë” ë¹ ë¥¸ í•™ìŠµ í™•ì¥ ì†ë„ë¥¼ ë³´ì˜€ê³ , ì¶”ë¡  ì‹œ ì‹œìŠ¤í…œ 2 ë°©ì‹(ì¦‰, ë°˜ë³µ ê³„ì‚°)ì„ ì‚¬ìš©í•  ê²½ìš° ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, í›ˆë ¨ ë°ì´í„°ì—ì„œ ë²—ì–´ë‚œ(out-of-distribution) ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ë” ë›°ì–´ë‚œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

ê²°ë¡ ì ìœ¼ë¡œ, EBTëŠ” í•™ìŠµê³¼ ì‚¬ê³  ëª¨ë‘ì—ì„œ í™•ì¥ ê°€ëŠ¥í•œ ë§¤ìš° ìœ ë§í•œ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



This paper investigates whether System 2 Thinkingâ€”a deliberate, reasoning-based computation styleâ€”can emerge from purely unsupervised learning, without additional supervision or task-specific fine-tuning. The authors introduce Energy-Based Transformers (EBTs), a new type of Energy-Based Model (EBM) that learns to assign unnormalized energy values to inputâ€“prediction pairs. Predictions are generated by minimizing this energy via gradient descent, enabling the model to â€œthink longerâ€ and allocate computation dynamically.

Unlike prior methods limited by modality or task, EBTs generalize across both discrete (text) and continuous (visual) modalities. They scale better than Transformer++ modelsâ€”up to 35% faster across data, model size, and computation. Inference-time â€œSystem 2 Thinkingâ€ (i.e., extra reasoning steps) boosts EBT performance by 29% more than Transformer++ on language tasks and yields better image denoising results than diffusion models with 99% fewer forward passes.

Importantly, EBTs show stronger generalization on out-of-distribution data despite similar or worse pretraining performance, suggesting they are better thinkers and learners. This positions EBTs as a promising paradigm for scaling both learning and reasoning in future AI systems.



* Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()   

<br/>

# ë‹¨ì–´ì •ë¦¬  
*  







 
<br/>
# Methodology    




---


ì´ ë…¼ë¬¸ì€ \*\*Energy-Based Transformer (EBT)\*\*ë¼ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì „í†µì ì¸ Transformerì˜ í•œê³„â€”ì •ì ì¸ ì—°ì‚°, ë¶ˆí™•ì‹¤ì„± ì¶”ë¡  ë¶ˆê°€ëŠ¥, ì˜ˆì¸¡ ê²€ì¦ ë¶€ì¬â€”ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ \*\*ì—ë„ˆì§€ ê¸°ë°˜ ëª¨ë¸ (Energy-Based Model; EBM)\*\*ì…ë‹ˆë‹¤.

###  ì£¼ìš” êµ¬ì„± ìš”ì†Œ

1. **ëª¨ë¸ êµ¬ì¡° (Architecture)**:

   * **Decoder-only EBT**: GPTì²˜ëŸ¼ í•œ ë°©í–¥(Autoregressive)ìœ¼ë¡œ ë™ì‘í•˜ë©°, ë¬¸ì¥ ìƒì„± ë“±ì— ì í•©.
   * **Bidirectional EBT**: BERT ë˜ëŠ” Diffusion Transformerì²˜ëŸ¼ ì–‘ë°©í–¥ìœ¼ë¡œ ì •ë³´ë¥¼ ì£¼ê³ ë°›ìœ¼ë©° ì´ë¯¸ì§€ ë³µì› ë“±ì— ì í•©.
   * ë‘ êµ¬ì¡° ëª¨ë‘ ì…ë ¥â€“ì˜ˆì¸¡ ìŒì— ëŒ€í•´ ì—ë„ˆì§€ ê°’ì„ ë¶€ì—¬í•¨ìœ¼ë¡œì¨ ê·¸ í˜¸í™˜ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.

2. **í•™ìŠµ ë°©ì‹ (Training Objective)**:

   * ì´ˆê¸° ì˜ˆì¸¡ê°’(ì˜ˆ: ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆ)ì—ì„œ ì¶œë°œí•˜ì—¬, ì—ë„ˆì§€ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ë°˜ë³µì ìœ¼ë¡œ ì˜ˆì¸¡ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ (gradient descent).
   * ì†ì‹¤ í•¨ìˆ˜ëŠ” ê¸°ì¡´ Transformerì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤:

     * í…ìŠ¤íŠ¸: Cross-Entropy
     * ì´ë¯¸ì§€: Mean Squared Error

3. **í•™ìŠµ ê¸°ë²• ë° ì•ˆì •í™” ê¸°ë²•**:

   * **Replay Buffer**: ì˜ˆì „ ì˜ˆì¸¡ ê²½ë¡œë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ëŠ” ë²„í¼ë¡œ ì•ˆì •ì ì¸ ì—ë„ˆì§€ í•™ìŠµ ìœ ë„
   * **Langevin Dynamics**: ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ íƒìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥
   * **ëœë¤ ìŠ¤í… í¬ê¸°/íšŸìˆ˜**: ë‹¤ì–‘í•œ ê²½ë¡œ í•™ìŠµì„ í†µí•´ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

4. **ì¶”ë¡  ë°©ë²• (Inference / Thinking Process)**:

   * **Thinking Longer**: ë” ë§ì€ gradient descent step ìˆ˜í–‰
   * **Self-Verification**: ì—¬ëŸ¬ ì˜ˆì¸¡ í›„ë³´ë¥¼ ìƒì„± í›„ ê°€ì¥ ì—ë„ˆì§€ê°€ ë‚®ì€ ê²ƒì„ ì„ íƒ

5. **í•™ìŠµ ë°ì´í„°ì™€ ì‘ì—… (Training Data & Tasks)**:

   * **í…ìŠ¤íŠ¸**: RedPajamaV2 ë°ì´í„°ì…‹ 100B ìƒ˜í”Œ (Autoregressive Language Modeling)
   * **ë¹„ë””ì˜¤**: Something-Something V2 (ë‹¤ìŒ í”„ë ˆì„ ì˜ˆì¸¡)
   * **ì´ë¯¸ì§€**: COCO 2014 (ì´ë¯¸ì§€ ë…¸ì´ì§• ë³µì›)

ì´ ë°©ì‹ì€ ê¸°ì¡´ Transformer++ ëŒ€ë¹„ ë” ë›°ì–´ë‚œ í™•ì¥ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆê³ , íŠ¹íˆ ì¶”ë¡  ì‹œ "ìƒê°í•˜ëŠ”" ê³¼ì •ì—ì„œ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë˜ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.

---

### ğŸ‡ºğŸ‡¸ English Version

This paper proposes **Energy-Based Transformers (EBTs)**, a novel model class that integrates the principles of **Energy-Based Models (EBMs)** into Transformer architectures to enable **System 2 Thinking**â€”deliberate, iterative reasoning through energy minimization.

###  Key Components

1. **Model Architecture**:

   * **Decoder-only EBT**: Inspired by GPT, used for autoregressive tasks like next-token prediction.
   * **Bidirectional EBT**: Similar to BERT or Diffusion Transformers, used for denoising or bidirectional tasks.
   * Both assign a scalar **energy value** to inputâ€“prediction pairs, acting as verifiers of compatibility.

2. **Training Objective**:

   * Training simulates a **thinking process**: starting from a random prediction, the model updates it iteratively using **gradient descent** to minimize energy.
   * Objective functions:

     * **Cross-entropy** for language modeling
     * **Mean squared error** for image denoising

3. **Training Stability Techniques**:

   * **Replay Buffer**: Stores past trajectories to stabilize learning.
   * **Langevin Dynamics**: Adds Gaussian noise during optimization to encourage exploration.
   * **Random step size and number of steps**: Enhances generalization by diversifying update paths.

4. **Inference Strategy (Thinking Process)**:

   * **Thinking Longer**: More optimization steps during inference.
   * **Self-Verification**: Generates multiple candidates and selects the one with lowest energy.

5. **Training Data and Tasks**:

   * **Text**: RedPajamaV2 (100B samples), used for autoregressive language modeling.
   * **Video**: Something-Something V2, used for next-frame prediction.
   * **Image**: COCO 2014, used for image denoising.

This method demonstrates better scaling trends than Transformer++, and its inference-time thinking mechanism further enhances generalizationâ€”especially for **out-of-distribution (OOD)** data.

---





   
 
<br/>
# Results  


ì´ ë…¼ë¬¸ì—ì„œëŠ” EBT(Energy-Based Transformer)ì˜ ì„±ëŠ¥ì„ ë‹¤ì–‘í•œ \*\*ëª¨ë‹¬ë¦¬í‹°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤)\*\*ì™€ \*\*ì‘ì—…(task)\*\*ì—ì„œ ê¸°ì¡´ì˜ ê°•ë ¥í•œ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ ê²€ì¦í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ **í•™ìŠµ í™•ì¥ì„±**ê³¼ **ì‚¬ê³ (Thinking) í™•ì¥ì„±**, **OOD(Out-of-Distribution) ì¼ë°˜í™”**ë¥¼ ì¤‘ì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.

---

###  ì£¼ìš” ë¹„êµ ëª¨ë¸

* **Transformer++**: í‘œì¤€ GPT-ìœ í˜• ëª¨ë¸ (Autoregressive ê¸°ë°˜)
* **Diffusion Transformer (DiT)**: ì´ë¯¸ì§€ ë³µì›ì—ì„œ ë¹„êµë¨
* **ê¸°íƒ€**: RNN, ê¸°ì¡´ EBM (ë¹„êµ ë°°ê²½ìš©)

---

###  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ì‘ì—…

| ëª¨ë‹¬ë¦¬í‹° | ì‘ì—…           | ë°ì´í„°ì…‹                                                               |
| ---- | ------------ | ------------------------------------------------------------------ |
| í…ìŠ¤íŠ¸  | ì–¸ì–´ ëª¨ë¸ë§, ì¶”ë¡    | RedPajamaV2 (100B), GSM8K, SQuAD, BigBench Math QA, Dyck Languages |
| ë¹„ë””ì˜¤  | ë‹¤ìŒ í”„ë ˆì„ ì˜ˆì¸¡    | Something-Something V2                                             |
| ì´ë¯¸ì§€  | ì´ë¯¸ì§€ ë””ë…¸ì´ì§•, ë¶„ë¥˜ | COCO 2014, ImageNet-1k                                             |

---

###  ì£¼ìš” ê²°ê³¼ ìš”ì•½

1. **í•™ìŠµ í™•ì¥ì„± (Learning Scalability)**
   EBTëŠ” ë°ì´í„° í¬ê¸°, íŒŒë¼ë¯¸í„° ìˆ˜, FLOPs, ëª¨ë¸ ê¹Šì´ ë“± ëª¨ë“  ë©´ì—ì„œ Transformer++ë³´ë‹¤ **ìµœëŒ€ 35% ë” ë¹ ë¥´ê²Œ í™•ì¥**ë¨.

2. **ì‚¬ê³ (Thinking) í™•ì¥ì„± (System 2 Thinking)**
   ì¶”ë¡  ì‹œê°„ ë™ì•ˆ ë” ë§ì´ "ìƒê°"í• ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒë¨.

   * ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ **29% ì¶”ê°€ í–¥ìƒ** (Thinking Longer + Self-Verification)
   * EBTëŠ” Transformer++ì™€ ë‹¬ë¦¬ **í† í° ë‹¨ìœ„ë¡œ ë” ë§ì€ ì—°ì‚° ì‹œ ì„±ëŠ¥ ì¦ê°€**

3. **OOD ì¼ë°˜í™”**

   * í›ˆë ¨ ë°ì´í„°ì™€ ë‹¤ë¥¸ ë¶„í¬ì˜ ë°ì´í„°ì—ì„œë„ ì„±ëŠ¥ ìœ ì§€ ë˜ëŠ” í–¥ìƒë¨.
   * ì˜ˆ: Dyck Languagesì™€ ê°™ì€ ë³µì¡í•œ ë¬¸ë²• ë°ì´í„°ì—ì„œ **Thinkingì„ í• ìˆ˜ë¡ ì„±ëŠ¥ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€**

4. **ì´ë¯¸ì§€ ë³µì› ë° ë¶„ë¥˜ ì„±ëŠ¥**

   * EBTëŠ” DiT ëŒ€ë¹„ **99% ì ì€ ê³„ì‚°ëŸ‰**ìœ¼ë¡œ ë” ë‚˜ì€ ì´ë¯¸ì§€ ë””ë…¸ì´ì§• ì„±ëŠ¥ (PSNR ë° MSE ê¸°ì¤€)
   * ImageNet ë¶„ë¥˜ì—ì„œ \*\*Top-1 ì •í™•ë„ 5.32%, Top-5 ì •í™•ë„ 13.2%\*\*ë¡œ DiT ëŒ€ë¹„ ì•½ **10ë°° í–¥ìƒ**

---


In the **Results** section, the authors evaluate the performance of Energy-Based Transformers (EBTs) across **text, video, and image tasks**, focusing on **learning scalability**, **System 2 Thinking**, and **out-of-distribution (OOD) generalization**.

---

###  Baseline / Competing Models

* **Transformer++**: Strong GPT-style baseline for language modeling.
* **Diffusion Transformer (DiT)**: Compared on image denoising.
* Others like RNNs or prior EBMs are mentioned for architectural background.

---

###  Test Datasets & Tasks

| Modality | Task                         | Dataset                                                     |
| -------- | ---------------------------- | ----------------------------------------------------------- |
| Text     | Language modeling, reasoning | RedPajamaV2, GSM8K, SQuAD, BigBench Math QA, Dyck Languages |
| Video    | Next-frame prediction        | Something-Something V2                                      |
| Image    | Denoising, classification    | COCO 2014, ImageNet-1k                                      |

---

### Key Results

1. **Learning Scalability**
   EBTs outperform Transformer++ across **all axes** (data, parameters, FLOPs, depth), achieving up to **35% faster scaling**.

2. **System 2 Thinking (Inference-Time Reasoning)**

   * Improves performance significantly during inference:

     * Up to **+29% gain** on text tasks using **Thinking Longer + Self-Verification**.
   * Unlike Transformer++, EBTs **improve per-token performance** with more computation.

3. **Out-of-Distribution Generalization**

   * EBTs generalize better to **OOD datasets**, such as BigBench Dyck Languages.
   * **Linear trend**: More OOD â†’ More gains from "thinking".

4. **Image Denoising and Classification**

   * EBTs outperform DiTs in **PSNR and MSE** using **99% fewer forward passes**.
   * On ImageNet-1k, EBTs achieve **Top-1 accuracy: 5.32%, Top-5: 13.2%**, which is nearly **10Ã— higher** than DiTs.

---





<br/>
# ì˜ˆì œ  




####  ì˜ˆì‹œ 1. **í›ˆë ¨ ë°ì´í„° ì˜ˆì‹œ (Autoregressive Language Modeling)**

* **ë°ì´í„°ì…‹**: RedPajamaV2 (100B ìƒ˜í”Œ)
* **í˜•ì‹**: ì˜ì–´ ë¬¸ì„œ í…ìŠ¤íŠ¸, ì˜ˆ:
  `"The quick brown fox jumps over the lazy dog."`
* **EBT ë™ì‘**:
  EBTëŠ” ë‹¤ìŒ í† í° `"jumps"`ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì²˜ìŒì—ëŠ” ë¬´ì‘ìœ„ ë¶„í¬ì—ì„œ ì‹œì‘í•´, ì—ë„ˆì§€ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ì ì°¨ `"jumps"`ë¡œ ìˆ˜ë ´í•¨.
* **íŠ¹ì§•**:
  ì‰¬ìš´ ë‹¨ì–´(ì˜ˆ: "the", "is")ëŠ” ë¹ ë¥´ê²Œ ì—ë„ˆì§€ê°€ ìˆ˜ë ´í•˜ì§€ë§Œ, ì˜ˆì¸¡ì´ ì–´ë ¤ìš´ ë‹¨ì–´("fox", "problem")ëŠ” ë†’ì€ ì—ë„ˆì§€ ê°’ì„ ìœ ì§€í•˜ë©´ì„œ ëŠë¦¬ê²Œ ìˆ˜ë ´í•¨ â†’ **ë¶ˆí™•ì‹¤ì„± í•™ìŠµ**

####  ì˜ˆì‹œ 2. **í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì‹œ (Out-of-Distribution Reasoning Task)**

* **ë°ì´í„°ì…‹**: BigBench Dyck Languages (ë¬¸ë²•ì ìœ¼ë¡œ ì¤‘ì²©ëœ ê´„í˜¸ ì˜ˆì¸¡)
* **ë¬¸ì œ ì˜ˆì‹œ**:
  ì…ë ¥ ë¬¸ìì—´: `"([[]([])])"`
* **í…ŒìŠ¤í¬**:
  ëª¨ë¸ì´ ì´ ì‹œí€€ìŠ¤ê°€ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€ë¥¼ íŒë³„í•˜ê±°ë‚˜ ë‹¤ìŒ ê´„í˜¸ë¥¼ ì˜ˆì¸¡
* **EBT ê²°ê³¼**:

  * Transformer++ëŠ” ì¼ë°˜ì ì¸ ì‹œí€€ìŠ¤ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ì¤‘ì²© êµ¬ì¡°ì— ëŒ€í•´ ì„±ëŠ¥ í•˜ë½
  * EBTëŠ” **System 2 Thinking**(ìƒê°í•˜ëŠ” ë‹¨ê³„ ìˆ˜ ì¦ê°€, í›„ë³´ ì¤‘ ê²€ì¦)ì„ í†µí•´ ë” ë‚˜ì€ ì„±ëŠ¥ ë„ì¶œ
  * Thinking stepì„ ëŠ˜ë¦´ìˆ˜ë¡ ì •í™•ë„ ê°œì„ 

####  ì˜ˆì‹œ 3. **ì´ë¯¸ì§€/ë¹„ë””ì˜¤ í…ŒìŠ¤í¬ ì˜ˆì‹œ (ë¹„ë””ì˜¤ í”„ë ˆì„ ì˜ˆì¸¡)**

* **ë°ì´í„°ì…‹**: Something-Something V2
* **ìƒí™© ì˜ˆì‹œ**:
  ì´ˆê¸° í”„ë ˆì„ì—ì„œ ì‚¬ëŒì´ ì˜·ì„ ë“¤ê³  ìˆìŒ â†’ ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ì˜·ì´ ë” ëª…í™•í•˜ê²Œ ë³´ì¼ì§€ ì˜ˆì¸¡
* **EBT íŠ¹ì§•**:

  * ì´ˆë°˜ í”„ë ˆì„ì—ëŠ” ë†’ì€ ì—ë„ˆì§€ â†’ ë¶ˆí™•ì‹¤ì„± ë†’ìŒ
  * ì˜·ì´ ë“œëŸ¬ë‚ ìˆ˜ë¡ ì—ë„ˆì§€ê°€ ë‚®ì•„ì§ â†’ ëª¨ë¸ì´ ë” í™•ì‹ ì„ ê°€ì§

---


###  Example 1. **Training Data Example (Autoregressive Language Modeling)**

* **Dataset**: RedPajamaV2 (100B samples)
* **Input**: A sentence like:
  `"The quick brown fox jumps over the lazy dog."`
* **EBT behavior**:
  To predict the next token `"jumps"`, the model starts from a random distribution and iteratively minimizes the energy until it converges to the correct token.
* **Key observation**:
  Simple words like `"the"` or `"is"` quickly reach low energy, while harder tokens like `"fox"` or `"problem"` converge more slowly â†’ reflects **uncertainty modeling**.

###  Example 2. **Test Data Example (Out-of-Distribution Reasoning Task)**

* **Dataset**: BigBench Dyck Languages
* **Input example**: A sequence like `"([[]([])])"`
* **Task**:
  Determine whether the nested brackets are grammatically correct or predict the next bracket.
* **EBT results**:

  * Transformer++ performs poorly on deeply nested structures.
  * EBT improves performance using **System 2 Thinking** (e.g., multiple steps + self-verification).
  * More thinking steps = better accuracy.

###  Example 3. **Video Task Example (Next Frame Prediction)**

* **Dataset**: Something-Something V2
* **Scenario**:
  A person begins lifting a blue garment into view.
* **EBT behavior**:

  * Early frames have high energy (high uncertainty).
  * As the object becomes visible, energy decreases (model becomes more confident).
* **Key takeaway**:
  EBT learns to model uncertainty naturally without supervision in continuous visual scenes.

---




<br/>  
# ìš”ì•½   



Energy-Based Transformer(EBT)ëŠ” ì…ë ¥-ì˜ˆì¸¡ ìŒì˜ í˜¸í™˜ì„±(ì—ë„ˆì§€)ì„ í•™ìŠµí•˜ì—¬, ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡  ì¤‘ 'ìƒê°í•˜ëŠ” ê³¼ì •(System 2 Thinking)'ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ì‹¤í—˜ ê²°ê³¼, EBTëŠ” ê¸°ì¡´ Transformer++ë³´ë‹¤ í•™ìŠµ í™•ì¥ì„±ê³¼ ì¶”ë¡  ì„±ëŠ¥ì—ì„œ ìš°ìˆ˜í•˜ë©°, íŠ¹íˆ OOD ë°ì´í„°ì— ëŒ€í•´ ë” ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ ì¤‘ì²© ê´„í˜¸ë¥¼ íŒë³„í•˜ëŠ” BigBench Dyck taskì—ì„œ EBTëŠ” ìƒê° ë‹¨ê³„ë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ ì •í™•ë„ê°€ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.


Energy-Based Transformers (EBTs) learn to assign energy to inputâ€“prediction pairs and perform reasoning by minimizing this energy through iterative inference.
Experiments show that EBTs outperform Transformer++ in both training scalability and inference-time performance, especially on out-of-distribution data.
For example, on the BigBench Dyck task involving nested brackets, EBT accuracy improves as the number of thinking steps increases.



<br/>  
# ê¸°íƒ€  


---


---

###  Figure 1: Autoregressive Architecture ë¹„êµ

* **ë‚´ìš©**: ê¸°ì¡´ AR Transformer, RNN, Diffusion Transformer, EBT ì•„í‚¤í…ì²˜ ë¹„êµ
* **ì¸ì‚¬ì´íŠ¸**:

  * EBTëŠ” Diffusion ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, noiseê°€ ì•„ë‹ˆë¼ energyë¥¼ ì§ì ‘ ì˜ˆì¸¡í•˜ë©° **ëª…ì‹œì  ê²€ì¦ ê¸°ëŠ¥ì´ ìˆìŒ**
  * ê¸°ì¡´ ëª¨ë¸ì€ ë™ì  ê³„ì‚°, ë¶ˆí™•ì‹¤ì„± ì¶”ë¡ , ê²€ì¦ ê¸°ëŠ¥ ì¤‘ ì¼ë¶€ë§Œ ê°€ëŠ¥í•˜ì§€ë§Œ, **EBTëŠ” ëª¨ë‘ ê°€ëŠ¥í•¨**

---

###  Figure 2 & 3: Thinking ê³¼ì • ì‹œê°í™”

* **ë‚´ìš©**:

  * Figure 2: ë§¤ stepë§ˆë‹¤ ì˜ˆì¸¡ì„ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ energyë¥¼ ì¤„ì´ëŠ” êµ¬ì¡°
  * Figure 3: energy landscapeì—ì„œ gradient descentë¥¼ í†µí•´ ì˜ˆì¸¡ì´ ìˆ˜ë ´í•˜ëŠ” ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„
* **ì¸ì‚¬ì´íŠ¸**:

  * **Thinking = Optimization** ê°œë…ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ
  * **ë¶ˆí™•ì‹¤ì„±**ì´ ë†’ì€ ê²½ìš° ìˆ˜ë ´ì´ ëŠë¦¬ê³ , ì—ë„ˆì§€ê°€ ë†’ê²Œ ìœ ì§€ë¨ â†’ ì¶”ë¡ ì— ë” ë§ì€ ë‹¨ê³„ í•„ìš”

---

###  Table 1: êµ¬ì¡°ë³„ ì¸ì§€ì  ê¸°ëŠ¥ ë¹„êµ

* **ë‚´ìš©**: ë‹¤ì–‘í•œ ëª¨ë¸ì´ Cognitive Facet 1\~3 (ë™ì  ì—°ì‚°, ë¶ˆí™•ì‹¤ì„± ì¶”ë¡ , ê²€ì¦) ê¸°ëŠ¥ì„ ê°€ì§€ëŠ”ì§€ ì—¬ë¶€
* **ì¸ì‚¬ì´íŠ¸**:

  * ê¸°ì¡´ Transformer, RNN, Diffusionì€ ì¼ë¶€ë§Œ ì§€ì›
  * EBTë§Œì´ ì„¸ ê°€ì§€ ê¸°ëŠ¥ ëª¨ë‘ ì¶©ì¡±

---

###  Table 2: Thinking ê¸°ë²• ë³„ ì„±ëŠ¥ (Ablation)

* **ë‚´ìš©**: Langevin Dynamics, Replay Buffer, Random Step ë“±ì„ ì œê±°í–ˆì„ ë•Œ ì„±ëŠ¥ ë¹„êµ
* **ì¸ì‚¬ì´íŠ¸**:

  * ëª¨ë“  regularization ê¸°ë²•ì„ í•¨ê»˜ ì“¸ ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥
  * ëœë¤ ìŠ¤í… í¬ê¸° ì œê±° ì‹œ ì„±ëŠ¥ ê±°ì˜ ì†ì‹¤ â†’ **ë‹¤ì–‘í•œ ê²½ë¡œ íƒìƒ‰ì´ ì¤‘ìš”í•¨**

---

###  Figure 4â€“5: í•™ìŠµ í™•ì¥ì„± (Scalability)

* **ë‚´ìš©**: ë°ì´í„° í¬ê¸°, batch size, depth, FLOPs, íŒŒë¼ë¯¸í„° ìˆ˜ ë“±ì—ì„œì˜ í•™ìŠµ ì†ë„ ë¹„êµ
* **ì¸ì‚¬ì´íŠ¸**:

  * EBTê°€ Transformer++ë³´ë‹¤ **ìµœëŒ€ 35% ë” ë¹ ë¥¸ í™•ì¥ë¥ **ì„ ê°€ì§
  * íŠ¹íˆ \*\*ê¹Šì´(deep depth)\*\*ì—ì„œ í° ì°¨ì´ë¥¼ ë³´ì—¬ reasoningì— ìœ ë¦¬í•¨

---

###  Figure 6â€“7: Thinking ì„±ëŠ¥ ë° OOD ì¼ë°˜í™”

* **ë‚´ìš©**: Thinking step ì¦ê°€ ì‹œ ì„±ëŠ¥ ê°œì„ , OOD ë°ì´í„°ì¼ìˆ˜ë¡ ë” í° í–¥ìƒ
* **ì¸ì‚¬ì´íŠ¸**:

  * **System 2 Thinkingì´ OOD ì¼ë°˜í™”ì— íš¨ê³¼ì **
  * ìƒê° ë‹¨ê³„ë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ ì •í™•ë„ê°€ ì„ í˜•ì ìœ¼ë¡œ í–¥ìƒë¨

---

###  Figure 8 & 11: ë¶ˆí™•ì‹¤ì„± ì‹œê°í™” (í…ìŠ¤íŠ¸, ë¹„ë””ì˜¤)

* **ë‚´ìš©**: ì‰¬ìš´ ë‹¨ì–´("the")ëŠ” ë¹ ë¥´ê²Œ ì—ë„ˆì§€ ìˆ˜ë ´, ì–´ë ¤ìš´ ë‹¨ì–´("problem")ëŠ” ë†’ì€ ì—ë„ˆì§€ ìœ ì§€
* **ì¸ì‚¬ì´íŠ¸**:

  * EBTëŠ” **ë¶ˆí™•ì‹¤ì„± ì •ë„ë¥¼ token/frame ë‹¨ìœ„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµ**í•¨

---

###  Figure 9 & Table 4: ë¹„ë””ì˜¤/ì´ë¯¸ì§€ ì‹¤í—˜

* **ë‚´ìš©**: ì´ë¯¸ì§€ ë””ë…¸ì´ì§•ì—ì„œ EBT vs DiT ì„±ëŠ¥ ë¹„êµ
* **ì¸ì‚¬ì´íŠ¸**:

  * EBTëŠ” **99% ì ì€ forward passë¡œ DiTë³´ë‹¤ PSNR, MSE ì„±ëŠ¥ ìš°ìˆ˜**
  * ImageNet ë¶„ë¥˜ ì •í™•ë„ë„ 10ë°° ì´ìƒ ë†’ìŒ â†’ **ì´ë¯¸ì§€ í‘œí˜„ í•™ìŠµì—ë„ ê°•ë ¥**

---

###  Figure 10: OOD ì´ë¯¸ì§€ ë³µì› í’ˆì§ˆ

* **ë‚´ìš©**: ì‹œê°ì ìœ¼ë¡œ DiTë³´ë‹¤ EBTì˜ ë³µì› ì´ë¯¸ì§€ê°€ ë” ì„ ëª…í•¨
* **ì¸ì‚¬ì´íŠ¸**:

  * ì ì€ ì—°ì‚°ìœ¼ë¡œë„ **ë” ë‚˜ì€ ì§ˆì˜ ì´ë¯¸ì§€ ìƒì„± ê°€ëŠ¥**

---

###  Figure 12: PSNR ì„±ëŠ¥ vs Thinking Steps

* **ë‚´ìš©**: ì´ë¯¸ì§€ ë””ë…¸ì´ì§•ì—ì„œ thinking step ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ PSNR í–¥ìƒ
* **ì¸ì‚¬ì´íŠ¸**:

  * **ì¶”ë¡  ì‹œê°„ ë™ì•ˆ ë” ìƒê°í• ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ**ë¨ â†’ System 2 Thinking íš¨ê³¼ í™•ì¸

---

###  ë¶€ë¡ (Appendix Sections C, D, E, etc.)

* **Section C**: System 2 Thinking ì •ì˜ ë° ì•Œê³ ë¦¬ì¦˜ ì •ë¦¬
* **Section D**: í•˜ì´í¼íŒŒë¼ë¯¸í„°, FLOP ê³„ì‚° ë°©ì‹ ë“± ì‹¤í—˜ ì„¸ë¶€
* **Section E**: Diffusion ëª¨ë¸ê³¼ EBT ë¹„êµ ë¶„ì„
* **Section F**: System 2 Thinking ì¶”ê°€ facet ì œì•ˆ
* **Section H**: ì´ˆë³´ììš© EBM ê°œìš”ì™€ pseudocode ì œê³µ

---


---

###  Figure 1: Architecture Comparison

* Compares AR Transformers, RNNs, Diffusion Transformers, and EBTs
* **Insight**: Only EBTs support all three cognitive facetsâ€”dynamic computation, uncertainty modeling, and prediction verification.

---

###  Figures 2â€“3: Thinking Process

* Visualizes prediction refinement via energy minimization
* **Insight**: Models iterate predictions like humans thinkâ€”uncertainty slows convergence.

---

###  Table 1: Cognitive Facets Comparison

* Shows which architectures support dynamic compute, uncertainty, and verification
* **Insight**: EBTs uniquely support all three.

---

###  Table 2: Ablation Study on Thinking

* Removes Langevin, Replay Buffer, etc., and compares performance
* **Insight**: Full combination performs best; random step size is especially critical.

---

###  Figures 4â€“5: Learning Scalability

* EBT outpaces Transformer++ in data, batch, depth, FLOPs, and parameter scaling
* **Insight**: EBTs scale **up to 35% faster**, especially in deep models â†’ better for reasoning.

---

###  Figures 6â€“7: Thinking & Generalization

* More inference steps â†’ better performance, especially on OOD data
* **Insight**: **System 2 Thinking improves generalization**, showing linear performance gains.

---

###  Figures 8 & 11: Uncertainty Visualization

* EBT assigns higher energy to uncertain tokens/frames
* **Insight**: Learns token/frame-level uncertainty **without supervision**.

---

###  Figure 9 & Table 4: Image/Video Results

* EBT beats DiT in PSNR, MSE, and classification with **99% fewer steps**
* **Insight**: Stronger representations and generalization in continuous domains.

---

###  Figure 10: OOD Image Quality

* EBT produces clearer restored images than DiT
* **Insight**: Better visual quality with far less compute.

---

###  Figure 12: PSNR vs Inference Steps

* More steps â†’ higher PSNR
* **Insight**: Confirms that **thinking longer improves prediction quality**.

---

###  Appendix

* **Section C**: Defines System 2 Thinking and formal algorithms
* **Section D**: Hyperparameters, FLOP calculations
* **Section E**: EBT vs Diffusion models
* **Section F**: Additional cognitive facets
* **Section H**: EBM introduction and pseudocode




<br/>
# refer format:     



@article{gladstone2025energy,
  title={Energy-Based Transformers are Scalable Learners and Thinkers},
  author={Gladstone, Alexi and Nanduru, Ganesh and Islam, Md Mofijul and Han, Peixuan and Ha, Hyeonjeong and Chadha, Aman and Du, Yilun and Ji, Heng and Li, Jundong and Iqbal, Tariq},
  journal={arXiv preprint arXiv:2507.02092},
  year={2025},
  url={https://arxiv.org/abs/2507.02092}
}
  


Gladstone, Alexi, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, and Tariq Iqbal. 2025. â€œEnergy-Based Transformers Are Scalable Learners and Thinkers.â€ arXiv preprint arXiv:2507.02092. https://arxiv.org/abs/2507.02092.





---
layout: post
title:  "[2023]TIES-MERGING: Resolving Interference When Merging Models"  
date:   2024-03-24 17:05:29 -0400
categories: study
---

{% highlight ruby %}


한줄 요약: 

짧은 요약(Abstract) :    
* 전이 학습, 즉 사전 훈련된 모델을 하위 작업에 추가로 미세 조정하는 것은 개선된 하류 작업 성능, 더 빠른 수렴, 그리고 더 나은 샘플 효율성을 포함한 상당한 이점을 제공할 수 있음  
* 이러한 이점으로 인해 단일 작업만 수행할 수 있는 작업별로 미세 조정된 모델이 대량으로 생산되었으며, 이들은 서로로부터 이득을 얻지 못함  
* 최근에는 다양한 작업별 모델을 단일 다중 작업 모델로 결합하는 솔루션으로 모델 병합 기술이 등장했으며, 이는 추가 훈련을 수행하지 않고도 가능함  
* 그러나 기존의 병합 방법은 대부분 다른 모델의 매개변수 간 간섭을 무시하며, 이는 여러 모델을 병합할 때 성능이 크게 저하되는 결과를 초래함  
* 본 논문에서는 이전 병합 기술이 두 가지 주요 간섭 원인으로 인해 중요한 정보를 부주의하게 잃어버린다는 것을 보임: (a) 중복된 매개변수 값으로 인한 간섭과 (b) 모델 간 특정 매개변수 값의 부호에 대한 불일치  
* 이를 해결하기 위해 본 논문에서는 모델을 병합할 때 세 가지 새로운 단계를 도입하는 TRIM ELECT SIGN & MERGE (TIES-MERGING) 방법을 제안함: (1) 미세 조정 중에 소량만 변경된 매개변수를 재설정, (2) 부호 충돌 해결, 그리고 (3) 최종 합의된 부호와 일치하는 매개변수만 병합  
* 저자들은 TIES-MERGING이 다양한 모달리티, 도메인, 작업 수, 모델 크기, 아키텍처 및 미세 조정 설정을 포함한 다양한 설정에서 여러 기존 방법을 능가한다는 것을 발견함  
* 저자들은 또한 모델 매개변수에 대한 다양한 유형의 간섭의 영향을 추가로 분석하고 부호 간섭을 해결하는 것의 중요성을 강조함 

  
Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1EY3YTGw00PVcptxf0Au_4tZzHH6r7tAL?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 

<br/>
# 1 Introduction  
* 사전 훈련된 모델들은 다양한 실제 애플리케이션에서 광범위하게 사용됨  
이러한 모델들은 특정 작업에 특화되도록 추가로 미세 조정되는데, 이는 더 적은 작업별 레이블이 있는 데이터로도 성능이 향상될 수 있음을 의미함  
이러한 이점으로 인해, 시각을 위한 ViT나 언어를 위한 T5와 같은 인기 있는 사전 훈련된 모델들로부터 유도된 수천 개의 미세 조정 체크포인트들이 출시됨  
그러나 각 작업마다 별도의 미세 조정 모델을 가지는 것은 여러 단점이 있음: (1) 각 새로운 애플리케이션마다 별도의 모델을 저장하고 배포해야 함, 그리고 (2) 독립적으로 훈련된 모델들은 관련 작업으로부터 정보를 활용하여 도메인 내 성능을 개선하거나 도메인 외 일반화를 향상시킬 수 없음  
다중 작업 학습은 이러한 문제들을 해결할 수 있지만, 모든 작업에 대한 동시 접근이 필요하고 비용이 많이 들며, 모든 작업에 대해 다중 작업 훈련이 유익하도록 데이터셋을 어떻게 최적으로 혼합해야 하는지 결정하는 것은 복잡하고 자원 집약적임 

<br/>
# 2 Related Works  
신경망의 손실 함수는 일반적으로 비볼록이지만, 다른 훈련 실행에서 매개변수 값들이 때때로 증가 없이 보간될 수 있음(즉, 모드 연결됨)이 최근 연구를 통해 입증됨  
예를 들어, 두 신경망 사이에 최적화 궤적의 일부가 공유된다면 정확도를 낮추지 않고 보간할 수 있음을 Frankle et al.이 보임  
반면에, Neyshabur et al.은 완전히 분리된 최적화 궤적을 가진 두 신경망을 순진하게 보간하면 그들의 정확도가 급격히 떨어질 수 있음을 보임  
Entezari et al.은 신경망의 순열 대칭을 고려하면 주어진 아키텍처로 훈련된 모든 신경망이 선형 모드 연결됨을 가설로 제시함  
따라서 Ainsworth et al., Singh and Jaggi, Wang et al.은 순열을 찾는 기술과 최적 운송을 기반으로 하여 처음부터 훈련된 신경망을 더 잘 정렬하여 손실 증가 없이 보간할 수 있도록 함  

모델 병합과 다양한 사용 사례. 동일한 사전 훈련된 모델에서 초기화된 서로 다른 미세 조정 모델들은 종종 최적화 궤적의 일부를 공유하므로 종종 순열 대칭을 고려하지 않고 병합될 수 있음  
따라서 미세 조정 모델을 병합하면 단일 대상 작업의 성능을 향상시킬 수 있음, 도메인 외 일반화를 개선하고, 다른 작업으로부터 생성된 다중 작업 모델을 만들며, 연합 학습, 압축, 멀티모달 병합 모델, 지속적인 학습 및 기타 설정에 사용될 수 있음  
응용 분야의 범위는 단순한 매개변수 평균화를 넘어 성능을 향상시키는 방법들의 증가로 이어짐. RegMean은 모델의 병합된 매개변수에 대한 폐쇄 형식 해결책을 제안함, 이는 모델의 각 개별 선형 계층에 대해 지역 선형 회귀 문제를 해결함  
그러나 이는 모델과 같은 크기의 추가 데이터 통계를 전송해야 하며 이를 계산하기 위한 추가 추론 단계가 필요함  
Fisher Merging은 단순 평균화를 넘어서 각 모델의 개별 매개변수의 중요성을 식별하기 위해 Fisher 정보 행렬을 사용하고 병합할 때 이를 사용하여 매개변수를 가중치함  
그러나 이는 여러 체크포인트를 병합할 때 큰 이득을 보이지 않으며, 또한 그라디언트를 계산해야 하므로 높은 메모리 비용이 듬  
Task Arithmetic은 작업 벡터를 생성하고 덧셈과 같은 산술 연산을 수행하여 다중 작업 체크포인트를 얻기 위해 모델을 병

합하는 방법을 제시함  
Ortiz-Jiménez 등의 동시작업은 사전 훈련 중에 발생하는 가중치 분리 속성에 기초한 모델 병합에 대한 이론적 통찰력을 제공함  
그들은 탄젠트 공간에서 모델을 미세 조정하면 이 속성이 강화되어 더 나은 병합된 모델을 이끌어낸다고 보임  
저자들의 방법은 이전의 모델 병합 작업들을 따르지만, 병합하는 동안 서로 다른 매개변수 간의 간섭도 고려함 .

<br/>
# 3 Background and Motivation
작업 설정에서는 {t1 ... tn}과 같은 작업 세트와 T5 또는 ViT와 같은 사전 훈련된 모델이 주어지며, 저자들은 전체 모델을 미세 조정하거나 매개변수 효율적인 미세 조정(PEFT) 방법을 사용함  
이 경우, 훈련 가능한 매개변수를 θ, 초기화를 θinit, 그리고 미세 조정된 매개변수를 θft로 표시함  
이 논문에서는 여러 작업에 대해 미세 조정된 모델 매개변수 θft에 대한 접근을 가정하고 이러한 모델의 가중치를 단일 다중 작업 모델로 병합하는 방법을 고안함  
저자들은 Ilharco et al.을 따라 작업 벡터를 사용하여 병합을 수행함  
구체적으로, 작업 t에 대한 작업 벡터 τt ∈ Rd는 τt = θftt − θinitt로 정의되며, 이는 각 작업별 모델의 미세 조정 단계에서 발생하는 변경 사항에 초점을 맞추게 해주며, 모델의 가중치의 가중 평균을 적절한 스케일링과 함께 계산하는 것과 동일함  

모델 매개변수의 중복성  
저자들은 주어진 작업 벡터 내에서 많은 값들이 중복되어 있으며(그림 2에서 표시됨), 제거해도 작업의 성능에 영향을 주지 않음을 먼저 보임  
구체적으로, 그림 3은 각 작업 벡터를 상위 k% 가장 큰 크기의 값들만 유지하고 나머지를 초기 값으로 재설정(즉, 작업 벡터 내 해당 값을 0으로 설정)하는 "트리밍"할 때, 열한 개의 작업별 모델에 대한 평균 성능을 보여줌  
그림 3은 k의 다양한 값에 대한 평균 성능을 보여주며, 상위 20%의 값만 유지하는 것이 모든 매개변수를 유지하는 것과 비교해도 비슷한 결과를 제공함을 보여줌  
T5 모델에 대한 추가 세부 사항 및 결과는 부록 C.3을 참조함  
이는 미세 조정 중에 도입된 많은 매개변수 변경이 중복되며, 따라서 병합하는 동안 이러한 값들을 무시하면 영향력 있는 매개변수와의 간섭을 방지하면서 작업의 성능을 저하시키지 않을 수 있음을 보여줌 


<br/>
# 4 TIES-MERGING: TRIM, ELECT SIGN & MERGE  
앞서 언급한 문제들을 해결하기 위해 저자들은 TIES-MERGING(TRIM ELECT SIGN & MERGE)를 제안함  
이는 병합을 수행하기 전에 언급된 종류의 간섭을 해결하려는 목적임  

작업 벡터 τt ∈ Rd는 d차원 매개변수 공간에서 초기화에 대해 낮은 손실 영역으로 이동하는 데 필요한 방향과 양을 나타냄  
τt 내의 각 항목(특정 매개변수에 해당)은 d차원 공간의 축으로 생각할 수 있음  
매개변수의 부호는 이 축을 따라 손실을 감소시키는 방향(양수 또는 음수)을 나타냄  
따라서 주어진 작업 벡터 τt는 부호 벡터 γt ∈ Rd와 크기 벡터 µt ∈ Rd로 분해될 수 있으며 τt = γt ⊙ µt로 표현됨, 여기서 ⊙는 요소별 곱임  
공식적으로 γt = sgn(τt)이며 sgn(x) ∗ |x| = x를 반환하고 +1, 0, 또는 -1의 값을 가짐  
크기 벡터 µt는 µt = |τt|로 정의되며 µit 값은 초기화로부터 i번째 차원에서 필요한 이동량을 알려줌  

여러 작업별 모델 {θt}nt=1을 병합하기 위해 저자들은 먼저 해당하는 작업 벡터 {τt}nt=1를 생성함  
이러한 작업 벡터들이 주어지면 TIES-MERGING 방법은 병합을 수행하기 위해 세 단계를 따름(그림 1 및 알고리즘 1 참조):  

1. 절삭: 각 작업 t에 대해, 작업 벡터 τt에서 상위 k% 값을 그들의 크기에 따라 유지하고 나머지 하위 (100-k)%의 중복된 매개변수들을 재설정하여 0으로 만들어 τˆt를 생성함  
2. 선출: 그 다음, 서로 다른 모델 간의 매개변수 p에 대한 부호 불일치를 해결하는 병합 모델에 대한 집합된 선출 부호 벡터 γm을 생성함  
3. 분리된 병합: 그 후 각 매개변수 p에 대해, 집합된 선출된 부호와 동일한 부호를 가진 모델에서만 매개변수 값을 유지하고 그들의 평균을 계산함 


<br/>
# 5 Experimental Setup
기초 방법론으로서, 저자들은 TIES-MERGING을 네 가지 기본 병합 방법과 비교함: (1) 단순 평균화는 모든 개별 모델의 요소별 평균을 계산하며, θm = (Σt=1^n θt)/n로 표현될 수 있음 (2) Fisher Merging은 Fisher 정보 행렬 Fˆt의 대각선 근사치를 사용하여 작업 t에 대한 각 매개변수의 중요성을 측정하고, 최종 병합된 모델은 모델의 근사 Fisher 행렬 내 각 매개변수를 해당 값으로 재조정함으로써 얻어짐 (3) RegMean은 주어진 계층의 입력 활성화에 대한 내적 행렬을 저장하고 전송해야 하며, 병합된 모델의 활성화와 개별 모델의 활성화 사이의 거리를 최소화하는 목적으로 최소 제곱 회귀 문제를 해결함으로써 폐쇄 형식 해결책을 계산함 (4) 작업 산술은 작업 벡터를 초기 모델에 스케일링하여 추가함으로써 병합된 모델을 생성함, θm = θinit + λ ∗ (Σt=1^n τt)임  
추가적으로 저자들은 병합 과정에 관여한 개별 미세 조정된 모델의 성능과 모든 작업 데이터셋의 연결에 대해 훈련된 다중 작업 모델의 성능을 제시함  
더 자세한 정보는 부록 C.1, C.2 및 C.6을 참조하시오

<br/>
# 6 Main Results  
주요 실험 설정은 다양한 실험적 상황에서 TIES-MERGING과 다른 방법들의 성능을 평가하기 위한 것임  
저자들은 매개변수 효율적인 미세 조정(PEFT) 방법인 (IA)3를 중심으로 작업 벡터가 매개변수 중 도입된 것에 기초하여 계산되는 상황을 고려함  
특히 저자들은 T0-3B를 기본 모델로 사용하고 문장 완성(COPA, H-SWAG, Story Cloze 데이터셋), 자연어 추론(ANLI, CB, RTE), 공동 참조 해결(WSC, Winogrande), 단어 의미 구별(WiC)을 포함한 열한 개의 데이터셋의 훈련 분할에서 (IA)3 모델을 미세 조정함  
(IA)3에 도입된 매개변수를 미세 조정할 때 저자들은 Public Pool of Prompts(P3)에서 각 데이터셋의 각 예제를 다른 문자열에 해당하는 프롬프트 텍스트-텍스트 형식으로 변환하기 위해 프롬프트 템플릿을 사용함  
(IA)3 실험에서 저자들은 모든 템플릿에 걸쳐 중간 점수를 보고함  

<br/>
# 7 Additional Results and Analysis  
저자들은 다양한 설정에서 TIES-MERGING의 성능을 평가하기 위해 실험을 수행함  
저자들은 먼저 매개변수 효율적인 미세 조정 방법인 (IA)3을 중심으로 작업 벡터가 매개변수 중 도입된 것에 기초하여 계산되는 상황을 고려함  
특히 저자들은 T0-3B를 기본 모델로 사용하고 문장 완성, 자연어 추론, 공동 참조 해결, 단어 의미 구별을 포함한 열한 개의 데이터셋에서 (IA)3 모델을 미세 조정함  
(IA)3에 도입된 매개변수를 미세 조정할 때 저자들은 Public Pool of Prompts(P3)에서 각 데이터셋의 각 예제를 다른 문자열에 해당하는 프롬프트 텍스트-텍스트 형식으로 변환하기 위해 프롬프트 템플릿을 사용함  
(IA)3 실험에서 저자들은 모든 템플릿에 걸쳐 중간 점수를 보고함 

<br/>
# 8 Conclusion  
모델 병합 시 발생할 수 있는 간섭 문제를 해결하기 위해 저자들은 TIES-MERGING을 소개함  
TIES-MERGING은 미세 조정된 모델의 값에서 저진폭 변화를 제거하고 병합되는 모델 간의 부호 불일치를 해결함  
실험적으로 TIES-MERGING이 다양한 설정과 도메인에서 병합된 다중 작업 모델의 성능을 향상시키며, 고정된 하이퍼파라미터를 가진 간단한 방법임에도 불구하고 그렇다는 것을 발견함  
저자들의 연구는 또한 모델 매개변수에 대한 다양한 유형의 간섭의 영향을 조명하고 병합 과정에서 부호의 중요성을 강조함  
제한 사항과 향후 방향에 대한 논의는 부록 A를 참조하시오  


<br/>  
# 요약  

* TIES-MERGING 방법은 모델 병합 시 발생하는 간섭 문제를 해결하기 위해 제안되었으며, 이는 미세 조정된 모델 값에서 소액 변화를 제거하고 모델 간의 부호 불일치를 해결함으로써 다양한 설정과 도메인에서 병합된 다중 작업 모델의 성능을 향상시킴

실험 결과는 TIES-MERGING이 특히 (IA)3 모델을 사용하여 다양한 작업을 병합할 때 상위 기준선보다 우수한 성능을 보임을 보여줌

저자들은 또한 TIES-MERGING이 병합 과정에서 중복된 매개변수와 부호 불일치 문제를 효과적으로 해결함으로써 강력하고 효율적인 병합 모델을 제공한다고 주장함

이 방법은 고정된 하이퍼파라미터를 사용하며, 추가적인 분석을 통해 모델 매개변수에 대한 간섭의 영향과 병합 과정에서 부호의 중요성을 강조함

TIES-MERGING의 실험적 검증은 다양한 모델 크기, 아키텍처 및 미세 조정 설정을 포함한 다양한 실험 설정에서 수행되었으며, 병합된 모델은 개별 미세 조정된 모델과 비교하여 일관되게 더 나은 성능을 보임

이 연구는 미래의 모델 병합 연구에 중요한 기여를 하며, 모델 병합 과정에서 발생할 수 있는 주요 문제들을 해결하는 효과적인 접근 방법을 제시함


The TIES-MERGING method is proposed to address interference issues in model merging, eliminating minor changes in fine-tuned model values and resolving sign discrepancies between models to enhance the performance of merged multi-task models across various settings and domains

Experimental results show that TIES-MERGING, especially using the (IA)3 model for merging various tasks, consistently outperforms the top baseline

The authors also claim that TIES-MERGING effectively resolves issues of redundant parameters and sign disagreements during the merging process, providing a powerful and efficient merged model

This method employs fixed hyperparameters and further analysis highlights the impact of interference on model parameters and the importance of signs during the merging process

Experimental validation of TIES-MERGING is conducted across a range of experimental setups, including various model sizes, architectures, and fine-tuning settings, consistently showing better performance compared to individual fine-tuned models

This research makes a significant contribution to future model merging studies, presenting an effective approach to addressing major issues that can arise during the model merging process
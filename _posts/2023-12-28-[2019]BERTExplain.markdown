---
layout: post
title:  "[2019]What does BERT learn about the structure of language?"
date:   2023-12-28 14:59:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    

* BERT 네트워크가 언어의 구조적 정보를 포착할 수 있다는 가능성 설명
* BERT가 영어 구조에 대해 어떤 요소들을 학습하는지 탐구하기 위해 일련의 실험을 수행
* 이 실험들을 통해, BERT의 하위 레이어에서는 문구 수준의 정보를, 중간 레이어에서는 표면적 특성에서부터 구문론적, 의미론적 특성까지 다양한 언어 정보가 계층적으로 인코딩되어 있음을 발견
* BERT는 또한 주어-동사 일치와 같은 장거리 의존성 정보를 처리할 때 더 깊은 레이어를 필요로 하는 것으로 나타났으며, BERT의 표현이 전통적인 나무 구조와 유사한 방식으로 구성되어 있는 것을 확인

Useful sentences :  
*   

{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* unveil: 밝히다  
* swiftly: 신속하게  
* margin: 여유  
* span-level: 특정한 길이나 범위(span)에 걸친 문자열(Span은 단일 단어일 수도 있고, 여러 단어의 시퀀스일 수도)  
* element-wise: 배열, 벡터, 행렬과 같은 데이터 구조에서 동일한 위치에 있는 요소들끼리 연산을 수행하는 것을 의미  
* element-wise Product: 두 벡터 또는 행렬에서 동일한 위치에 있는 각 요소들을 곱하는 연산. 예를 들어, 두 벡터 A = [a1, a2, a3]와 B = [b1, b2, b3]가 있을 때, 이들의 element-wise product는 [a1b1, a2b2, a3*b3]. 이는 머신 러닝에서 특징 간의 상호작용을 모델링할 때 자주 사용.  
* element-wise Difference: 이는 두 벡터나 행렬의 동일한 위치에 있는 요소들 간의 차이를 계산하는 것을 의미. 같은 예에서 A와 B의 element-wise difference는 [a1-b1, a2-b2, a3-b3].  
* chunk:  텍스트 내에서 응집된 단어 그룹(구)을 식별하는 과정, Chunking은 일반적으로 태깅된 텍스트(각 단어에 품사 태깅이 되어 있는)를 사용하며, 명사구나 동사구와 같은 구를 식별하기 위해 특정 패턴이나 규칙을 적용, 이 과정은 자연어 이해, 정보 추출, 기계 번역과 같은 다양한 NLP 응용 프로그램에서 중요한 초석  
* t-SNE: t-Distributed Stochastic Neighbor Embedding은 고차원 데이터를 시각화하기 위해 널리 사용되는 기계 학습 알고리즘. 이 방법은 데이터의 고차원 구조를 보존하면서 이를 2차원 또는 3차원 공간에 효과적으로 투영. t-SNE는 특히 데이터 포인트 간의 국소적인 구조(local structure)와 데이터 그룹 간의 관계를 시각화하는 데 유용. 자세한 알고리즘은 아래와 같음   
** 1. 유사도 측정: 고차원 공간에서 각 데이터 포인트 간의 유사도를 측정. 이 유사도는 보통 가우시안 분포(Gaussian distribution)를 사용하여 계산.  
** 2. 저차원 매핑: 고차원 데이터 포인트를 저차원(2D 또는 3D) 공간으로 매핑. 이때, 고차원에서의 유사도가 저차원에서도 유지되도록 함.  
** 3. t-분포 사용: 저차원 공간에서는 t-분포를 사용하여 데이터 포인트 간의 거리를 조정. 이는 데이터 포인트들이 서로 너무 가깝게 모이는 것을 방지하고, 군집 간의 관계를 더 명확하게 만들어 줌.  
* quantify: 양적으로 측정하거나 수치화하는 것을 의미. 이 용어는 어떤 대상의 양, 정도, 크기, 빈도 등을 수치적으로 나타내는 과정, 자연어 처리나 기계 학습 분야에서 'quantify'는 모델의 성능, 데이터의 특성, 알고리즘의 효율성 등을 수치로 표현하는 것 의미.  
* Normalized Mutual Information metric:  클러스터링 결과의 질을 평가하는 데 사용되는 메트릭입니다. 이 메트릭은 클러스터링이 얼마나 잘 이루어졌는지를 양적으로 평가하는 데 사용, NMI는 두 확률 변수 간의 상호 정보(Mutual Information, MI)를 정규화한 값. 상호 정보는 두 확률 변수가 얼마나 많은 정보를 공유하는지를 측정하는 값이며, 이를 통해 변수들 간의 의존성을 평가.  NMI는 이 상호 정보를 두 변수의 엔트로피 값으로 정규화하여, 서로 다른 크기의 데이터 세트나 클러스터링 결과 간에도 비교가 가능하도록 함.   
* unearthing: 땅을 파다에서 유례된 말, 숨겨진/미지의 정보나 사실을 발견하거나 드러내는 것  
* auxiliary: 보조의, 부수적인. 언어학에서 보조 동사(auxiliary verb)는 주 동사의 의미를 보완하는 역할을 하며, 기술 분야에서는 보조 장치나 시스템이 주 시스템을 지원하는 역할  
* auxiliary Classification: 주된 작업을 보조하기 위해 사용되는 분류 작업. 기계 학습에서 이는 주된 학습 목표 외에 추가적인 학습 신호를 제공하는 분류 작업을 의미. 예를 들어, 복잡한 문제를 해결하기 위해 네트워크를 훈련시키는 동안, 보조 분류 작업을 도입하여 네트워크가 더 유용한 특징을 학습하도록 유도  
  

<br/>

# 1 Introduction  
* BERT(양방향 인코더 표현에서의 변환기)는 양방향 변환기 네트워크의 변종으로, 문맥에서 마스킹된 단어를 예측(MLM)하고 두 문장이 연속적인지 분류(NSP)하는 훈련을 받음(training)  
* 이 훈련된 모델은 질문 응답 및 언어 추론과 같은 다운스트림 NLP(자연어 처리) 작업에 큰 수정 없이 미세 조정될 수 있음(fine-tuned)
* BERT는 GLUE 벤치마크의 11가지 NLP 작업에서 이전의 최신 모델들을 크게 능가  
* 이러한 눈에 띄는 결과는 BERT가 언어의 구조적 정보를 "학습"할 수 있음을 시사  

* 이 연구에서는 BERT의 다양한 계층에서 학습된 표현의 성격을 조사하기 위한 일련의 실험을 수행
** 1. 하위 계층에서 구문 수준 정보를 포착하고 이 정보가 상위 계층에서 희석된다는 것을 보여줌  
** 2. 프로빙 작업을 사용하여 BERT가 하위 계층에서 표면적 특성, 중간 계층에서 구문적 특성, 상위 계층에서 의미적 특성을 포함하는 풍부한 언어 정보 계층 구조를 포착한다는 것을 보여줌  
** 3. BERT 표현이 주어-동사 일치를 추적하는 능력을 테스트하고, 더 어려운 장거리 의존성을 다루기 위해 BERT가 더 깊은 계층을 요구한다는 것을 발견
** 4. 최근(2019) 소개된 Tensor Product Decomposition Network(TPDN)를 사용하여 BERT의 표현이 고전적인 나무 같은 구조를 암시적으로 포착한다는 다양한 가설을 탐구  


# 2 BERT  
* BERT는 2017에 소개된 Transformer 네트워크를 기반
* 이 모델은 모든 계층에서 왼쪽과 오른쪽 문맥을 동시에 고려하여 양방향 표현을 사전 훈련
* BERT의 표현들은 입력에서 무작위로 마스킹된 단어들을 예측하고, 주어진 문장이 특정 문장 뒤에 오는지 분류함으로써 공동으로 최적화

* 이 섹션은 BERT의 여러 계층에서 얻은 범위 표현의 군집 성능을 보여주는 표(Table 1)와 함께 BERT의 첫 번째 및 마지막 두 계층에서 계산된 범위 임베딩의 2D t-SNE 플롯(Figure 1)을 포함
* 이러한 시각적 자료들은 BERT가 어떻게 다양한 언어적 특성을 처리하고 표현하는지에 대한 이해를 도움  


# 3 Phrasal Syntax
* BERT가 어떻게 구문 수준(phrase-level) 정보를 포착하는지에 대해 설명  
* 이전의 연구(Peters et al. 2018)에서는 LSTM 기반 언어 모델이 구문 수준의 정보를 포착할 수 있음을 봄  
* 그러나 BERT와 같이 전통적인 언어 모델링 목표로 훈련되지 않은 모델에서도 이러한 현상이 관찰되는지는 불분명
* 이 섹션에서는 BERT의 각 계층에서 추출한 범위 표현(span representations)을 통해 이 질문을 조사  

* 이를 위해, 토큰 시퀀스에서 각 계층(l)에서의 범위 표현을 계산  
* 이는 해당 계층의 첫 번째와 마지막 숨겨진 벡터를 연결하고, 그들의 요소별 곱과 차이를 포함하여 구성  
* CoNLL 2000 청킹 데이터셋에서 무작위로 선택된 3000개의 레이블이 지정된 청크와 500개의 청크로 레이블이 지정되지 않은 범위를 사용  

* t-SNE(비선형 차원 축소 알고리즘)를 사용하여 다양한 계층에서 얻은 범위 표현을 시각화  
* 이를 통해 BERT가 주로 하위 계층에서 구문 수준 정보를 포착하고, 이 정보가 상위 계층으로 갈수록 점차 희석된다는 것을 관찰할 수 있음  


# 4 Probing Tasks  
* 신경 모델에 인코딩된 언어적 특징을 밝히는 데 도움이 되는 프로빙(진단적) 작업에 대해 설명  
* 이 작업은 모델의 최종 출력을 사용하여 관심 있는 언어 현상을 예측하는 보조 분류 작업을 설정함으로써 수행  
* 보조 분류기가 언어적 속성을 예측할 수 있다면 원래 모델은 그 속성을 인코딩할 가능성이 높음  

* 이 연구에서는 BERT의 각 계층이 다양한 유형의 언어적 특징을 인코딩하는 능력을 평가하기 위해 Conneau et al. (2018)에 의해 생성된 열 가지 프로빙 문장 수준 데이터셋/작업을 사용  
* 이러한 작업은 세 가지 범주로 그룹화:  
** 표면 작업: 문장 길이(SentLen) 및 문장 내 단어 존재 여부(WC)를 탐색  
** 구문 작업: 단어 순서에 대한 민감도(BShift), 구문 트리의 깊이(TreeDepth), 구문 트리의 최상위 구성 요소의 순서(TopConst)를 검사  
** 의미적 작업: 주절의 주어와 직접 목적어 수(SubjNum, ObjNum), 명사/동사의 무작위 교체에 대한 민감도(SOMO), 조정된 절의 구성 요소들의 무작위 교환에 대한 민감도(CoordInv) 등을 테스트  
* 프로빙 작업의 결과는 BERT가 표면 정보를 하위 계층에서, 구문 정보를 중간 계층에서, 의미 정보를 상위 계층에서 인코딩한다는 것을 보여줌  
* 이는 BERT가 언어의 다양한 측면을 계층적으로 처리한다는 것을 나타냄  


# 5 Subject-Verb Agreement
* 주어-동사 일치 현상을 탐구  
* 이는 신경 모델이 구문 구조를 인코딩하는지 여부를 탐사하는 대리 작업  
* 이 작업에서는 주어와 동사 사이에 다른 수의 명사(반대 수를 가진 유인자)가 개입할 때, 동사의 수를 예측하는 작업의 난이도가 증가  
** BERT가 주어-동사 일치의 여러 자극에 대해 놀라울 정도로 잘 대응한다는 것을 보여줌  
* 이 연구는 Goldberg의 연구를 확장하여 BERT의 각 계층에서 이 테스트를 수행하고, 유인자의 수를 통제  

* 이 연구에서는 SentEval 툴킷(Conneau and Kiela 2018)을 사용하여 이진 분류기를 구축  
* 이 분류기는 마스킹된 해당 동사에서의 활성화를 특징으로 사용  

* 실험 결과는 BERT의 중간 계층이 대부분의 경우에서 잘 수행되며, 이는 섹션 4의 결과와 일치  
* 여기서 구문적 특징이 중간 계층에서 잘 포착  
* 특히 주의할 만한 점은, 유인자의 수가 증가함에 따라 BERT의 더 높은 계층(예: 8번 계층)이 주어와 동사 사이에 더 많은 단어가 개입하는 장거리 의존성 문제를 더 낮은 계층(예: 7번 계층)보다 더 잘 다루는 것으로 나타남  
* 이는 NLP 작업에서 경쟁력을 발휘하기 위해 BERT에 더 깊은 계층이 필요함을 강조  


# 6 Compositional Structure  
* BERT가 학습한 표현의 구성적 특성을 이해할 수 있는지를 탐구   
* 이를 위해 Tensor Product Decomposition Networks(TPDN)를 사용  
* TPDN은 입력 토큰("filler") 표현을 사전에 선택된 역할 체계(role scheme)에 따라 명시적으로 구성  
** 예를 들어, 단어의 역할은 구문 트리에서 루트 노드로부터의 경로에 따라 결정될 수 있음  

* BERT의 각 계층에 대해 다섯 가지 다른 역할 체계를 사용  
* 각 단어의 역할은 그것의 왼쪽에서 오른쪽 인덱스, 오른쪽에서 왼쪽 인덱스, 두 인덱스의 순서 쌍, 구문 트리 내의 위치(Stanford PCFG Parser의 형식화된 버전) 및 단어 위치를 무시하는 문장 내 모든 단어에 공통적인 인덱스(단어 가방)에 기반  
* 또한, 무작위 이진 트리에 기반한 역할 체계도 정의합니다.

* 이 연구에서는 SNLI 말뭉치의 전제 문장들에 대해 TPDN 모델을 훈련  
* BERT의 입력 계층에서 사전 훈련된 단어 임베딩으로 TPDN의 필러 임베딩을 초기화하고, 이를 고정시킨 후 선형 투영을 학습하고, 평균 제곱 오차(MSE) 손실 함수를 사용  
* 다른 학습 가능한 매개변수에는 역할 임베딩과 BERT의 임베딩 크기와 일치하도록 텐서 곱셈 합 위에 선형 투영이 포함  

* Table 4는 사전 훈련된 BERT 표현과 TPDN이 BERT를 근사하기 위해 훈련된 표현 사이의 MSE를 보여줌  
* 연구 결과, BERT는 트리 기반 체계를 암시적으로 구현하며, 해당 체계를 따르는 TPDN 모델이 대부분의 계층에서 BERT의 표현을 가장 잘 근사  
* 이는 주목할 만한 결과로, BERT는 순전히 주의 메커니즘에 의존하면서도 고전적인 나무와 같은 구조를 인코딩  

* 이 연구에 의해 동기를 얻어, Raganato와 Tiedemann(2018)의 작업을 따라 자기주의 가중치에서 유도된 의존성 트리에 대한 사례 연구를 수행  
* Figure 2는 레이어 #2의 주의 헤드 #11에서 각 단어 쌍에 대한 자기주의 가중치를 얻어, 금본의 루트('are')를 시작 노드로 설정하고 Chu-Liu-Edmonds 알고리즘을 사용하여 추론된 의존성을 보여줌  
* 결정자-명사 의존성("the keys", "the cabinet", "the table")과 주어-동사 의존성("keys"와 "are")이 정확하게 포착됨  
** 놀랍게도, "key", "cabinet", "table" 간의 의존성 체인을 통해 술어-논항 구조가 부분적으로 모델링되는 것으로 보임  


# 7 Related Work
* BERT와 관련된 기존 연구들을 검토  
** 언어 모델 사전 훈련을 위한 다양한 신경망 아키텍처(CNNs, Transformers, RNNs)가 하류 작업 정확도와 맥락화된 단어 표현의 질적 특성에 어떻게 영향을 미치는지 연구가 있었음  
** BERT 모델이 주어-동사 일치에 대한 구문 정보를 잘 포착한다는 것을 보여주는 연구있었음  
** BERT의 언어적 특성을 다루거나 구문 구조를 잘 추출할 수 있다는 것을 보여주는 증빙 연구들 있었음  


# 8 Conclusion
* BERT는 영어의 구조적 특성을 포착하며, 구문적 현상을 풍부하게 인코딩  
* BERT는 구문 수준 정보를 반영하는 구문 표현을 학습하고, 표면에서 의미적 특성에 이르기까지 언어 신호의 계층을 구성  
* BERT는 장거리 의존성 정보를 모델링하기 위해 더 깊은 계층을 필요로 하며, 내부 표현은 전통적인 구문 분석과 유사한 구성적 모델링을 반영  
* 더 다양한 구문 구조와 단어 순서의 유연성을 가진 다른 도메인에서의 연구도 유망함  



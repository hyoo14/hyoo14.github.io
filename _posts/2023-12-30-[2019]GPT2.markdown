---
layout: post
title:  "[2019]Language Models are Unsupervised Multitask Learners"
date:   2023-12-30 18:07:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    
* 이 연구에서는 질문 답변, 기계 번역, 독해 및 요약과 같은 작업들이 일반적으로 특정 작업에 대한 지도 학습을 통해 접근된다고 언급
* 그러나 연구진은 새로운 데이터셋인 WebText를 통해 언어 모델이 이러한 작업들을 명시적인 지도 없이도 학습하기 시작한다는 것을 보여줌  
* 이러한 조건에서, 언어 모델이 생성한 답변은 CoQA 데이터셋에서 55 F1 점수를 달성하여, 127,000개 이상의 훈련 예제를 사용하지 않고도 4개 기준 시스템 중 3개의 성능과 맞먹거나 능가  
* 이 논문에서 소개된 가장 큰 모델인 GPT-2는 1.5B 파라미터의 트랜스포머 모델로, 8개의 테스트된 언어 모델링 데이터셋 중 7개에서 최고의 결과를 달성  
* 이러한 결과들은 언어 처리 시스템이 자연스럽게 발생하는 시연을 통해 작업을 수행하는 방법을 학습하는 것에 대한 유망한 경로를 제시  

Useful sentences :  
*   

{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1pRHgqTRjZ9l3aVzePZ7_Dd-PkNWs_w3p?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* capacity: 용량-모델이 처리할 수 있는 정보의 양 또는 복잡성  
* capacity of language model: 언어 모델의 용량-언어 모델이 학습하고 처리할 수 있는 언어적 정보의 양 또는 복잡성  
* erratic: 변덕스러운-일관성이 없거나 예측할 수 없는 행동 또는 패턴  
* erratic behavior: 변덕스러운 행동 - 일정하지 않고 예측하기 어려운 행위 또는 반응.  
* suspicion:  의심 - 불확실함이나 믿음의 부족  
* nascent: 초기의, 발생 단계의 - 발달의 초기 단계에 있는  
* factorize: 인수분해하다 - 수학적으로 숫자나 다항식을 그 구성 요소로 분해하는 것  
* preliminary: 예비의, 초기의 - 본격적인 것에 앞서 이루어지는 준비 단계나 초기 단계  
* toy-ish setup:  장난감 같은 설정 - 실제적이거나 실용적이지 않고 단순화된 실험적 설정  
* well-posed setup: 잘 구성된 설정 - 명확하고 해결 가능한 조건을 갖춘 실험적 또는 이론적 설정  
* karma: 업 - 행위에 대한 결과나 영향을 나타내는 불교 및 힌두교 용어  
* de-duplication: 중복 제거 - 데이터 중복을 제거하는 과정  
* layer normalizatoin: 층 정규화 - 신경망에서 각 층의 입력을 정규화하는 기술  
** 평균과 표준편차 계산: 각 층의 활성화(activation)에 대해 평균과 표준편차를 계산.  
** 정규화: 각 활성화를 평균에서 빼고 표준편차로 나누어 정규화. 이 과정은 각 층의 입력을 더 균일한 분포로 만듬.  
** 재조정 및 재배치: 정규화된 값을 재조정(scale) 및 재배치(shift)하기 위해 학습 가능한 파라미터를 사용. 이를 통해 모델이 필요에 따라 정규화된 값의 스케일을 조정.  
* exponentiated: 지수화된 - 수학적으로 지수 함수를 적용한  
* exponentiated version: 지수화된 버전 - 원래 값에 지수 함수를 적용한 형태  
* punctuation: 구두점 - 문장의 구조를 명확히 하기 위해 사용되는 기호  
* de-tokenizer: 디토크나이저 - 토큰화된 텍스트를 원래의 형태로 복원하는 도구  
* destructive pre-processing: 파괴적 전처리 - 원본 데이터에 손상을 주는 전처리 방법  
* LAMBADA: Language Modeling Broadened to Account for Discourse Aspects 데이터셋 - 텍스트의 긴 범위 의존성을 테스트하는 데이터셋, 장기적인 의존성을 모델링하는 능력을 평가하기 위해 설계, 사람이 성공적으로 마지막 단어를 예측하기 위해서는 적어도 50개 토큰의 문맥이 필요  
** 예시:
문장: "The only way to keep your health is to eat what you don't want, drink what you don't like, and do what you'd rather not. It is _______"  
해답: "nonsense"  
이 예시에서 "nonsense"라는 마지막 단어를 정확히 예측하기 위해서는 전체 문장의 문맥을 이해하는 것이 필요        
* schema: 구조, 계획, 또는 조직적인 틀을 의미. 다양한 분야에서 다양한 의미로 사용되는데, 예를 들어 데이터베이스에서는 데이터의 구조화된 틀을, 심리학에서는 사람의 인지 구조나 지식의 조직 방식을, 컴퓨터 과학에서 스키마는 데이터의 조직, 형식, 제약 조건을 정의하는 구조를 나타냄  
* Winograd Schema challenge: 윈오그라드 스키마 챌린지 - 자연어 이해에 대한 상식 추론 능력을 평가하는 테스트  
* capability: 능력 - 수행할 수 있는 능력 또는 특정 작업을 수행하는 데 필요한 기능  
* resolution: 해결 - 문제나 분쟁의 해결 또는 분명한 결정  
* resolution of the ambiguity: 모호성의 해결 - 모호한 상황이나 문장의 의미를 명확히 하는 것  
* CoQA: Conversation Question Answering dataset  
* TL;DR: Too Long; Didn't Read 의 약자 - 긴 내용을 요약한 짧은 버전  
* resemble: 닮다 - 어떤 것이 다른 것과 유사하거나 비슷  
* ROGUE metric: 'Recall-Oriented Understudy for Gisting Evaluation'의 약자, 주로 텍스트 요약에서 사용되는 평가 지표. 이 지표는 생성된 요약문과 참조(또는 기준) 요약문 간의 유사성을 측정  
** ROUGE-N: N-gram 오버랩을 기반으로 하며, N은 단어의 연속적인 시퀀스를 나타냄. 예를 들어, ROUGE-1은 단일 단어(1-gram) 오버랩에 기반한 일치도를 측정하고, ROUGE-2는 두 단어 시퀀스(2-gram)의 일치도를 측정.
** ROUGE-L: 가장 긴 공통 부분 문자열(Longest Common Subsequence)을 사용하여 일치도를 측정. 이는 단어 순서에 덜 민감하며, 전체적인 구조적 유사성을 고려함  
* BLEU: Bilingual Evaluation Understudy, 기계 번역의 품질을 평가하는 데 널리 사용되는 지표, 기계 번역된 텍스트가 얼마나 원본(인간 번역) 텍스트와 유사한지를 측정, BLEU 점수는 0에서 1 사이의 값으로, 더 높은 값이 더 나은 번역 품질을 나타냄. BLEU는 여러 참조 번역을 사용하여 번역의 다양성을 고려할 수 있으며, 일반적으로 번역의 품질을 신속하고 객관적으로 평가하는 데 유용  
** N-gram 오버랩: 기계 번역된 문장과 참조 문장 간의 N-gram(연속된 N개의 단어) 일치도를 측정  
** 정확도의 조정: 긴 문장에서 짧은 문장으로의 오버랩을 페널티하는 브레버티 페널티(Brevity Penalty)를 적용하여 길이에 따른 편향을 조정  
* Brevity Penalty: BLEU 점수 계산에 사용되는 요소로, 기계 번역된 문장의 길이가 참조 번역(인간 번역)에 비해 너무 짧을 경우에 적용되는 페널티입니다. 이는 기계 번역이 너무 간결하게 번역되어 필요한 정보가 누락될 가능성을 방지하기 위한 것  
** c를 기계 번역된 문장의 길이, r을 참조 번역의 길이라고 할 때,  
** 만약 c가 r보다 작다면 (즉, 기계 번역이 참조 번역보다 짧다면), Brevity Penalty는 exp(1 - r/c)로 계산    
** 그렇지 않으면 (기계 번역이 참조 번역과 같거나 더 길면), Brevity Penalty는 1(즉, 페널티가 적용되지 않음).  
** Brevity Penalty는 기계 번역이 너무 간결하게 번역하여 중요한 내용을 빼먹지 않도록 보장하기 위해 사용  
* lexicon: 어휘집, 특정 언어, 사람, 분야 또는 주제와 관련된 단어들의 집합.  
* factoid: 종종 사실처럼 보이지만 검증되지 않았거나 완전히 사실이 아닌 정보  
* factoid-style questions: 간단하고 구체적인 사실에 대한 답을 요구하는 질문  
* hybridize: 두 가지 이상의 다른 방법, 스타일, 기술 등을 결합하는 것  
* over-reporting: 필요 이상으로 많이 또는 과장해서 보고하는 것  
* Bloom: 사람 이름..  
* Bloom filters: 버튼 블룸(Burton Bloom)에 의해 개발된, 공간 효율적인 확률적 데이터 구조. 이 구조는 집합에 어떤 요소가 속해 있는지를 빠르고 효율적으로 검사할 수 있게 해 줌  
** 공간 효율성: Bloom 필터는 요소의 존재 여부를 검사하는 데 필요한 저장 공간이 매우 적음. 이 때문에 대용량 데이터셋에 효과적.  
** 확률적 검사: 요소가 필터에 존재하는지 여부를 검사할 때, Bloom 필터는 '정확히 존재한다' 또는 '존재하지 않는다'고 답할 수 있음. 하지만 요소가 존재하지 않는다고 보고되는 경우에만 100% 확신할 수 있음. 요소가 존재한다고 보고될 때는 확률적으로 정확하지만, 오진(false positive)이 발생할 수 있음.
** 해시 함수의 사용: Bloom 필터는 여러 개의 해시 함수를 사용하여 요소를 필터에 추가하거나 검사. 요소가 추가될 때, 그 요소는 모든 해시 함수에 의해 다른 위치에 표시.  
** 오진(false positives): Bloom 필터는 오진을 일으킬 수 있지만, 오진율은 필터의 크기와 사용된 해시 함수의 수에 따라 조절. 오진율이 높아질수록 필터는 더 적은 공간을 사용하지만, 오진의 가능성이 높아짐.  
** 응용 분야: Bloom 필터는 네트워크 시스템에서 스팸 필터링, 웹 캐시의 중복 검사, 데이터베이스 시스템에서 중복 데이터 감지 등 다양한 분야에서 사용.  
* alphanumeric: 알파벳과 숫자를 모두 포함하는 것  
* alphanumberic words: 알파벳과 숫자를 모두 포함한 단어  
* delimiter: 구분자, 데이터 필드나 요소를 구분하는 데 사용되는 문자나 문자열  
* worryingly: 걱정스럽게  
* quantifying: 정량화, 수치로 표현하거나 측정하는 것.  
* scalable fuzzy matching: 유사한 항목을 매칭하는 방법으로, 확장 가능하고 정확하지 않은(퍼지한) 매칭을 허용함  
** 'Scalable fuzzy matching'은 데이터 매칭이나 검색에서 두 문자열이 완벽하게 일치하지 않아도 유사성을 기반으로 매칭을 수행하는 기술.  
** '퍼지(fuzzy)'라는 용어는 문자열 간의 완전한 일치뿐만 아니라 부분적인 일치나 근접한 일치도 고려한다는 것을 의미. 예를 들어, 오타나 다른 언어의 표현 방식을 고려할 때 유용.  
** '확장 가능한(scalable)'이라는 말은 이 기술이 작은 데이터셋뿐만 아니라 매우 큰 데이터셋에도 효율적으로 적용될 수 있음을 나타냄. 즉, 데이터셋의 크기가 증가해도 성능이 유지되거나 향상됨.  
** 이 기술은 데이터 클렌징, 정보 검색, 자연어 처리 등 다양한 분야에서 중요.  
* n-gram overlap based de-duplication: 텍스트에서 n-그램(연속된 n개의 항목)의 중복을 제거하는 방법.  
* chit-chat dialog: 잡담 대화  
* qualitatively performing: 질적 수행, 수치적인 결과보다는 질적인 측면에서 수행하는 것.  
* zero-shot: 사전에 특정 작업에 대한 학습 없이 해당 작업을 수행하는 능력   
** 'Zero-shot learning'은 머신러닝 모델이 학습 과정에서 본 적 없는 새로운 작업이나 카테고리에 대해 예측을 수행하는 것을 의미.  
** 이는 모델이 학습 데이터셋에서 직접적인 경험을 하지 않은 작업에 대해서도 일반화할 수 있는 능력을 갖추고 있음을 의미.  
** 예를 들어, 언어 모델이 특정 도메인의 텍스트를 학습하지 않았음에도 불구하고, 그 도메인에 대해 질문에 답할 수 있는 경우, 이를 '제로샷' 능력이라고 함.  
** 제로샷 학습은 특히 데이터가 제한적인 상황에서 유용하며, 인공지능의 범용성과 유연성을 향상시키는 데 중요한 연구 분야.  
* likelihood: 가능성, 가능도, 특정 사건이 발생할 확률 또는 가능성  
** 특정 모델 또는 매개변수가 주어진 데이터를 생성할 확률을 의미  
** 'Likelihood'는 특정 매개변수 값에서 관찰된 데이터셋이 나타날 확률을 의미하며, MLE에서는 이 가능성을 최대화하는 매개변수 값을 찾음   
* Qualifying: 자격을 부여하다, 조건 설정, 제한    








<br/>

# 1. Introduction  
* 현재의 기계 학습 시스템은 대규모 데이터셋과 고용량 모델, 지도 학습을 사용하여 특정 작업에 대해 뛰어난 성능  
* 이러한 시스템들은 데이터 분포나 작업 사양의 작은 변화에도 취약   
* 이들은 특정 작업에 대해서만 전문적인 기능을 수행하는 '좁은 전문가(narrow experts)'로, 연구자들은 보다 다양한 작업을 수행할 수 있는 '능숙한 일반가(competent generalists)'를 개발하는 것이 목표  
* 현재 기계 학습 시스템이 단일 작업과 단일 도메인 데이터셋에 대한 훈련의 한계를 지니고 있으며, 이것이 일반화의 부족을 가져온 주요 요인  
* 복수의 도메인과 작업에 걸쳐 훈련하고 성능을 측정하는 것이 중요  
* 또한, 다중 작업 학습(Multitask learning)이 일반적인 성능 향상을 위한 유망한 방법이지만, 자연어 처리(NLP) 분야에서는 아직 초기 단계  
* 이 연구의 핵심은 언어 모델이 지도 학습 없이도 다양한 하위 작업을 수행할 수 있다는 것을 보여주는 것  
* 이 논문은 언어 모델이 파라미터나 구조의 수정 없이도 '제로샷' 설정에서 여러 작업을 수행할 수 있음을 보여주며, 이러한 접근이 일부 작업에서 경쟁력 있는 결과를 달성할 수 있음을 입증  


# 2. Approach  
## 2.1 Training Dataset  
* 이 연구에서는 과거 언어 모델들이 주로 뉴스 기사, 위키피디아, 소설과 같은 단일 도메인의 텍스트로 훈련  
* 가능한 한 다양하고 광범위한 데이터셋을 구축하여 다양한 도메인과 맥락에서 자연스러운 언어 작업의 시연을 수집하는 것을 목표  
* Common Crawl과 같은 웹 스크랩이 다양하고 거의 무한한 텍스트의 원천이 될 수 있으나, 데이터 품질 문제가 있음  
* 따라서 연구자들은 인간이 큐레이팅/필터링한 웹 페이지만을 스크랩하여 'WebText'라는 데이터셋을 구성  
* 이 데이터셋은 Reddit에서 최소 3개의 카르마를 받은 모든 아웃바운드 링크를 포함하며, 약 8백만 개의 문서로 구성되어 총 40GB의 텍스트를 포함  

## 2.2 Input Representation  
* 일반적인 언어 모델은 모든 문자열의 확률을 계산하고 생성할 수 있어야함.  
* 기존 대규모 언어 모델은 소문자화, 토큰화, OOV(사전에 없는 단어) 토큰과 같은 전처리 단계를 포함  
* 연구진은 바이트 페어 인코딩(Byte Pair Encoding, BPE)을 사용하여 문자 및 단어 수준 언어 모델링 사이의 실용적인 중간 지점을 찾음  
* 이 방법은 빈번한 기호 시퀀스에 대해 단어 수준의 입력을 사용하고 드문 기호 시퀀스에 대해서는 문자 수준의 입력을 사용  
* 이 입력 표현을 사용함으로써, 연구진은 단어 수준 언어 모델의 실증적 이점과 바이트 수준 접근 방식의 일반성을 결합할 수 있었음  

## 2.3 Model  
* 이 연구에서 사용된 모델은 트랜스포머(Transformer) 기반 아키텍처로 구성된 모델  
* 이 모델은 레이어 정규화(layer normalization)가 각 서브 블록의 입력으로 이동되고 최종 자기 주의(self-attention) 블록 후에 추가 레이어 정규화가 추가됨   
* 모델 깊이에 따라 잔여 경로에서의 누적을 고려하는 수정된 초기화를 사용  
* 연구진은 4가지 크기의 모델을 훈련하고 벤치마크했으며, 가장 큰 모델인 GPT-2는 GPT보다 한 자릿수 더 많은 매개변수를 가지고 있음  
* 각 모델의 학습률은 WebText의 5% 유보 샘플에서 가장 좋은 혼란도(perplexity)를 위해 수동으로 조정  


# 3. Experiments  
# 3.1 Language Modeling  
* 언어 모델이 어떻게 '언어 모델링'이라는 기본 작업에 대해 '제로-샷 도메인 전환'(즉, 별도의 학습 없이 새로운 도메인에 적용하는 것)을 수행하는지를 평가  
** 제로-샷 도메인 전환: '제로-샷 도메인 전환'이란 모델이 특정 도메인 또는 데이터셋에 대해 별도로 학습되지 않았음에도 불구하고, 새로운 도메인이나 작업에 적용될 수 있음을 의미. 이는 모델이 기존에 학습한 지식을 다른 상황에 유연하게 적용할 수 있음을 나타냄.  
** 언어 모델링 성능 평가: GPT-2는 바이트 레벨에서 작동하며, 손실이 발생하는 전처리나 토큰화가 필요하지 않음. 따라서 어떤 언어 모델링 벤치마크에서도 평가될 수 있음. 이러한 유연성은 모델이 다양한 데이터셋에 대해 어떻게 일반화하는지를 평가하는 데 중요.  
** 성능 측정 지표: 언어 모델링 데이터셋에서의 결과는 보통 문자, 바이트, 또는 단어 당 평균 음의 로그 확률(negative log probability)의 조정된 버전으로 보고. 이 연구에서는 WebText 언어 모델을 사용하여 데이터셋의 로그 확률을 계산하고, 이를 표준 단위 수로 나누어 성능을 평가​​.  

# 3.2 Children's Book Test  
* 어린이 책 테스트(CBT)는 언어 모델이 이름, 명사, 동사, 전치사 등 다양한 단어 범주에서 어떻게 수행하는지 조사. 
* 이 테스트는 누락된 단어를 맞추는 'cloze' 테스트 형식으로, 언어 모델이 선택지 중 가장 높은 확률을 갖는 단어를 예측하도록 요구  
* 모델 크기가 커질수록 성능이 향상되며, 인간의 성능과 비슷한 수준에 도달.  

# 3.3 LAMBADA  
* LAMBADA 데이터 세트는 텍스트에서 장기적인 의존성을 모델링하는 능력을 테스트.  
* 특히, 사람이 예측하기 위해 최소 50개의 토큰이 필요한 문장의 마지막 단어를 예측하는 것이 목표.  
* 이 연구에서는 기존의 최고 성능을 상당히 뛰어넘는 결과를 보여줌.  

# 3.4 Winograd Schema Challenge  
* 이 챌린지는 시스템이 텍스트에서 모호함을 해결하는 공통 상식 추론 능력을 측정   
* GPT-2는 이전 연구보다 상당히 높은 정확도로 모호함을 해결할 수 있음을 보여줌  

# 3.5 Reading Comprehension  
* CoQA 데이터 세트는 다양한 도메인의 문서와 이에 대한 질문-답변 스타일의 대화를 포함  
* 이는 모델이 대화의 역사를 고려하여 질문에 답하는 능력을 시험  

# 3.6 Summarization  
* GPT-2가 CNN과 Daily Mail 데이터 세트에서 요약 작업을 수행하는 능력을 평가  
* 모델은 주어진 기사에 대해 자동적으로 요약문을 생성하며, 기존의 신경망 기반 기술과 유사한 수준의 성능을 보여줌.  

# 3.7 Translation  
* GPT-2가 언어 간 번역을 수행할 수 있는지를 평가  
* 예를 들어 영어 문장과 프랑스어 문장 쌍을 모델에 제시한 후, 영어 문장에 대한 프랑스어 번역을 생성하도록 함  
* 이 실험에서는 기본적인 번역 능력을 보여줌  

# 3.8 Question Answering  
* 이 부분은 언어 모델이 사실 기반 질문에 얼마나 정확하게 답할 수 있는지를 평가  
* GPT-2는 자연어 질문에 대한 답변을 생성하며, 정확도는 낮지만 모델의 용량이 중요한 요소임을 보여줌  


# 4. Generalization vs Memorization  
* 언어 모델이 데이터를 어떻게 일반화하고 기억하는지에 대해 다룸   
* 언어 모델이 정보를 단순히 기억하는 것인지, 아니면 새로운 상황에 적용할 수 있는 일반적인 규칙을 학습하는지를 탐구  


* GPT-2가 텍스트 데이터를 어떻게 처리하는지에 초점  
** 주요 관심사는 모델이 데이터를 단순히 '기억'하는 것인지 아니면 새로운 상황에 적용할 수 있는 일반적인 규칙을 '일반화'하는지 여부  
** 모델이 단순히 학습 데이터를 암기하는 것과 실제로 언어의 구조와 의미를 이해하여 새로운 상황에 적용하는 것 사이에는 큰 차이가 있기 때문  
** GPT-2가 다양한 텍스트 샘플에서 얼마나 잘 일반화하는지, 그리고 기억에 의존하는 정도를 평가하기 위해 여러 실험을 수행
** 이러한 실험은 모델이 단순한 암기보다는 보다 복잡한 언어 이해와 처리 기능을 발휘하는지를 평가하는 데 중점  
** 또한, 모델이 새로운 상황이나 데이터에 얼마나 잘 적응하는지, 그리고 학습 과정에서 얼마나 많은 정보를 '기억'하는지를 분석  
* 결과적으로, 언어 모델의 능력을 평가하는 데 있어 일반화와 기억화 사이의 균형을 이해하는 것이 중요  
** 이는 언어 모델의 실제 성능과 응용 가능성을 평가하는 데 있어 핵심적인 요소  


# 5. Related Work  
* 언어 모델의 성능 개선: 이전 연구들은 대규모 데이터셋에서 큰 언어 모델의 성능을 측정하고 개선  
** 예를 들어, Dai와 Le (2015)는 RNN 기반의 미세 조정(fine-tuning) 접근법을 개선했으며, Conneau 등(2017a)은 자연 언어 추론 모델에 의해 학습된 표현의 전이 성능을 연구  
** Subramanian 등(2018)은 대규모 다중 과제 학습에 대해 탐구  

*Seq2Seq 모델과 LM 프리트레이닝: Ramachandran 등(2016)은 seq2seq 모델이 사전 훈련된 언어 모델을 인코더와 디코더로 초기화함으로써 이익을 얻는다는 것을 보여줌  
** 더 최근의 연구(Wolf 등 2019, Dinan 등 2018)는 LM 프리트레이닝이 대화 기반 질문 응답 시스템과 같은 어려운 생성 과제에 유용함을 보여줌  

* 다양한 연구들:   
** Jozefowicz 등(2016)은 10억 단어 벤치마크에서 RNN 기반 언어 모델을 확장  
** Bajgar 등(2016)은 기존 어린이 책 테스트 데이터셋을 프로젝트 구텐베르크에서 생성한 더 큰 훈련 데이터셋으로 보완하여 결과를 개선  
** Hestness 등(2017)은 모델 용량과 데이터셋 크기에 따른 다양한 딥 러닝 모델의 성능 변화를 철저히 분석  

* 생성 모델의 학습된 기능성: 이전에 문서화된 예로는 RNN 언어 모델의 셀이 줄 너비 추적과 인용/주석 감지를 수행하는 것  
** Liu 등(2018)은 위키피디아 기사를 생성하기 위해 훈련된 모델이 언어 간 이름 번역을 학습했다는 관찰이 이 연구에 영감  

* 텍스트 코퍼스 구축 방법: 이전 연구들은 웹 페이지의 대규모 텍스트 코퍼스를 구축하고 필터링하는 대안적 접근법을 탐구  
** 예를 들어, iWeb 코퍼스(Davies 2018)가 있음  

* 언어 과제를 위한 사전 훈련 방법: 
** GloVe(Pennington 등 2014)는 Common Crawl의 모든 단어 벡터 표현 학습을 확장  
** Skip-thought 벡터(Kiros 등 2015)는 텍스트에 대한 깊은 표현 학습에 대한 초기 중요한 연구  
** McCann 등(2017)은 기계 번역 모델에서 파생된 표현의 사용을 탐구  



# 6. Discussion  
* 언어 모델의 표현 학습: 이전 연구들은 지도학습 및 비지도학습 사전 훈련 방법의 표현을 학습하고, 이해하며, 비판적으로 평가하는 데 집중  
** GPT-2의 결과는 비지도학습 작업 학습이 탐색할 가치가 있는 추가적인 연구 영역임을 제안  
** 이러한 발견은 사전 훈련 기술이 하류 NLP 작업에서 널리 성공할 수 있는 이유를 부분적으로 설명  

* 독해력 테스트 성능: GPT-2는 독해력 테스트에서 지도학습 기반 기준과 비슷한 경쟁력 있는 성능을 제로-샷 설정에서 보여줌    
** 그러나 요약과 같은 다른 작업에서는 양적 지표에 따르면 여전히 기본적인 수준의 성능  
** 실제 응용에 있어서 GPT-2의 제로-샷 성능은 아직 사용하기에는 미흡  

* 다양한 NLP 작업에서의 성능: GPT-2는 많은 전통적인 NLP 작업에서 제로-샷 성능을 연구했지만, 평가할 수 있는 추가적인 작업들이 많이 있음  
** 여전히 많은 실용적인 작업에서 GPT-2의 성능이 무작위 선택보다 나은 것은 아님    
** 질문 응답 및 번역과 같은 공통 작업에서도 언어 모델은 충분한 용량을 갖추었을 때에만 기본 기준을 능가하기 시작  

* 세부 조정과 성능의 상한: 제로-샷 성능은 GPT-2가 많은 작업에서 가능한 성능의 기준을 설정하지만, 세부 조정으로 얻을 수 있는 최대 성능은 불분명  
** GPT-2의 전적으로 추상적인 출력은 많은 질문 응답 및 독해력 데이터셋에서 현재 최고의 기술인 추출적 포인터 네트워크 기반 출력과 상당히 다름  
** 따라서, GPT의 성공적인 세부 조정을 고려할 때, decaNLP 및 GLUE와 같은 벤치마크에서의 세부 조정을 조사할 계획  
** GPT-2의 추가적인 훈련 데이터와 용량이 BERT에 의해 입증된 단방향 표현의 비효율성을 극복할 수 있는지는 불분명   

# 7. Conclusion  
* 크고 다양한 데이터셋으로 훈련된 대규모 언어 모델은 다양한 도메인과 데이터셋에서 높은 성능을 발휘  
* GPT-2는 테스트된 8개의 언어 모델링 데이터셋 중 7개에서 최고 수준의 성능을 제로-샷 설정으로 달성  
* 모델이 제로-샷 설정에서 수행할 수 있는 작업의 다양성은 충분히 다양한 텍스트 코퍼스의 가능성을 최대화하기 위해 훈련된 고용량 모델이 명시적인 감독 없이도 많은 작업을 수행하는 방법을 배울 수 있음을 시사  
* 이 결론은 GPT-2가 언어 모델링 분야에서 중요한 진전을 나타내며, 대규모 모델이 다양한 자연어 처리 작업을 수행할 수 있는 잠재력을 가지고 있음을 강조  
* 이러한 모델은 지도 학습 없이도 다양한 작업을 수행하는 능력을 보여주며, 이는 언어 모델링 분야에서 중요한 발전  
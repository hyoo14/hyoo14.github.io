---
layout: post
title:  "[2019]Language Models are Unsupervised Multitask Learners"
date:   2023-12-30 18:07:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    
* 이 연구에서는 질문 답변, 기계 번역, 독해 및 요약과 같은 작업들이 일반적으로 특정 작업에 대한 지도 학습을 통해 접근된다고 언급
* 그러나 연구진은 새로운 데이터셋인 WebText를 통해 언어 모델이 이러한 작업들을 명시적인 지도 없이도 학습하기 시작한다는 것을 보여줌  
* 이러한 조건에서, 언어 모델이 생성한 답변은 CoQA 데이터셋에서 55 F1 점수를 달성하여, 127,000개 이상의 훈련 예제를 사용하지 않고도 4개 기준 시스템 중 3개의 성능과 맞먹거나 능가  
* 이 논문에서 소개된 가장 큰 모델인 GPT-2는 1.5B 파라미터의 트랜스포머 모델로, 8개의 테스트된 언어 모델링 데이터셋 중 7개에서 최고의 결과를 달성  
* 이러한 결과들은 언어 처리 시스템이 자연스럽게 발생하는 시연을 통해 작업을 수행하는 방법을 학습하는 것에 대한 유망한 경로를 제시  

Useful sentences :  
*   

{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1pRHgqTRjZ9l3aVzePZ7_Dd-PkNWs_w3p?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 
<br/>

# 1. Introduction  
* 현재의 기계 학습 시스템은 대규모 데이터셋과 고용량 모델, 지도 학습을 사용하여 특정 작업에 대해 뛰어난 성능  
* 이러한 시스템들은 데이터 분포나 작업 사양의 작은 변화에도 취약   
* 이들은 특정 작업에 대해서만 전문적인 기능을 수행하는 '좁은 전문가(narrow experts)'로, 연구자들은 보다 다양한 작업을 수행할 수 있는 '능숙한 일반가(competent generalists)'를 개발하는 것이 목표  
* 현재 기계 학습 시스템이 단일 작업과 단일 도메인 데이터셋에 대한 훈련의 한계를 지니고 있으며, 이것이 일반화의 부족을 가져온 주요 요인  
* 복수의 도메인과 작업에 걸쳐 훈련하고 성능을 측정하는 것이 중요  
* 또한, 다중 작업 학습(Multitask learning)이 일반적인 성능 향상을 위한 유망한 방법이지만, 자연어 처리(NLP) 분야에서는 아직 초기 단계  
* 이 연구의 핵심은 언어 모델이 지도 학습 없이도 다양한 하위 작업을 수행할 수 있다는 것을 보여주는 것  
* 이 논문은 언어 모델이 파라미터나 구조의 수정 없이도 '제로샷' 설정에서 여러 작업을 수행할 수 있음을 보여주며, 이러한 접근이 일부 작업에서 경쟁력 있는 결과를 달성할 수 있음을 입증  


# 2. Approach  
## 2.1 Training Dataset  
* 이 연구에서는 과거 언어 모델들이 주로 뉴스 기사, 위키피디아, 소설과 같은 단일 도메인의 텍스트로 훈련  
* 가능한 한 다양하고 광범위한 데이터셋을 구축하여 다양한 도메인과 맥락에서 자연스러운 언어 작업의 시연을 수집하는 것을 목표  
* Common Crawl과 같은 웹 스크랩이 다양하고 거의 무한한 텍스트의 원천이 될 수 있으나, 데이터 품질 문제가 있음  
* 따라서 연구자들은 인간이 큐레이팅/필터링한 웹 페이지만을 스크랩하여 'WebText'라는 데이터셋을 구성  
* 이 데이터셋은 Reddit에서 최소 3개의 카르마를 받은 모든 아웃바운드 링크를 포함하며, 약 8백만 개의 문서로 구성되어 총 40GB의 텍스트를 포함  

## 2.2 Input Representation  
* 일반적인 언어 모델은 모든 문자열의 확률을 계산하고 생성할 수 있어야함.  
* 기존 대규모 언어 모델은 소문자화, 토큰화, OOV(사전에 없는 단어) 토큰과 같은 전처리 단계를 포함  
* 연구진은 바이트 페어 인코딩(Byte Pair Encoding, BPE)을 사용하여 문자 및 단어 수준 언어 모델링 사이의 실용적인 중간 지점을 찾음  
* 이 방법은 빈번한 기호 시퀀스에 대해 단어 수준의 입력을 사용하고 드문 기호 시퀀스에 대해서는 문자 수준의 입력을 사용  
* 이 입력 표현을 사용함으로써, 연구진은 단어 수준 언어 모델의 실증적 이점과 바이트 수준 접근 방식의 일반성을 결합할 수 있었음  

## 2.3 Model  
* 이 연구에서 사용된 모델은 트랜스포머(Transformer) 기반 아키텍처로 구성된 모델  
* 이 모델은 레이어 정규화(layer normalization)가 각 서브 블록의 입력으로 이동되고 최종 자기 주의(self-attention) 블록 후에 추가 레이어 정규화가 추가됨   
* 모델 깊이에 따라 잔여 경로에서의 누적을 고려하는 수정된 초기화를 사용  
* 연구진은 4가지 크기의 모델을 훈련하고 벤치마크했으며, 가장 큰 모델인 GPT-2는 GPT보다 한 자릿수 더 많은 매개변수를 가지고 있음  
* 각 모델의 학습률은 WebText의 5% 유보 샘플에서 가장 좋은 혼란도(perplexity)를 위해 수동으로 조정  


# 3. Experiments  
# 3.1 Language Modeling  
* 언어 모델이 어떻게 '언어 모델링'이라는 기본 작업에 대해 '제로-샷 도메인 전환'(즉, 별도의 학습 없이 새로운 도메인에 적용하는 것)을 수행하는지를 평가  
** 제로-샷 도메인 전환: '제로-샷 도메인 전환'이란 모델이 특정 도메인 또는 데이터셋에 대해 별도로 학습되지 않았음에도 불구하고, 새로운 도메인이나 작업에 적용될 수 있음을 의미. 이는 모델이 기존에 학습한 지식을 다른 상황에 유연하게 적용할 수 있음을 나타냄.  
** 언어 모델링 성능 평가: GPT-2는 바이트 레벨에서 작동하며, 손실이 발생하는 전처리나 토큰화가 필요하지 않음. 따라서 어떤 언어 모델링 벤치마크에서도 평가될 수 있음. 이러한 유연성은 모델이 다양한 데이터셋에 대해 어떻게 일반화하는지를 평가하는 데 중요.  
** 성능 측정 지표: 언어 모델링 데이터셋에서의 결과는 보통 문자, 바이트, 또는 단어 당 평균 음의 로그 확률(negative log probability)의 조정된 버전으로 보고. 이 연구에서는 WebText 언어 모델을 사용하여 데이터셋의 로그 확률을 계산하고, 이를 표준 단위 수로 나누어 성능을 평가​​.  

# 3.2 Children's Book Test  
* 어린이 책 테스트(CBT)는 언어 모델이 이름, 명사, 동사, 전치사 등 다양한 단어 범주에서 어떻게 수행하는지 조사. 
* 이 테스트는 누락된 단어를 맞추는 'cloze' 테스트 형식으로, 언어 모델이 선택지 중 가장 높은 확률을 갖는 단어를 예측하도록 요구  
* 모델 크기가 커질수록 성능이 향상되며, 인간의 성능과 비슷한 수준에 도달.  

# 3.3 LAMBADA  
* LAMBADA 데이터 세트는 텍스트에서 장기적인 의존성을 모델링하는 능력을 테스트.  
* 특히, 사람이 예측하기 위해 최소 50개의 토큰이 필요한 문장의 마지막 단어를 예측하는 것이 목표.  
* 이 연구에서는 기존의 최고 성능을 상당히 뛰어넘는 결과를 보여줌.  

# 3.4 Winograd Schema Challenge  
* 이 챌린지는 시스템이 텍스트에서 모호함을 해결하는 공통 상식 추론 능력을 측정   
* GPT-2는 이전 연구보다 상당히 높은 정확도로 모호함을 해결할 수 있음을 보여줌  

# 3.5 Reading Comprehension  
* CoQA 데이터 세트는 다양한 도메인의 문서와 이에 대한 질문-답변 스타일의 대화를 포함  
* 이는 모델이 대화의 역사를 고려하여 질문에 답하는 능력을 시험  

# 3.6 Summarization  
* GPT-2가 CNN과 Daily Mail 데이터 세트에서 요약 작업을 수행하는 능력을 평가  
* 모델은 주어진 기사에 대해 자동적으로 요약문을 생성하며, 기존의 신경망 기반 기술과 유사한 수준의 성능을 보여줌.  

# 3.7 Translation  
* GPT-2가 언어 간 번역을 수행할 수 있는지를 평가  
* 예를 들어 영어 문장과 프랑스어 문장 쌍을 모델에 제시한 후, 영어 문장에 대한 프랑스어 번역을 생성하도록 함  
* 이 실험에서는 기본적인 번역 능력을 보여줌  

# 3.8 Question Answering  
* 이 부분은 언어 모델이 사실 기반 질문에 얼마나 정확하게 답할 수 있는지를 평가  
* GPT-2는 자연어 질문에 대한 답변을 생성하며, 정확도는 낮지만 모델의 용량이 중요한 요소임을 보여줌  


# 4. Generalization vs Memorization  
* 언어 모델이 데이터를 어떻게 일반화하고 기억하는지에 대해 다룸   
* 언어 모델이 정보를 단순히 기억하는 것인지, 아니면 새로운 상황에 적용할 수 있는 일반적인 규칙을 학습하는지를 탐구  


* GPT-2가 텍스트 데이터를 어떻게 처리하는지에 초점  
** 주요 관심사는 모델이 데이터를 단순히 '기억'하는 것인지 아니면 새로운 상황에 적용할 수 있는 일반적인 규칙을 '일반화'하는지 여부  
** 모델이 단순히 학습 데이터를 암기하는 것과 실제로 언어의 구조와 의미를 이해하여 새로운 상황에 적용하는 것 사이에는 큰 차이가 있기 때문  
** GPT-2가 다양한 텍스트 샘플에서 얼마나 잘 일반화하는지, 그리고 기억에 의존하는 정도를 평가하기 위해 여러 실험을 수행
** 이러한 실험은 모델이 단순한 암기보다는 보다 복잡한 언어 이해와 처리 기능을 발휘하는지를 평가하는 데 중점  
** 또한, 모델이 새로운 상황이나 데이터에 얼마나 잘 적응하는지, 그리고 학습 과정에서 얼마나 많은 정보를 '기억'하는지를 분석  
* 결과적으로, 언어 모델의 능력을 평가하는 데 있어 일반화와 기억화 사이의 균형을 이해하는 것이 중요  
** 이는 언어 모델의 실제 성능과 응용 가능성을 평가하는 데 있어 핵심적인 요소  


# 5. Related Work  
* 언어 모델의 성능 개선: 이전 연구들은 대규모 데이터셋에서 큰 언어 모델의 성능을 측정하고 개선  
** 예를 들어, Dai와 Le (2015)는 RNN 기반의 미세 조정(fine-tuning) 접근법을 개선했으며, Conneau 등(2017a)은 자연 언어 추론 모델에 의해 학습된 표현의 전이 성능을 연구  
** Subramanian 등(2018)은 대규모 다중 과제 학습에 대해 탐구  

*Seq2Seq 모델과 LM 프리트레이닝: Ramachandran 등(2016)은 seq2seq 모델이 사전 훈련된 언어 모델을 인코더와 디코더로 초기화함으로써 이익을 얻는다는 것을 보여줌  
** 더 최근의 연구(Wolf 등 2019, Dinan 등 2018)는 LM 프리트레이닝이 대화 기반 질문 응답 시스템과 같은 어려운 생성 과제에 유용함을 보여줌  

* 다양한 연구들:   
** Jozefowicz 등(2016)은 10억 단어 벤치마크에서 RNN 기반 언어 모델을 확장  
** Bajgar 등(2016)은 기존 어린이 책 테스트 데이터셋을 프로젝트 구텐베르크에서 생성한 더 큰 훈련 데이터셋으로 보완하여 결과를 개선  
** Hestness 등(2017)은 모델 용량과 데이터셋 크기에 따른 다양한 딥 러닝 모델의 성능 변화를 철저히 분석  

* 생성 모델의 학습된 기능성: 이전에 문서화된 예로는 RNN 언어 모델의 셀이 줄 너비 추적과 인용/주석 감지를 수행하는 것  
** Liu 등(2018)은 위키피디아 기사를 생성하기 위해 훈련된 모델이 언어 간 이름 번역을 학습했다는 관찰이 이 연구에 영감  

* 텍스트 코퍼스 구축 방법: 이전 연구들은 웹 페이지의 대규모 텍스트 코퍼스를 구축하고 필터링하는 대안적 접근법을 탐구  
** 예를 들어, iWeb 코퍼스(Davies 2018)가 있음  

* 언어 과제를 위한 사전 훈련 방법: 
** GloVe(Pennington 등 2014)는 Common Crawl의 모든 단어 벡터 표현 학습을 확장  
** Skip-thought 벡터(Kiros 등 2015)는 텍스트에 대한 깊은 표현 학습에 대한 초기 중요한 연구  
** McCann 등(2017)은 기계 번역 모델에서 파생된 표현의 사용을 탐구  



# 6. Discussion  
* 언어 모델의 표현 학습: 이전 연구들은 지도학습 및 비지도학습 사전 훈련 방법의 표현을 학습하고, 이해하며, 비판적으로 평가하는 데 집중  
** GPT-2의 결과는 비지도학습 작업 학습이 탐색할 가치가 있는 추가적인 연구 영역임을 제안  
** 이러한 발견은 사전 훈련 기술이 하류 NLP 작업에서 널리 성공할 수 있는 이유를 부분적으로 설명  

* 독해력 테스트 성능: GPT-2는 독해력 테스트에서 지도학습 기반 기준과 비슷한 경쟁력 있는 성능을 제로-샷 설정에서 보여줌    
** 그러나 요약과 같은 다른 작업에서는 양적 지표에 따르면 여전히 기본적인 수준의 성능  
** 실제 응용에 있어서 GPT-2의 제로-샷 성능은 아직 사용하기에는 미흡  

* 다양한 NLP 작업에서의 성능: GPT-2는 많은 전통적인 NLP 작업에서 제로-샷 성능을 연구했지만, 평가할 수 있는 추가적인 작업들이 많이 있음  
** 여전히 많은 실용적인 작업에서 GPT-2의 성능이 무작위 선택보다 나은 것은 아님    
** 질문 응답 및 번역과 같은 공통 작업에서도 언어 모델은 충분한 용량을 갖추었을 때에만 기본 기준을 능가하기 시작  

* 세부 조정과 성능의 상한: 제로-샷 성능은 GPT-2가 많은 작업에서 가능한 성능의 기준을 설정하지만, 세부 조정으로 얻을 수 있는 최대 성능은 불분명  
** GPT-2의 전적으로 추상적인 출력은 많은 질문 응답 및 독해력 데이터셋에서 현재 최고의 기술인 추출적 포인터 네트워크 기반 출력과 상당히 다름  
** 따라서, GPT의 성공적인 세부 조정을 고려할 때, decaNLP 및 GLUE와 같은 벤치마크에서의 세부 조정을 조사할 계획  
** GPT-2의 추가적인 훈련 데이터와 용량이 BERT에 의해 입증된 단방향 표현의 비효율성을 극복할 수 있는지는 불분명   

# 7. Conclusion  
* 크고 다양한 데이터셋으로 훈련된 대규모 언어 모델은 다양한 도메인과 데이터셋에서 높은 성능을 발휘  
* GPT-2는 테스트된 8개의 언어 모델링 데이터셋 중 7개에서 최고 수준의 성능을 제로-샷 설정으로 달성  
* 모델이 제로-샷 설정에서 수행할 수 있는 작업의 다양성은 충분히 다양한 텍스트 코퍼스의 가능성을 최대화하기 위해 훈련된 고용량 모델이 명시적인 감독 없이도 많은 작업을 수행하는 방법을 배울 수 있음을 시사  
* 이 결론은 GPT-2가 언어 모델링 분야에서 중요한 진전을 나타내며, 대규모 모델이 다양한 자연어 처리 작업을 수행할 수 있는 잠재력을 가지고 있음을 강조  
* 이러한 모델은 지도 학습 없이도 다양한 작업을 수행하는 능력을 보여주며, 이는 언어 모델링 분야에서 중요한 발전  
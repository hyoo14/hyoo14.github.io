---
layout: post
title:  "[2019]Language Models are Unsupervised Multitask Learners"
date:   2023-12-30 18:07:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    
* 자연어 처리 작업에 대해 다루고 있습니다. 이 연구에서는 질문 답변, 기계 번역, 독해 및 요약과 같은 작업들이 일반적으로 특정 작업에 대한 지도 학습을 통해 접근된다고 언급합니다. 그러나 연구진은 새로운 데이터셋인 WebText를 통해 언어 모델이 이러한 작업들을 명시적인 지도 없이도 학습하기 시작한다는 것을 보여줍니다. 이러한 조건에서, 언어 모델이 생성한 답변은 CoQA 데이터셋에서 55 F1 점수를 달성하여, 127,000개 이상의 훈련 예제를 사용하지 않고도 4개 기준 시스템 중 3개의 성능과 맞먹거나 능가합니다. 이 논문에서 소개된 가장 큰 모델인 GPT-2는 1.5B 파라미터의 트랜스포머 모델로, 8개의 테스트된 언어 모델링 데이터셋 중 7개에서 최고의 결과를 달성합니다. 이러한 결과들은 언어 처리 시스템이 자연스럽게 발생하는 시연을 통해 작업을 수행하는 방법을 학습하는 것에 대한 유망한 경로를 제시합니다​​.  

Useful sentences :  
*   

{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1pRHgqTRjZ9l3aVzePZ7_Dd-PkNWs_w3p?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 
<br/>

# 1. Introduction  
* 논문 "Language Models are Unsupervised Multitask Learners"의 서론 부분은 기계 학습 시스템의 현재 상태와 이 연구의 방향성에 대해 설명합니다. 현재의 기계 학습 시스템은 대규모 데이터셋과 고용량 모델, 지도 학습을 사용하여 특정 작업에 대해 뛰어난 성능을 보이지만, 이러한 시스템들은 데이터 분포나 작업 사양의 작은 변화에도 취약하다고 언급합니다. 이들은 특정 작업에 대해서만 전문적인 기능을 수행하는 '좁은 전문가(narrow experts)'로 묘사되며, 연구자들은 보다 다양한 작업을 수행할 수 있는 '능숙한 일반가(competent generalists)'를 개발하고자 합니다.

이 논문은 현재 기계 학습 시스템이 단일 작업과 단일 도메인 데이터셋에 대한 훈련의 한계를 지니고 있으며, 이것이 일반화의 부족을 가져온 주요 요인이라고 지적합니다. 복수의 도메인과 작업에 걸쳐 훈련하고 성능을 측정하는 것이 중요하다고 제안합니다. 또한, 다중 작업 학습(Multitask learning)이 일반적인 성능 향상을 위한 유망한 방법이지만, 자연어 처리(NLP) 분야에서는 아직 초기 단계라고 언급합니다.

이 연구의 핵심은 언어 모델이 지도 학습 없이도 다양한 하위 작업을 수행할 수 있다는 것을 보여주는 것입니다. 이 논문은 언어 모델이 파라미터나 구조의 수정 없이도 '제로샷' 설정에서 여러 작업을 수행할 수 있음을 보여주며, 이러한 접근이 일부 작업에서 경쟁력 있는 결과를 달성할 수 있음을 입증합니다​​.


# 2. Approach  
## 2.1 Training Dataset  
이 연구에서는 과거 언어 모델들이 주로 뉴스 기사, 위키피디아, 소설과 같은 단일 도메인의 텍스트로 훈련되었다고 언급합니다. 연구자들은 가능한 한 다양하고 광범위한 데이터셋을 구축하여 다양한 도메인과 맥락에서 자연스러운 언어 작업의 시연을 수집하는 것을 목표로 합니다. Common Crawl과 같은 웹 스크랩이 다양하고 거의 무한한 텍스트의 원천이 될 수 있으나, 데이터 품질 문제가 있습니다. 따라서 연구자들은 인간이 큐레이팅/필터링한 웹 페이지만을 스크랩하여 'WebText'라는 데이터셋을 만들었습니다. 이 데이터셋은 Reddit에서 최소 3개의 카르마를 받은 모든 아웃바운드 링크를 포함하며, 약 8백만 개의 문서로 구성되어 총 40GB의 텍스트를 포함합니다​​.

## 2.2 Input Representation  
일반적인 언어 모델은 모든 문자열의 확률을 계산하고 생성할 수 있어야 합니다. 기존 대규모 언어 모델은 소문자화, 토큰화, OOV(사전에 없는 단어) 토큰과 같은 전처리 단계를 포함합니다. 연구진은 바이트 페어 인코딩(Byte Pair Encoding, BPE)을 사용하여 문자 및 단어 수준 언어 모델링 사이의 실용적인 중간 지점을 찾았습니다. 이 방법은 빈번한 기호 시퀀스에 대해 단어 수준의 입력을 사용하고 드문 기호 시퀀스에 대해서는 문자 수준의 입력을 사용합니다. 이 입력 표현을 사용함으로써, 연구진은 단어 수준 언어 모델의 실증적 이점과 바이트 수준 접근 방식의 일반성을 결합할 수 있었습니다​​.

## 2.3 Model  
이 연구에서 사용된 모델은 트랜스포머(Transformer) 기반 아키텍처를 사용합니다. 이 모델은 OpenAI GPT 모델의 세부 사항을 대체로 따르지만, 몇 가지 수정이 있습니다. 예를 들어, 레이어 정규화(layer normalization)가 각 서브 블록의 입력으로 이동되고 최종 자기 주의(self-attention) 블록 후에 추가 레이어 정규화가 추가됩니다. 모델 깊이에 따라 잔여 경로에서의 누적을 고려하는 수정된 초기화를 사용합니다. 연구진은 4가지 크기의 모델을 훈련하고 벤치마크했으며, 가장 큰 모델인 GPT-2는 GPT보다 한 자릿수 더 많은 매개변수를 가지고 있습니다. 각 모델의 학습률은 WebText의 5% 유보 샘플에서 가장 좋은 혼란도(perplexity)를 위해 수동으로 조정되었습니다​​.


# 3. Experiments  
# 3.1 Language Modeling  
# 3.2 Children's Book Test  
# 3.3 LAMBADA  
# 3.4 Winograd Schema Challenge  
# 3.5 Reading Comprehension  
# 3.6 Summarization  
# 3.7 Translation  
# 3.8 Question Answering  

# 4. Generalization vs Memorization  

# 5. Related Work  

# 6. Discussion  

# 7. Conclusion  
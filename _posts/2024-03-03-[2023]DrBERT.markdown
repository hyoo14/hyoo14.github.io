---
layout: post
title:  "[2023]DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains"  
date:   2024-03-03 18:19:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    

* 사전 훈련된 언어 모델들이 의료 분야에서 높은 성능을 보였으며, 이는 주로 영어 자료에 기반함
* 이 연구에서는 프랑스어 기반 공개 웹 데이터와 개인 의료 데이터로 훈련된 여러 PLMs를 비교 분석
* PLMs의 성능을 최적화하기 위한 다양한 학습 전략을 평가  
* 특히, "DrBERT"라는 프랑스어 기반 바이오메디컬 전문 PLM을 소개  
** 이는 현재까지 자유 라이선스로 이용 가능한 가장 큰 의료 데이터 코퍼스에서 훈련됨  
* 특정 도메인에서 PLMs의 효과성을 향상시킬 수 있는 새로운 방안(다양한 의료 관련 텍스트 소스를 크롤링-질병 및 상태에 대한 설명, 치료 및 약물 정보, 일반 건강 관련 조언, 공식 과학 회의 보고서, 익명화된 임상 사례, 과학 문헌, 논문, 프랑스어 번역 쌍, 대학 건강 과정)을 제시   


* Pre-trained language models have shown high performance in the medical field, mainly based on English resources
* This study compares several PLMs trained on French-based public web data and private medical data
* Evaluates various learning strategies to optimize the performance of PLMs
* Introduces "DrBERT," a French-based biomedical specialized PLM
** This is trained on the largest medical data corpus available under a free license to date
* Presents a new approach to enhance the effectiveness of PLMs in specific domains by crawling various medical related text sources(descriptions of diseases and conditions, treatment and medication information, general health advice, official scientific conference reports, anonymized clinical cases, scientific literature, theses, French translation pairs, university health courses)




Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1m06sdY73i6gCXT-akaT-Q6XmvEECA943?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* scratch: 처음부터 시작함   
* scratch of a new model: 새 모델을 처음부터 만드는 것  
* constrained: 제한된, 구속된  
* constrained number: 제한된 수량  
* disseminate: 퍼뜨리다, 전파하다  
* socio-demographic: 사회 인구학적을 의미하는 말로, 사회적 및 인구학적 특성을 결합한 용어. 이는 개인이나 그룹의 나이, 성별, 직업, 교육 수준, 소득, 결혼 상태, 거주 지역 등 사회적 및 경제적 배경에 관한 정보를 포함. 사회 인구학적 정보는 사람들의 행동, 선호도, 필요 등을 이해하는 데 중요하며, 시장 조사, 공중 보건, 사회학적 연구 등 다양한 분야에서 활용  
* posology: 약학에서 약물의 용량, 투여 방법, 투여 빈도 등을 연구하는 학문 영역을 의미  
* corroborating: 뒷받침하는, 확인하는  
* aggregated: 집계된, 모아진  


<br/>

# 1 Introduction  
* 최근 몇 년 동안 사전 훈련된 언어 모델이 다양한 자연어 처리 과제에서 최고의 성능을 달성하고 있음  
*처음에는 일반 도메인 데이터로 훈련된 모델들이었지만, 특정 도메인을 더 효과적으로 처리하기 위해 전문화된 모델들이 등장하기 시작함  
* 이 논문에서는 프랑스어 의료 도메인에 대한 사전 훈련된 언어 모델의 원래 연구를 제안함  
* 처음으로 웹에서 수집한 공개 데이터와 의료 기관에서 수집한 비공개 데이터에 대해 훈련된 언어 모델의 성능을 비교함  
* 또한 생물 의학적 과제 세트에 대한 다양한 학습 전략을 평가함  
* 특히, 이미 존재하는 생물 의학적 사전 훈련된 언어 모델을 우리의 목표 데이터에 추가로 사전 훈련함으로써 이점을 얻을 수 있음을 보여줌  
* 마지막으로 프랑스어 생물 의학 분야를 위한 첫 번째 전문화된 사전 훈련된 언어 모델인 DrBERT와 이 모델이 훈련된 가장 큰 의료 데이터 코퍼스를 공개함  
<br/>

# 2 Related work  
* BERT는 마스크된 언어 모델 개념과 양방향 트랜스포머를 사용하여 사전 훈련된 문맥화된 단어 표현 모델로, 2018년 Devlin et al.에 의해 제안됨  
* 이후로 RoBERTa(Liu et al, 2019)와 같은 새로운 방법이 등장하여 BERT의 초기 모델을 개선함  
* RoBERTa는 여러 가지 설계 변경을 통해 BERT 모델을 개선함  
* CamemBERT(Martin et al, 2020)는 프랑스어 서브셋 OSCAR 코퍼스를 기반으로 한 RoBERTa 기반 모델임   
* 최근에는 영어를 주로 사용하는 생물 의학 및 임상 분야를 위한 언어 모델이 개발되었음  
* 이 모델들은 트랜스포머 기반 아키텍처의 비감독 사전 훈련을 사용함  
* BioBERT(Lee et al, 2019), BlueBERT(Peng et al, 2019), ClinicalBERT(Huang et al, 2019) 등이 이에 해당함  
* 영어 이외의 언어에서는 BERT 기반 모델이 드물며, 대부분 지속적인 사전 훈련에 의존함  
* 스페인어(Carrino et al, 2021)와 터키어(Türkmen et al, 2022) 모델만이 처음부터 훈련됨  
* 프랑스어에 대해서는 생물 의학 도메인을 위해 특별히 구축된 공개적으로 이용 가능한 모델이 없음  
<br/>

# 3 Pre-training datasets  
* 데이터 원본은 학습의 대상이 되는 목표 하위 작업에 맞춰진 데이터 소스의 중요성을 강조한 이전 작업에서 강조됨  
* 그러나 의료 데이터는 사용자 데이터 보호, 환자의 건강 정보 보호 등 민감한 성격 때문에 획득하기 매우 어려움  
* 대량의 웹 데이터 수집은 이 부족함을 극복할 수 있는 해결책으로 보임  
* 하지만 이 웹 문서들은 품질 면에서 다양함  
* 웹에서 특정 도메인 데이터를 기반으로 한 PLM과 임상 데이터 웨어하우스의 비공개 문서를 기반으로 한 PLM 간의 비교는 이루어지지 않았음  
* 프랑스어를 위한 두 가지 다른 의료 데이터셋이 추출됨  
* 첫 번째는 다양한 무료 온라인 소스에서 크롤링한 데이터를 모음  
* 두 번째는 낭트 대학 병원의 비공개 병원 체류 보고서임  
* 공개 웹 기반 데이터는 NACHOSlarge라는 코퍼스를 구성하며, 7.4GB의 데이터를 포함함  
* 비공개 데이터셋은 NBDWsmall이라고 하며, 4GB의 데이터를 포함함  
* 비교 가능한 실험을 수행하기 위해, 비공개 데이터와 동일한 크기의 NACHOS 서브 코퍼스(NACHOSsmall)를 추출함  
* 마지막으로, 두 데이터셋 모두에 적용된 전처리 단계를 설명함  
** 제공된 텍스트 데이터는 SentencePiece(Kudo and Richardson 2018)를 사용하여 서브워드 단위로 분할되었음  
** SentencePiece는 Byte-Pair Encoding(BPE)(Sennrich et al, 2016)과 WordPiece(Wu et al, 2016)의 확장으로, 사전 토크나이징(단어 또는 토큰 수준에서)이 필요 없어 언어별 토크나이저가 필요 없음  
** 32k 서브워드 토큰의 어휘 크기를 사용함  
** 처음부터 사전 훈련된 모델(4.2절 참조)에 대해, 사전 훈련 데이터셋의 모든 문장을 사용하여 토크나이저를 구축함  
<br/>


# 4 Models pre-training   
* 이 섹션에서는 사전 훈련된 언어 모델들을 다양한 의료 도메인 데이터를 사용하여 평가한 내용을 다루고 있음  
* 여기서는 두 가지 주요 데이터셋, 즉 NACHOSsmall과 NBDWsmall에 대해 언급되어 있으며, 거의 두 배의 데이터를 가진 NACHOSlarge 데이터를 사용한 모델의 성능도 조사됨  
* 또한 공개 NACHOSsmall과 비공개 NBDWsmall 소스를 결합한 총 8GB의 데이터(NBDWmixed)를 사용하여 사전 훈련한 모델도 탐색함으로써 공개 데이터와 비공개 데이터를 결합하는 것이 저자원 도메인에서 실행 가능한 접근 방식인지를 보여주고자 함  

* 사전 훈련 전략으로는 세 가지가 있음:  
1. 서브워드 토크나이저를 포함한 전체 모델을 처음부터 훈련하는 것  
2. 프랑스어에 대한 현재 최고의 언어 모델인 CamemBERT를 의료 특정 데이터에 대해 추가로 사전 훈련하면서 초기 토크나이저를 유지하는 것  
3. 영어로 된 의료 분야에 특화된 언어 모델인 PubMedBERT를 프랑스어 의료 데이터에 추가로 사전 훈련하는 것  

* 특히, 세 번째 전략은 프랑스어 의료 데이터에 추가로 사전 훈련된 영어 의료 모델의 성능을 일반적인 프랑스어 모델을 기반으로 한 다른 모델과 비교하는 것을 목표로 함  
* 의료 분야에서 언어 간에 공유되는 용어가 많기 때문에 두 언어 간의 자원 혼합이 유용할 수 있음을 보여주고자 함  

* 모델 아키텍처는 모두 CamemBERT 기본 구성을 사용하며, 이는 RoBERTa 기본 아키텍처와 동일함  
* 이 아키텍처에는 12개의 레이어, 768개의 숨겨진 차원, 12개의 주의 헤드, 110M개의 매개변수가 포함됨.  자원 제한으로 인해 모델의 대형 버전은 훈련되지 않았음  


<br/>

# 5 Downstream evaluation tasks  
* 다운스트림 평가 과제는 프랑스어 의료 도메인에 특화된 사전 훈련 언어 모델의 성능을 검증하기 위한 여러 NLP 작업으로 구성됨  
*여기에는 공개적으로 이용 가능한 데이터셋과 비공개 데이터셋이 모두 포함됨  
* 이 연구에서는 특히 세 가지 주요 작업을 중점적으로 평가함  
** 첫 번째 작업은 의료 보고서에서 명명된 개체를 식별하는 것으로, 금색 표본에서 얻은 100개의 긴 의료 보고서를 대상으로 함  
** 이 보고서들은 프랑스어 음성 전사에서 가져온 것이며, 나이, 도시, 날짜, 이메일, 병원, 전화번호, 복용량, 기간, 형태, 약물 및 용법과 같은 12개의 클래스로 명명된 개체가 BIO 형식을 사용하여 주석이 달림  
** 두 번째 작업은 급성 심부전 구조화를 위한 의료 보고서 작성으로, 낭트 대학 병원에서 나온 350개의 병원 체류 보고서(총 3511 문장으로 나뉨)를 대상으로 함  
*** 이 보고서들은 만성 심부전의 원인, 급성 심부전을 유발하는 요인, 당뇨병, 흡연 상태, 심박수, 혈압, 체중, 신장, 의료 치료, 고혈압 및 좌심실 박출률과 같은 임상 정보에 관련된 46개의 개체 유형으로 주석이 달림  
** 세 번째 작업은 급성 심부전 분류로, 낭트 대학 병원에서 나온 1639개의 병원 체류 보고서를 대상으로 하며, 이 보고서들은 급성 심부전의 진단 유무에 따라 긍정적 또는 부정적으로 라벨링됨  
** 비공개 작업 중 하나는 의료 보고서의 전문 분야를 분류하는 작업으로, 전사된 의료 보고서의 전문 분야를 지정하는 분류 작업임  
*** 이 데이터셋은 7356개의 프랑스어 의료 보고서로 구성되며, 정신과, 비뇨기과, 내분비학, 심장학, 당뇨병학, 감염학 등 6개의 전문 분야에 걸쳐 수동으로 주석이 달린 후 동등하게 샘플링됨  

<br/>  

# 6 Results and Discussions  
* 연구는 의료 보고서의 명명된 개체 식별, 급성 심부전 구조화, 급성 심부전 분류 등의 주요 작업에 초점을 맞추고 있으며, 이러한 작업들을 통해 모델들의 성능을 비교  
* 사전 훈련 전략의 영향력에 대해서는, 처음부터 완전히 사전 훈련된 모델들(DrBERT와 ChuBERT)이 공개 및 비공개 데이터 소스와 과제들 모두에서 가장 좋은 결과를 내는 경향이 있음을 관찰  
* 이 모델들은 사전 훈련 데이터의 양이나 종류에 관계없이 모든 비공개 작업에서 최고의 결과를 얻었고, 공개 작업의 거의 모든 경우(7개 중 5개)에서도 최고의 결과를 달성  
* 또한, 영어 의료 모델인 PubMedBERT를 프랑스어 의료 데이터로 추가 사전 훈련한 모델이 일반 프랑스어 모델을 기반으로 한 다른 모델들과 비교할 때 공개 작업의 나머지 2개에서 더 나은 성능을 보임을 확인  
* 이는 영어와 프랑스어 의료 데이터 사이에서 언어 간 지식 전달이 유효함을 보여줌  
* 사전 훈련 데이터의 양에 관해서는, 사전 훈련에 사용된 데이터의 양이 많을수록 모델의 성능이 향상되는 경향이 있으나, 대부분의 작업에서 소량의 데이터로 훈련된 시스템이 큰 데이터 모델에 이어 2위를 차지하는 등 큰 차이는 보이지 않음  
* 웹 기반 소스에서 사전 훈련된 모델이 공개 작업에 적용될 때 특히 우수한 성능을 보이며, 이는 공개 데이터셋에 포함된 정보와 웹에서 수집된 데이터 사이에 높은 연관성이 있음을 시사함  
* 비공개 작업에서는 NBDW 기반 모델이 공개 및 비공개 의료 데이터를 혼합한 경우(Table 6 참조)에 더 나은 성능을 보임  
* 일반 도메인 NLP 작업에 대한 모델들의 성능에서는 모든 모델의 성능이 모든 작업에서 감소함을 관찰  
* 가장 큰 성능 감소는 자연어 추론 작업에서 나타났으며, 이는 특화된 모델들이 다른 작업으로 일반화하기 어렵다는 것을 시사  
<br/>

# 7 Conclusion  
* 이 작업에서는 프랑스어를 위한 최초의 생물 의학 및 임상 분야에 특화된 트랜스포머 기반 언어 모델을 제안했음  
* 이 모델은 RoBERTa 아키텍처를 기반으로 하며, 공개 및 비공개 의료 도메인 작업 모음에 대한 이 모델들의 평가를 실시함으로써, 이 특정 모델들의 광범위한 평가 연구가 수행  
* 오픈 소스 DrBERT 모델들은 프랑스어 일반 모델(CamemBERT)과 영어 의료 모델(BioBERT, PubMedBERT, ClinicalBERT) 모두를 상대로 모든 의료 작업에서 최신 기술의 성능을 향상  
* 또한 웹에서 크롤링한 의료 데이터 4GB로 사전 훈련하는 것만으로도 의료 보고서에서 수집한 전문 데이터로 훈련된 모델들과 경쟁하거나 심지어 종종 능가할 수 있음을 보여줌  
* 결과는 또한 기존의 영어 도메인 특화 모델, 여기서는 PubMedBERT에 대한 지속적인 사전 훈련이 프랑스어 도메인 일반 모델에 대한 사전 훈련보다 프랑스어 생물 의학 하류 작업을 목표로 할 때 더 실행 가능한 솔루션이라는 것을 강조  
* 이 접근법의 성능을 더 많은 데이터를 사용하여 추가로 조사할 필요가 있음  
* DrBERT와 NACHOSlarge로 수행된 작업과 유사하게 조사할 필요가 있음  
* 사전 훈련된 모델들과 사전 훈련 스크립트는 온라인에서 MIT 오픈 소스 라이선스 하에 공개적으로 공개됨  
* NACHOS 데이터셋의 주요 목적은 커뮤니티에서 강력한 NLP 도구의 개발을 촉진하는 것임  
<br/>  

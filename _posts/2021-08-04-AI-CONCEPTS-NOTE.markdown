---
layout: post
title: AI CONCEPTS NOTE
date: '2021-08-04 11:52:10 +0900'
categories: _NLP study
published: true
---


한번 정리하고 넘어가고 싶어서 개념들 서술해봅니다.



-2-

랭기지모델 학습 - 단어 분포 모사. 마코브 어썸션 도입하여 더 낮은 엔그램으로 근사

좋은 모델 : 언어분포 잘 모사. 잘 정의된 테스트셋에 높은 확률 반환

펄플렉시티: 몇개중에 헷갈리고 있냐. 작을 수록 좋은 것

뉴럴랭기지 모델은 제너럴라이즈가 잘 된다는 것이 장점, 등장하지 않은 단어들도 처리 가능

엔트로피 불확실성 나타냄. 자주 일어나는 사건은 낮은 정보량. 드물게 발생하면 높은 정보량 가짐.
불확실성은 1/등장확률 에 비례하고, 정보량에 반비례
확률 낮은 것일 수록 정보량 많은 것, 불확실성 큰 것
정보량은 마이너스 로그 확률. 확률 높아질수록 정보량 낮아짐. 0가까워질수록 높아짐

엔트로피 높을수록 플랫 디스트리뷰션, 낮을수록 샤프한 디스트리뷰션

크로스엔트로피는 퍼플렉시티의 로그취한 값.
퍼플렉시티 작게하는 것이 목적으로 이는 크로스엔트로피 익스퍼넨셜 미니마이즈.
즉, 크로스엔트로피 미니마이즈로 볼 수 있음

클래시피케이션이니까 크로스엔트로피 쓴다고 봐도 되고
두 분포 차이 미니마이즈니까 크로스엔트로피 미니마이즈.
아키르릴후드 맥시마이즈 할꺼니까 네거티브 라이클리후드 마니마이즈 하는 건데 이게 크로스엔트로피 미니마이즈와 같음.

크로스엔트로피 로그 취하면 퍼플렉시티.
퍼플렉시티 익스퍼넨셜 취하면 크로스엔트로피임.

seq2seq의 many to many는 many to one 과 one to many 로 이해하는 것이 좋음

non-auto-regessive-현재 상태가 앞 뒤 문맥에 의해 정해짐
auto-regressive- 과거상태 의존

티처포싱 안 하면 many2many 개수 안 맞을 수도. eos먼저 뜨면
티처포싱 안 하면 이전 출력에 따라 현재 스테이트 달라짐 mle 이상해짐
mle다르게 수식 적용
그래서 실제 정답을 넣으줌. 그래서 학습이랑 테스트(추론) 다로 2개 짜야함
티처포싱 성능 저하될 수 있으나 기본적 성능 좋아서 걍 쓰면 되지만.. 왜 저하되냐면 순서까지도 기억해주는 오버피팅이 되기때문. 그래서 보통 반반티처포싱 등 사용

fluent한 문장 골라내는 일이나 다음단어 뽑아내는 일은 언어모델로 사실 같음.


-4-
seq to seq 도 auto encoder와 비슷
autoencoder - 특징 추출 하는 것.(차원축소(latent space:잠재 feature의 공간?) 및 복원을 통해)

decoder는 conditional language model이라고 볼 수 있음-인코더로부터 문장을 압축한 context vector를 바탕으로 문장생성
(conditional? 조건부 확률 느낌?)

classification(단어선택), discrete value 예측하는 거니까 크로스엔트로피 쓰면 됨. (소프트맥스 결과값에)
MLE중이니까 negative log likelihood 미니마이즈해야히니까 크로스엔트로피.
디코더가 컨디셔널랭기지 모델이니깐 퍼플렉시티 미니마이즈 해야하는 것. 그니깐 크로스엔트로피 익스포넨셜
이 코르스엔트로피니깐. 크로스엔트로피 쓰면 
결국 ground truth 분포와 모델 분포 삳이의 차이를 최소화하기 위함
(Ground-truth는 학습하고자 하는 데이터의 원본 혹은 실제 값)

어텐션?
키-벨류 펑션으로 미분가능함
파이선의 딕셔너리= {K:v1, K2:v2} ,,, d[K] 벨류리턴 - (딕셔너리설명)
기존 딕셔너리 key-value 펑션과 달리 query와 key의 "유사도"에 따라 value 반환!(weighted sum으로)
->lstm hidden state한계인 부족정보를 직접 encoder에서 조회해서 예측에 필요한 정보 얻어오는 것.
정보를 잘 얻어오기(이 과정이 attention) 위해 query 잘 만들어내는 과정을 학습 

attention은 QKV.
Q:현재 time-step의 decoder의 output
K: 각 time-step 별 encoder의 output
V: 각 time-step 별 encoder의 output
q와 k의 유사도 계산(encoder output token들)
유사도를 각각 encoder output toekn들에 곱해주고 더해서 현재 context vector 만들어줌.
쿼리 날린 decoder 히든스테이트와 context vector를 컨캣해줘서 새로운 히든스테이트를 얻음(이것이 반영 된 것)
근데 유사도 구할 때 유사도 팍팍 안 구해짐. 그래서 linear transform(linear layer 통과시킴) 후에 유사도 얻어옴.
그래서 우리는 linear transform을 학습해서 유사도를 잘 받아오게 학습해야함. 이것도 잘 학습해야함

비유해보면 "오리역에서 가장 편하게 밥먹는 집 어디야?" -> "오리역 가정식 백반집"(쿼리 바꾸는 거 학습한 것)
마음속의 상태(state)를 잘 반영하면서 좋은 쿼리를 만들기 위함임.
어텐션은 "쿼리를 잘 만들어내는(변환) 과정" 배우는 것이다. 여기서 batch matrix multiplication(BMM) 사용(행렬들 곱)
닷프로덕트가 코싸인시뮬러리티와 비슷. 소프트맥스까지 쒸우면 유사도라고 볼 수 있음

<PAD> 위치에 weight가 가지 않도록 하는 안전장치 추가->이것이 마스킹


input feeding? 샘플링과정서 손실되는 정보 최소화, 티쳐포싱으로 인한 학습/추론 사이의 괴리 최소화
인풋피딩? 아웃풋 히든스테이트를을 다음 인풋 히든스테이트에 컨캣

auto-regressive : 과거 자신의 상태를 참조하여 현재 자신의 상태를 업데이트. 시퀀스에서 이전 단어 보고
다음 단어 예측하는 것.

teacher forcing 통해 auto-regressive task에 대한 sequential modeling 가능. 하지만 training mode 와 inference mode의 괴리
(discrepancy) 생김

(dilde : ~ (물결표) )

pytorch ignite로 procedure 짜놓을 수 있음(lightening도 가능)

-5-

NLG - 가장 높은 확률 갖는 문장 예측. 일종의 shortest path 찾는 것으로 볼 수 있두아!

beam search 통해 greedy search 단점 	보완 가능(어느정도)

재미있어야하는 챗봇에서는 랜덤샘플링도 ㄱㅊ. 테스크에 따라 .. 번역에서는 일관성이 떨어져서 좀 그럼.

coverage penalty= 어텐션이 time step마다 다른 곳에 집중해야 좋은 번역일 것이라 생각(다 같은 곳만 집중시 한계니)
페널티로 성능 변화는 미미하긴함


그래서 트레이닝 vs 인퍼런스?
트레이닝은 티쳐포싱 통해서 한 스텝마다 정답이 나오게..(다음 스텝 인풋은 정답)
인퍼런스는 스텝 밟아가면서,, 스텝 결과값이 다음 인풋, 또 이 결과값이 또 다음 스텝의 인풋으로 쭉쭉 가는 것.

인퍼런스는 다음 타임스텝에 넣어주고 학습은 티처포싱이었음..(참고)	
이것은 오토리그레시브 속성(이전 타임스텝에 영향을 받ㄴ는 것. 쭉쭉)

기존에 포워드함수 하나 짜면 학습과 추론 다 되었다면, (학습 추론 같았기 때문.)
하지만 NLG task(auto regressive 속성 띄기 때문) 트레이닝과 인퍼런스 함수 따로 만들어줘야.
그래서 트레이닝함수 구현 했어도 서치라는 추론함수 만들어줌. 두 함수가 괴리가 일어나고
그래서 강화학습이나 듀얼러닝 같은 것들 사용해서 괴리 줄여줌

-6-
  
test set 적격성

noise 포함되지 않았는가?
실제 deploy 할 때와 같은 domain 인가? (이상적으로는 TRAIING 할 때와 같은 DOMAIN)

scoring metric. 해당 task 채점하기에 적헐한 metric 인가?


평가 크게 2가지
정성평가(INTRINSIC) 
-사람이
-인건비 비싸고 속도 느리고 주관개입가능(단)
-절대평가/상대평가 두가지 방법이 있음

정량평가(EXTRINSIC)
-자동으로
-저렴비용, 빠른속도, 객관적 평가가능(장)
-정확도가 낮을 수 있음(장)

-크로스엔트로피, 펠플렉시티로 평가(예)
-평가방법으로 BLEU가 있음

실제 DEPLOY를 위해서는 INTRINSIC EVALUATION을 꼭 거쳐야함.
(잦은 업데이트의 경우 EXTRINSIC EVALUATION) 
(배포할 때는 INTRINSIC 써서 비교통해 이전 모델과 비교 후 배포)


<BLEU -블루 매트릭>
테스트 문장에 대해 확률을 높게 반환할수록 좋은 모델, 테스트문장에 대해 PPL이 작을수록 좋은 언어모델.
PPL-몇개의 단어중에 헷갈리고 있는지. 이는 엔트로피랑 연결됨.
expCE = PPL, CE = logPPL ce미니마이지는 로그피피엘 미니마이지

하지만 PPL(CE)는 정확한 번역의 품질을 반영 못 함. 특히 어순 변화에 취약함. 

좋은 번역의 품질 반영을 위해 BLEU 사용.
BLEU는 가중평군. (각 ngram별)

PPL낮을 수록 좋음. 딥러닝 모델서 CE낮아지므로 PPL떨어짐 확인. 하지만 어순 변화에 취약
BLEU는 평균 precision이어서 높은 것이 좋은 것으로 어순 변화에 robust함. 사람 평가와 좋은 상관관계 보여줌
BLEU 좋아지면 성능 오른 것으로 보지만 유의어/동의어 대처는 떨어지고, intrinsic 필요(서비스를 위해서는)
multi-bleu.perl 사용하면 됨(범용임). 파이썬 안에서 막 찍어보려면 nltk에서 쓰는 것도 좋음(직접구현비추)

bleu 적용?  bpe tokenization 모두 detokenization, 한글은 mecab등으로 접사 및 특수문자 분리 후 bleu측정
영어 등 라틴기반은 moses 통해 특수문자 분리 후 측정, 띄워쓰기 없는 중국어 일본어는 mecab, jieba통해
seg 후 bleu측정


정성평가? 잘 선정된 test set으로 경쟁사로 평가
-절대평가: 실제 번역 품질 단계 나눠 평가. 일관적 평가 품질 유지가 관건. (A,B,C 매기기)
-상대평가: 벤치마크 대상 모아놓고 순위 매겨 평가. 객곽전 평가 품질 유지가 관건.(1,2,3등 구하면 되니 편함)
전부 블라인드 테스트.


ppl 다음 타임스텝의 확률 분포가 얼마나 샤프한지 플랫한지를 나타냄. 샤프할수록 좋은 언어모델로 간주
나는 좋아한다 학교 가는 것 / 학교 가는 것 나는 좋아한다 -> 둘 다 맞지만 이런 거 평가하기에는 ppl로 부족
근데 bleu는 미분이 안 되서 최적화에 사용 못 함
작은 차이에서 bleu로 평가하면 좋음(정량평가)
정성평가에서는 블라인드 테스트로 일관성과 객관성 유지가 중요

-7-
  
beam search가 많이 다뤄지진 않지만 서비스에서는 필수적인 부분
데이터 확보, 모델 개선 없이도 훨씬 나은 성능 거둘 수 있음

NLG는 사실 search 문제
(greedy search는 지금의 최선이 나중에는 나쁜 선택이될 수 있음, 
beam search는 top-k를 트래킹하여 그리디를 좀 더 안전하게 수행-auto regressive 단점 만회 어느정도)

top-k씩 계속 뽑는 것이 아니라
처음 tok-k, 이후 파생된 것들 중에서 누적 중에서 top-k만 뽑음
성능 제일 좋았다(seq2seq에서 transformer 쓰는 정도의 개선)
패러렐하게 처리하면 속도면에서 이득을 볼 수 있음(메모리 소모되지만..)
bean 크기는 5나 10정도.. 10이상은 거의 쓸 필요는 없다 개선이 거의 없어서..

빔서치 미니배치 패러렐라이즈 구현하면 속도와 성능 둘 다 좋을 수 있음
n샘플에 대한 빔서치 수행할 때 n x k 번의 inference가 수행
따라서 n x k 개 샘플의 미니배치에 대한 inference를 수행하면 됨

k에서 파생된 애들을 각각 패러렐하게 계산해주고 컨캣 한 후 다시 k개 나눠서 페러렐하게 하면 되겠네

실제로 ppl이 번역의 성능(bleu)와 정비례하지 않음(경험적으로)


  
-8-
 
복습:
어텐션은 마음의 상태(state)를 잘 반영하면서
좋은 검색 결과를 이끌어내는 쿼리를 얻기 위함
(예) 강남역에서 가장 회식하기 좋은 오리고기 맛있는 집은 어디야? -> 강남역 오리고기 회식장소 맛집


만약 검색을 다양하게 할 수 있다면? => 멀티헤드 어텐션
(예) 강남역 오리고기 회식장소 맛집, 오리로스 맛집, 강남역 가까운 오리고기 집, 저렴한 오리고기 등..(다양하게)


스케일드닷프로덕트 어텐션? -그라디언트를 좀 더 안정적으로 학습하게 상수를 나누어줌
(학습이 좀 더 안정적으로 수행)

멀티헤드어텐션: 스케일드닷프로덕트어텐션을 여러개 들고 있음(h개만큼-#head)
재미있는 것은 Q도 h개(헤드개수만큼), K, V도 h개 만큼. 
쿼리만 바꾸는 것이 아니라 k와 v도 정보를 빼오기 편리하게 학습하는 것. 이것이 키포인트

인코더에서 처음에 q,k,v가 같음(이전 블럭의 결과값)

디코더의 경우도 qkv 이전 블럭의 결과값
인코더의 최종 출력값 q,k,v 출처가 다 달라짐. 나중에 좀 더 자세히 설명

예전에는 디코더가 각 타임스텝마자 시퀀셜하게 어텐션이 들어갔다면
지금은 한꺼번에 q가 모든 타임스텝에 대해 k에 쿼리를 때려보는 것. 그래서 속도가 빠름. 대신에 메모리 좀 먹음
배치사이즈의 샘플 별 문장 별 디코더의 각 타임스텝 별 인코더의 각 타임스텝에 대한 웨이트가 들어간 텐서
(배치사이즈 샘플:미니배치의 각 문장 별)


멀티헤드 어텐션이란 어텐션을 여러번. 쿼리를 여러번 날림. 다양한 쿼리 날려서 더 다양한 내가 필요한 정보
얻어오자.
어텐션 자체로도 인코딩과 디코딩이 가능.
인코더에서 이전레이어 정보 잘 취합 때 어텐션 사용
디코더에서 이전 정보 잘 취합할 때 어텐션 사용
이를 통해 seq2seq 보다 더 뛰어난 구조 만들 수 있음


인코더는 셀프어텐션으로 구성. qkv는 이전 레이어의 출력값-즉,같은값
q가 모든 타임스텝을 동시에 연산. 레지두얼 커넥션으로 깊게 쌓을 수 있게 됨.
BigLM의 토대로 프리트레인 랭기지 모델의 토대가 됨

디코더블락-인코더와 달리 어텐션이 두개가 들어감. 하나는 인코더에서 오는 어텐션과 인코더와 같은 셀프어텐션.
인코더서 온 거는 kv가 인코더에서 옴.

사실 패드처리를 위한 마스크가 셀프어텐션과 인코더디코더어텐션에 들어가 있어야함
디코더의 셀프어텐션에 오토리그레시브한 학습을 할 때 마스킹해야함(티쳐포싱하니깐)
정리하면,
디코더는 인코더에서 온 어텐션(q는 이전레이어 출력값, k,v는 encoder에서 옴. 패드 마스크 들어감), 
셀프어텐션(마스크는 미래 보는 거 방지하는 마스크-autoregressive문제 방지용), qkv는 이전레이어출력값
-패드마스크는 추론할 때 필요. 학습할 때는 꼭 필요한 것은 아님.

추론할 때는 셀프어텐션마스크 필요 없음(어차피 모르니..)
단 모든 레이어의 t시점 이전의 모든 타임스텝의 히든스테이트가 필요함.(반영해야하니)

버트는 포지션인코딩 대신에 포지션임베딩레이어 사용해서 학습해서 진행
rnn과 달리 순서 정보 없기 때문에 트랜스포머에서는 포지션인코딩 써줘야하고 한번만 계산해 놓으면 됨.

sgd에 gradient clipping으로 seq2seq 학습시켰었음
기본적으로 러닝레이트 튜닝되게 잘해줘야함. 너무 크면 발산, 너무 작으면 학습속도가 느려짐.
절충안으로 러닝레이트 디케이해줌

아담쓰면 lr 건드릴 필요 없이 최고성능 보여줌
문제는 깊은 네트워크에서, 예를들면 트랜스포머에서 쓰면 성능이 잘 안 나옴
추정컨데 레지듀얼 커넥션 때문에 학습초기에 불안정한 그라디언트를 잘못된 모멘텀을 갖게 되는 듯?
그래서 warm-up and linear decay(noam decay) 씀 -> 처음에 러닝레이트를 작게 가져가서 불안정한 그라디언트로 인한
잘못된 모멘텀이 만들어지는 것을 방지해보자(이런 것들을 러닝레이트 스케줄링이라고함)
트라이얼해서 좋은 곳 찾는 나이브한 방법. warm-up step을 첫 epoch의 5%정도로
warm-up step에 굉장히 민감..
warm-up step 찾기 어려워서 그래서 RADM(Rectified Adam)이 나옴.
얼리스테이지의 샘플 부족으로 어댑티브러닝레이트가 바랍직하지 않은 variance 갖게 됨

layer norm 위치를 멀티헤드어텐션 앞에 놓이니 학습이 잘 됨. LN이gradient를 평탄하게 바꾸는 효과.
웜업 하던, RADAM쓰던 layer norm 해주니 성능 잘 나옴
(lr decay는 여전히 필요)
  
  
트랜스포머는 디코딩할 때 이전 스텝의 결과물 다 들고있어야함.(메모리 많이 듦)
메모리이슈땜에 rnn계열 lstm 꺼 쓰기도 함(요즘 NLG에서 디코더는, 성능이 크게 차이 안 나서..)

인코더가 성능 좌지우시. 그래서 인코더 트랜스포머, 디코더는 rnn쓰는 경우도 곧잘 있음
pre-LN transformer에서 adam써서 최적화문제 해결

residual connection으로 transformer깊게 쌓을 수 있음

어텐션의 닷프로덕트연산은 코싸인시밀러리티와 유사.
어텐션은 쿼리를 잘 만들어내서 key-value로부터 필요한 정보를 얻어내는 과정
디코더의 각 타임스텝마다 인코더로부터 어텐션을 통해 정보를 얻어와서 생성 토큰의 품질을 높임->seq2seq

멀티헤드어텐션은 각 헤드별로 어텐션을 수행하여 다양한 정보를 얻어올 수 있음(다양한 쿼리)
셀프어텐션통해 이전 레이어 정보를 인코딩/디코딩, 어텐션 통해 디코더 정보 얻어옴


-9-

멀티링구얼 기계번역?

인코더-디코더 구조의 모델들은 페러렐 코퍼스가 필요.

패러렐코퍼스 수집 비용이 매우 비쌈.
한정된 패러렐 코퍼스로 학습할 경우 인코더는 컨텍스트 임베딩 성능이 떨어지고
디코더는 제한된 성능의 언어모델이 될 수 밖에 없음

그래서 나온 해결방안이 앙상블.
패러렐 부족->디코더 성능 저하.

그래서 모노링구얼 코퍼스로 학습한 별도 언어모델을 디코더에 앙상블로 결합

shallow fusion: seq2seq lm을 interpolation,

deep fusion: 디코더와 lm의 히든 레이어 결과를 concat
: lm 상태에 따라 gate를 열고 닫아 정보의 흐름을 컨트롤


백트렌슬레이션.

이것도 모티브는 페러렐 코퍼스로 인한 디코더의 generation성능 떨어짐.
그래서.풍부한 모노링구얼 코퍼스를 통해 추가로 디코더를 학습시켜보는 것

정답과 입력값을 거꾸로 해보는 것.
정답 -> 모델 -> 입력 이런 구조

근데 정답 자체가 노이즈가 낀 것이어서 이게 너무 많으면 문제일 수도 있음


카피트렌슬레이션.
그냥 y 넣고 y 나오게 해주는 것.
정답 -> 모델 -> 정답
이것도 성능이 좀 나옴.


노이즈드 빔백 트렌슬레이션.
모델로부터 생성된 노이즈가 문제인데

아예 노이즈를 다양하게 섞어서 생성한 문장을 synthetic source sentece로 활용.
하지만 여전히 성능의 개선의 여지가 남음

그래서 나온 것이 tagged back translation
-신테틱 줄 때는 이거 bt라고 태그로 알려줌.
이렇게 했더니 슈도코퍼스의 비율에 상관없이 학습 가능
bleu 엄청 개선됨

->디코더의 언어모델의 성능을 향상시키는 것이 주 목적!(슈도코퍼스를 활용하여)


강화학습?

gan은 nlg 에서 feedforward는 되지만 backward가 안 되서(one-hot에서 softmax로 갈 수 없음)
못 씀

ppl이 bleu 보다 번역의 질을 잘 반영하지 못 함.(학습때 ppl씀..)

--> rl 통해 bleu에 대ㅐㅎ 미분 없이 학습 가능. 또한 샘플링 기반 방식이므로 티처포싱없이 학습 가능



rl: agent, environment, state, action, reward 로 구성

markov process: 이전상태 영향 받은 상태 이동. lm도 여기에 해당
markov decision process: 이전상태와 행해진 액션에 영향을 받는 것.
(policy, valuefunction, action-value function이 있음)
value base방식과 policy base 방식이 있음

policy gradient의 q함수가 미분될 필요가 없음.
따라서 보상함수로 미분 불가능ㅎ나 매우 족잡한(bleu) 함수를 사용할 수 있음
근데 reward함수 잘 짜야함. 평균점수 대비 보상을 준다던지 등

nlg에서 :
state: 조건으로 주어진 입력 문장 토큰들, 이전까지 생성된 토큰들
action:  현재 타임스텝에서 생성할 토큰을 고르는 것
reward: 완성된 생성문장과 실제 정답과의 유사도 bleu 

nlg는 에피소드가 짧은 장점이 있음, 그래서 actor-critic 같은 고급 알고리즘을 구사할 필요성이 매우 낮음
단점으로는 단어를 고르는 것이 action이므로 action space가 vocab으로 매우 크다

rl 장점? bleu로 optimize, (PG는 미분필요없이 bleu로 최적화 가능)
티처포싱 없애도됨. nlg는 autoregressive task이므로 티처포싱으로 학습. 그래서 학습과 추론 사이 괴리감 생김
근데 rl은 샘플링기반 학습이므로 학습과 추론 방법에 차이 없음


minimum risk training-policy gradient nlg버전
리스크redefine-q함수를 y가 아닌 s서 샘플링.(subset으로 좀 더 작은 범위)
몬테카를로로 구해줌
마이너스 리스크 미니마이즈 플러스 리워드 맥시마이즈 결국 같은거임

샘플링과 라이크리후드 차이
수식 잘 이해하고 코드로 옮기는 방법이 이득임

nllloss-negative log likelihood loss




{% highlight ruby %}

{% endhighlight %}

---
layout: post
title: AI CONCEPTS NOTE
date: '2021-08-04 11:52:10 +0900'
categories: _NLP study
published: true
---


한번 정리하고 넘어가고 싶어서 개념들 서술해봅니다.



-2-

랭기지모델 학습 - 단어 분포 모사. 마코브 어썸션 도입하여 더 낮은 엔그램으로 근사

좋은 모델 : 언어분포 잘 모사. 잘 정의된 테스트셋에 높은 확률 반환

펄플렉시티: 몇개중에 헷갈리고 있냐. 작을 수록 좋은 것

뉴럴랭기지 모델은 제너럴라이즈가 잘 된다는 것이 장점, 등장하지 않은 단어들도 처리 가능

엔트로피 불확실성 나타냄. 자주 일어나는 사건은 낮은 정보량. 드물게 발생하면 높은 정보량 가짐.
불확실성은 1/등장확률 에 비례하고, 정보량에 반비례
확률 낮은 것일 수록 정보량 많은 것, 불확실성 큰 것
정보량은 마이너스 로그 확률. 확률 높아질수록 정보량 낮아짐. 0가까워질수록 높아짐

엔트로피 높을수록 플랫 디스트리뷰션, 낮을수록 샤프한 디스트리뷰션

크로스엔트로피는 퍼플렉시티의 로그취한 값.
퍼플렉시티 작게하는 것이 목적으로 이는 크로스엔트로피 익스퍼넨셜 미니마이즈.
즉, 크로스엔트로피 미니마이즈로 볼 수 있음

클래시피케이션이니까 크로스엔트로피 쓴다고 봐도 되고
두 분포 차이 미니마이즈니까 크로스엔트로피 미니마이즈.
아키르릴후드 맥시마이즈 할꺼니까 네거티브 라이클리후드 마니마이즈 하는 건데 이게 크로스엔트로피 미니마이즈와 같음.

크로스엔트로피 로그 취하면 퍼플렉시티.
퍼플렉시티 익스퍼넨셜 취하면 크로스엔트로피임.

seq2seq의 many to many는 many to one 과 one to many 로 이해하는 것이 좋음

non-auto-regessive-현재 상태가 앞 뒤 문맥에 의해 정해짐
auto-regressive- 과거상태 의존

티처포싱 안 하면 many2many 개수 안 맞을 수도. eos먼저 뜨면
티처포싱 안 하면 이전 출력에 따라 현재 스테이트 달라짐 mle 이상해짐
mle다르게 수식 적용
그래서 실제 정답을 넣으줌. 그래서 학습이랑 테스트(추론) 다로 2개 짜야함
티처포싱 성능 저하될 수 있으나 기본적 성능 좋아서 걍 쓰면 되지만.. 왜 저하되냐면 순서까지도 기억해주는 오버피팅이 되기때문. 그래서 보통 반반티처포싱 등 사용

fluent한 문장 골라내는 일이나 다음단어 뽑아내는 일은 언어모델로 사실 같음.


-4-
seq to seq 도 auto encoder와 비슷
autoencoder - 특징 추출 하는 것.(차원축소(latent space:잠재 feature의 공간?) 및 복원을 통해)

decoder는 conditional language model이라고 볼 수 있음-인코더로부터 문장을 압축한 context vector를 바탕으로 문장생성
(conditional? 조건부 확률 느낌?)

classification(단어선택), discrete value 예측하는 거니까 크로스엔트로피 쓰면 됨. (소프트맥스 결과값에)
MLE중이니까 negative log likelihood 미니마이즈해야히니까 크로스엔트로피.
디코더가 컨디셔널랭기지 모델이니깐 퍼플렉시티 미니마이즈 해야하는 것. 그니깐 크로스엔트로피 익스포넨셜
이 코르스엔트로피니깐. 크로스엔트로피 쓰면 
결국 ground truth 분포와 모델 분포 삳이의 차이를 최소화하기 위함
(Ground-truth는 학습하고자 하는 데이터의 원본 혹은 실제 값)

어텐션?
키-벨류 펑션으로 미분가능함
파이선의 딕셔너리= {K:v1, K2:v2} ,,, d[K] 벨류리턴 - (딕셔너리설명)
기존 딕셔너리 key-value 펑션과 달리 query와 key의 "유사도"에 따라 value 반환!(weighted sum으로)
->lstm hidden state한계인 부족정보를 직접 encoder에서 조회해서 예측에 필요한 정보 얻어오는 것.
정보를 잘 얻어오기(이 과정이 attention) 위해 query 잘 만들어내는 과정을 학습 

attention은 QKV.
Q:현재 time-step의 decoder의 output
K: 각 time-step 별 encoder의 output
V: 각 time-step 별 encoder의 output
q와 k의 유사도 계산(encoder output token들)
유사도를 각각 encoder output toekn들에 곱해주고 더해서 현재 context vector 만들어줌.
쿼리 날린 decoder 히든스테이트와 context vector를 컨캣해줘서 새로운 히든스테이트를 얻음(이것이 반영 된 것)
근데 유사도 구할 때 유사도 팍팍 안 구해짐. 그래서 linear transform(linear layer 통과시킴) 후에 유사도 얻어옴.
그래서 우리는 linear transform을 학습해서 유사도를 잘 받아오게 학습해야함. 이것도 잘 학습해야함

비유해보면 "오리역에서 가장 편하게 밥먹는 집 어디야?" -> "오리역 가정식 백반집"(쿼리 바꾸는 거 학습한 것)
마음속의 상태(state)를 잘 반영하면서 좋은 쿼리를 만들기 위함임.
어텐션은 "쿼리를 잘 만들어내는(변환) 과정" 배우는 것이다. 여기서 batch matrix multiplication(BMM) 사용(행렬들 곱)
닷프로덕트가 코싸인시뮬러리티와 비슷. 소프트맥스까지 쒸우면 유사도라고 볼 수 있음

<PAD> 위치에 weight가 가지 않도록 하는 안전장치 추가->이것이 마스킹


input feeding? 샘플링과정서 손실되는 정보 최소화, 티쳐포싱으로 인한 학습/추론 사이의 괴리 최소화
인풋피딩? 아웃풋 히든스테이트를을 다음 인풋 히든스테이트에 컨캣

auto-regressive : 과거 자신의 상태를 참조하여 현재 자신의 상태를 업데이트. 시퀀스에서 이전 단어 보고
다음 단어 예측하는 것.

teacher forcing 통해 auto-regressive task에 대한 sequential modeling 가능. 하지만 training mode 와 inference mode의 괴리
(discrepancy) 생김

(dilde : ~ (물결표) )

pytorch ignite로 procedure 짜놓을 수 있음(lightening도 가능)

-5-

NLG - 가장 높은 확률 갖는 문장 예측. 일종의 shortest path 찾는 것으로 볼 수 있두아!

beam search 통해 greedy search 단점 	보완 가능(어느정도)

재미있어야하는 챗봇에서는 랜덤샘플링도 ㄱㅊ. 테스크에 따라 .. 번역에서는 일관성이 떨어져서 좀 그럼.

coverage penalty= 어텐션이 time step마다 다른 곳에 집중해야 좋은 번역일 것이라 생각(다 같은 곳만 집중시 한계니)
페널티로 성능 변화는 미미하긴함


그래서 트레이닝 vs 인퍼런스?
트레이닝은 티쳐포싱 통해서 한 스텝마다 정답이 나오게..(다음 스텝 인풋은 정답)
인퍼런스는 스텝 밟아가면서,, 스텝 결과값이 다음 인풋, 또 이 결과값이 또 다음 스텝의 인풋으로 쭉쭉 가는 것.

인퍼런스는 다음 타임스텝에 넣어주고 학습은 티처포싱이었음..(참고)	
이것은 오토리그레시브 속성(이전 타임스텝에 영향을 받ㄴ는 것. 쭉쭉)

기존에 포워드함수 하나 짜면 학습과 추론 다 되었다면, (학습 추론 같았기 때문.)
하지만 NLG task(auto regressive 속성 띄기 때문) 트레이닝과 인퍼런스 함수 따로 만들어줘야.
그래서 트레이닝함수 구현 했어도 서치라는 추론함수 만들어줌. 두 함수가 괴리가 일어나고
그래서 강화학습이나 듀얼러닝 같은 것들 사용해서 괴리 줄여줌

-6-
  
test set 적격성

noise 포함되지 않았는가?
실제 deploy 할 때와 같은 domain 인가? (이상적으로는 TRAIING 할 때와 같은 DOMAIN)

scoring metric. 해당 task 채점하기에 적헐한 metric 인가?


평가 크게 2가지
정성평가(INTRINSIC) 
-사람이
-인건비 비싸고 속도 느리고 주관개입가능(단)
-절대평가/상대평가 두가지 방법이 있음

정량평가(EXTRINSIC)
-자동으로
-저렴비용, 빠른속도, 객관적 평가가능(장)
-정확도가 낮을 수 있음(장)

-크로스엔트로피, 펠플렉시티로 평가(예)
-평가방법으로 BLEU가 있음

실제 DEPLOY를 위해서는 INTRINSIC EVALUATION을 꼭 거쳐야함.
(잦은 업데이트의 경우 EXTRINSIC EVALUATION) 
(배포할 때는 INTRINSIC 써서 비교통해 이전 모델과 비교 후 배포)


<BLEU -블루 매트릭>
테스트 문장에 대해 확률을 높게 반환할수록 좋은 모델, 테스트문장에 대해 PPL이 작을수록 좋은 언어모델.
PPL-몇개의 단어중에 헷갈리고 있는지. 이는 엔트로피랑 연결됨.
expCE = PPL, CE = logPPL ce미니마이지는 로그피피엘 미니마이지

하지만 PPL(CE)는 정확한 번역의 품질을 반영 못 함. 특히 어순 변화에 취약함. 

좋은 번역의 품질 반영을 위해 BLEU 사용.
BLEU는 가중평군. (각 ngram별)

PPL낮을 수록 좋음. 딥러닝 모델서 CE낮아지므로 PPL떨어짐 확인. 하지만 어순 변화에 취약
BLEU는 평균 precision이어서 높은 것이 좋은 것으로 어순 변화에 robust함. 사람 평가와 좋은 상관관계 보여줌
BLEU 좋아지면 성능 오른 것으로 보지만 유의어/동의어 대처는 떨어지고, intrinsic 필요(서비스를 위해서는)
multi-bleu.perl 사용하면 됨(범용임). 파이썬 안에서 막 찍어보려면 nltk에서 쓰는 것도 좋음(직접구현비추)

bleu 적용?  bpe tokenization 모두 detokenization, 한글은 mecab등으로 접사 및 특수문자 분리 후 bleu측정
영어 등 라틴기반은 moses 통해 특수문자 분리 후 측정, 띄워쓰기 없는 중국어 일본어는 mecab, jieba통해
seg 후 bleu측정


정성평가? 잘 선정된 test set으로 경쟁사로 평가
-절대평가: 실제 번역 품질 단계 나눠 평가. 일관적 평가 품질 유지가 관건. (A,B,C 매기기)
-상대평가: 벤치마크 대상 모아놓고 순위 매겨 평가. 객곽전 평가 품질 유지가 관건.(1,2,3등 구하면 되니 편함)
전부 블라인드 테스트.


ppl 다음 타임스텝의 확률 분포가 얼마나 샤프한지 플랫한지를 나타냄. 샤프할수록 좋은 언어모델로 간주
나는 좋아한다 학교 가는 것 / 학교 가는 것 나는 좋아한다 -> 둘 다 맞지만 이런 거 평가하기에는 ppl로 부족
근데 bleu는 미분이 안 되서 최적화에 사용 못 함
작은 차이에서 bleu로 평가하면 좋음(정량평가)
정성평가에서는 블라인드 테스트로 일관성과 객관성 유지가 중요




{% highlight ruby %}

{% endhighlight %}

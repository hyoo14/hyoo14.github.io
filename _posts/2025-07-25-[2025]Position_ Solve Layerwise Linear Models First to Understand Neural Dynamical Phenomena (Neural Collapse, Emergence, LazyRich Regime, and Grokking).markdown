---
layout: post
title:  "[2025]Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)"  
date:   2025-07-25 14:23:40 +0900
categories: study
---

{% highlight ruby %}


í•œì¤„ ìš”ì•½: 

ë”¥ëŸ¬ë‹ì˜ 4ê°€ì§€ í˜„ìƒì„ ì¸µë³„ ì„ í˜• ëª¨ë¸ë¡œ ì„¤ëª…(dynamical feedback principle)   
ë˜í•œ ì´ë¥¼ í†µí•œ í˜„ìƒ ì™„í™”ì˜ ê°€ëŠ¥ì„± ì œì‹œ   



ì§§ì€ ìš”ì•½(Abstract) :    


ë¬¼ë¦¬í•™ì—ì„œëŠ” ë³µì¡í•œ ì‹œìŠ¤í…œì„ ìµœì†Œí•œì˜ í•µì‹¬ ì›ë¦¬ë§Œ ë‹´ì€ í•´ì„ ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë‹¨ìˆœí™”í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì—ì„œë„ ìœ ì‚¬í•œ ì ‘ê·¼ì„ ì œì•ˆí•˜ë©°, ë³µì¡í•œ ë¹„ì„ í˜• ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ **layerwise linear model (ì¸µë³„ ì„ í˜• ëª¨ë¸)**ì„ ë¨¼ì € í•´ê²°í•  ê²ƒì„ ì£¼ì¥í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„ í˜• ëª¨ë¸ì€ ê° ì¸µì˜ ë™ì  ìƒí˜¸ì‘ìš© ì›ë¦¬ë¥¼ ë‹´ê³  ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ì‹ ê²½ë§ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¤ì–‘í•œ í˜„ìƒ(ì˜ˆ: Neural Collapse, Emergence, Lazy/Rich Regime, Grokking ë“±)ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ìë“¤ì€ **dynamical feedback principle (ë™ì  í”¼ë“œë°± ì›ë¦¬)**ì„ ì œì•ˆí•˜ë©°, ì´ ì›ë¦¬ë¥¼ í†µí•´ ë¹„ì„ í˜• ìš”ì†Œ ì—†ì´ë„ ì£¼ìš” ë”¥ëŸ¬ë‹ ë™ì‘ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„ í˜• ëª¨ë¸ì€ ìˆ˜í•™ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•˜ê³ , ì‹ ê²½ë§ ê³¼í•™ì˜ ë°œì „ì„ ê°€ì†í™”í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ ì œì‹œë©ë‹ˆë‹¤.



In physics, complex systems are often simplified into minimal, solvable models that retain only the core principles. In machine learning, layerwise linear models (e.g., linear neural networks) act as simplified representations of neural network dynamics. These models follow the dynamical feedback principle, which describes how layers mutually govern and amplify each otherâ€™s evolution. This principle extends beyond the simplified models, successfully explaining a wide range of dynamical phenomena in deep neural networks, including neural collapse, emergence, lazy and rich regimes, and grokking. In this position paper, we call for the use of layerwise linear models retaining the core principles of neural dynamical phenomena to accelerate the science of deep learning.





* Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()   

<br/>

# ë‹¨ì–´ì •ë¦¬  
*  







 
<br/>
# Methodology    



ì´ ë…¼ë¬¸ì—ì„œëŠ” ë³µì¡í•œ ë”¥ëŸ¬ë‹ í˜„ìƒì„ í•´ì„í•˜ê¸° ìœ„í•´ ì¸µë³„ ì„ í˜• ëª¨ë¸(layerwise linear models) ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì„ í˜• ëª¨ë¸ì€ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ê³„ê°€ ì„ í˜•ì¸ ëª¨ë¸ì´ì§€ë§Œ, ê° ì¸µì„ ë‚˜ëˆ„ì–´ ì¸µê°„ ê³±(product of parameters) ìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” êµ¬ì¡°ë¥¼ ê°–ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 
ğ‘“
(
ğ‘¥
)
=
ğ‘¥
âŠ¤
ğ‘Š
1
ğ‘Š
2
f(x)=x 
âŠ¤
 W 
1
â€‹
 W 
2
â€‹
  ê°™ì€ 2-layer linear neural networkë‚˜ 
ğ‘“
(
ğ‘¥
)
=
âˆ‘
ğ‘–
ğ‘¥
ğ‘–
ğ‘
ğ‘–
ğ‘
ğ‘–
f(x)=âˆ‘ 
i
â€‹
 x 
i
â€‹
 a 
i
â€‹
 b 
i
â€‹
  í˜•íƒœì˜ ëŒ€ê°ì„ í˜•(diagonal) ëª¨ë¸ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

ë…¼ë¬¸ì€ ì´ëŸ¬í•œ êµ¬ì¡°ì—ì„œ ë°œìƒí•˜ëŠ” **ë™ì  í”¼ë“œë°± ì›ë¦¬(dynamical feedback principle)**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ì›ë¦¬ëŠ” í•œ ì¸µì˜ íŒŒë¼ë¯¸í„° í¬ê¸°ê°€ ë‹¤ë¥¸ ì¸µì˜ ë³€í™” ì†ë„ì— ì˜í–¥ì„ ì£¼ë©°, ì´ë¡œ ì¸í•´ ê° ì¸µì´ ì„œë¡œë¥¼ ì¦í­í•˜ëŠ” ë¹„ì„ í˜•ì  ë™ì‘ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì›ë¦¬ë¥¼ í†µí•´ emergence, grokking, neural collapse ë“± ë‹¤ì–‘í•œ í˜„ìƒì´ ì„¤ëª…ë©ë‹ˆë‹¤.

ë˜í•œ, ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì´ˆê¸°í™” ì¡°ê±´(ì‘ì€ ì´ˆê¸°í™”, ê³„ì¸µ ê°„ ë¶ˆê· í˜• ë“±)ì´ë‚˜ ëª©í‘œ í•¨ìˆ˜ í¬ê¸°(target scale)ì— ë”°ë¼ ë°œìƒí•˜ëŠ” **lazy regime (ì„ í˜•ì  í•™ìŠµ)**ê³¼ rich regime (ë¹„ì„ í˜•ì  íŠ¹ì„± í•™ìŠµ) ì˜ ì „ì´ë¥¼ í•´ì„í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ ë³µì¡í•œ ë”¥ëŸ¬ë‹ í˜„ìƒì„ ë‹¨ìˆœí™”ëœ ìˆ˜í•™ì  ëª¨ë¸ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì£¼ìš” ê¸°ë²•ì…ë‹ˆë‹¤.




This paper employs layerwise linear modelsâ€”such as two-layer linear neural networks and diagonal linear networksâ€”to analyze and understand deep learning dynamics. These models maintain a layerwise multiplicative structure, where the output is a product of parameters across layers (e.g., 
ğ‘“
(
ğ‘¥
)
=
ğ‘¥
âŠ¤
ğ‘Š
1
ğ‘Š
2
f(x)=x 
âŠ¤
 W 
1
â€‹
 W 
2
â€‹
  or 
ğ‘“
(
ğ‘¥
)
=
âˆ‘
ğ‘–
ğ‘¥
ğ‘–
ğ‘
ğ‘–
ğ‘
ğ‘–
f(x)=âˆ‘ 
i
â€‹
 x 
i
â€‹
 a 
i
â€‹
 b 
i
â€‹
 ). Although they lack non-linear activations, the training dynamics are inherently non-linear due to inter-layer interactions.

A central methodological contribution is the introduction of the dynamical feedback principle, which describes how the magnitude of one layerâ€™s parameters governs the rate of change in another layer. This principle leads to amplifying feedback dynamics that are used to explain diverse deep learning phenomena such as emergence, grokking, neural collapse, and the lazy/rich regime transition.

The models are analyzed under various initial conditions (e.g., small initialization, imbalanced layers, target scaling), and are often exactly solvable, allowing clear mathematical insight into training behavior. By focusing on the dynamics of these simplified but expressive models, the authors demonstrate that much of deep learningâ€™s complexity can be explained without relying on non-linearity.




   
 
<br/>
# Results  



ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ ë³µì¡í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, ì¸µë³„ ì„ í˜• ëª¨ë¸(layerwise linear models) ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì—¬ëŸ¬ í˜„ìƒì„ ìˆ˜í•™ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê³ , ì‹¤í—˜ì ìœ¼ë¡œë„ ìœ ì‚¬í•œ ë™ì‘ì„ ë³´ì„ì„ í™•ì¸í•©ë‹ˆë‹¤.

Emergence (ëŒë°œì  ì„±ëŠ¥ í–¥ìƒ): ë‹¤ì¸µ ì„ í˜• ëª¨ë¸ì—ì„œ ê° feature ë˜ëŠ” skillì´ ì‹œê·¸ëª¨ì´ë“œ í˜•íƒœì˜ ì„±ì¥ ê³¡ì„ ì„ ë”°ë¼ í•™ìŠµë˜ë©°, í•™ìŠµ ìˆœì„œê°€ featureì˜ ë¶„ì‚°(E[xÂ²])ì´ë‚˜ ë¹ˆë„ì— ë”°ë¼ ë‹¬ë¼ì§ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ íŠ¹ì • ì‹œì ì—ì„œ ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ í–¥ìƒ(emergence) ì´ ë°œìƒí•©ë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œë„ ê´€ì¸¡ëœ í˜„ìƒê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

Neural Collapse (ì‹ ê²½ ë¶•ê´´): ë¶„ë¥˜ ë¬¸ì œì—ì„œ í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ í”¼ì²˜ë“¤ì´ ê° í´ë˜ìŠ¤ë³„ í‰ê·  ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ë ´í•˜ê³ , ì •ê·œí™”ëœ ë‹¨ìˆœ êµ¬ì¡°(simplex ETF) ë¥¼ ì´ë£¨ëŠ” í˜„ìƒì´ ì„ í˜• ëª¨ë¸ì—ì„œë„ ìˆ˜í•™ì ìœ¼ë¡œ ë„ì¶œë©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ResNet18 ë“± ì‹¤ì œ ëª¨ë¸ì—ì„œë„ ë™ì¼í•œ êµ¬ì¡°ê°€ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.

Lazy vs. Rich Regime: ì´ˆê¸°í™” ì¡°ê±´ì´ë‚˜ weight-to-target ë¹„ìœ¨ì— ë”°ë¼ ì„ í˜•ì  ë™ì—­í•™(lazy) ë˜ëŠ” ë¹„ì„ í˜•ì  íŠ¹ì„± í•™ìŠµ(rich) ìœ¼ë¡œ ë‚˜ë‰˜ë©°, rich regimeì—ì„œëŠ” íŠ¹ì§• í•™ìŠµ(feature learning)ì´ ë”ìš± í™œë°œíˆ ì¼ì–´ë‚©ë‹ˆë‹¤. ì´ ë³€í™”ëŠ” grokking (ì§€ì—°ëœ ì¼ë°˜í™”) í˜„ìƒê³¼ë„ ì—°ê²°ë˜ì–´, weight ì´ˆê¸°í™”ë‚˜ íƒ€ê²Ÿ í¬ê¸°ë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ grokking ì—†ì´ ë¹ ë¥¸ ì¼ë°˜í™”ë¥¼ ìœ ë„í•  ìˆ˜ ìˆìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì˜€ìŠµë‹ˆë‹¤.

ë¹„êµ ëª¨ë¸: ëŒ€ë¶€ë¶„ì˜ ê²°ê³¼ëŠ” 2-layer ReLU ì‹ ê²½ë§ì´ë‚˜ MLP(Multilayer Perceptron)ì™€ ë¹„êµë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ë¹„ì„ í˜• ëª¨ë¸ì´ ë³´ì´ëŠ” emergence, neural collapse, grokking ë“±ì˜ í˜„ìƒì´ ì„ í˜• ëª¨ë¸ë¡œë„ ì •í™•íˆ ì˜ˆì¸¡ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

ë©”íŠ¸ë¦­: í…ŒìŠ¤íŠ¸ ì •í™•ë„, í•™ìŠµ ê³¡ì„ , feature ë¶„ì‚°, rank ì¶”ì •, correlation (skill strength), NTK distance ë“±ì„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.




This paper demonstrates that layerwise linear models, despite their simplicity, can effectively reproduce and explain several complex phenomena observed in modern deep neural networks:

Emergence: The model exhibits sigmoidal saturation in learning different features, leading to abrupt performance jumps when certain feature variances or frequencies are reached. This behavior matches empirical observations in large language models.

Neural Collapse: The model shows that, over training, the final layer features converge to class-specific mean vectors forming a simplex equiangular tight frame (ETF)â€”a phenomenon observed in models like ResNet18 trained on CIFAR-10. This behavior is mathematically derived in the linear setting.

Lazy vs. Rich Regimes: By varying initialization imbalance or weight-to-target ratios, the model transitions between lazy (linear) dynamics and rich (feature-learning) dynamics. Notably, entering the rich regime removes the delayed generalization phase (grokking), enabling faster learning.

Comparative Models: The linear models were compared against nonlinear neural networks such as 2-layer ReLU models and 4-layer tanh MLPs. In all cases, the layerwise linear models accurately predicted the empirical trends of the nonlinear networks.

Evaluation Metrics: Key metrics included test accuracy, learning curves, feature rank, correlation metrics (skill strength), and NTK kernel distance, showing that linear models can quantitatively align with complex network behavior.





<br/>
# ì˜ˆì œ  



ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ í˜„ìƒ(emergence, grokking, neural collapse ë“±)ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë‹¨ìˆœí™”ëœ ì˜ˆì œ ë°ì´í„°ì™€ í…ŒìŠ¤í¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

Emergence ì‹¤í—˜ ì˜ˆì‹œ

ì…ë ¥ (Input): ê° featureê°€ power-law ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” **ë‹¤ì¤‘ íŒ¨ë¦¬í‹° í•¨ìˆ˜ (multitask sparse parity)**ì—ì„œ ë‚˜ì˜¨ pre-defined skill functions 
ğ‘”
ğ‘˜
(
ğ‘¥
)
g 
k
â€‹
 (x)

ì¶œë ¥ (Output): íŠ¹ì • skill functionë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ êµ¬ì„±ëœ ëª©í‘œ í•¨ìˆ˜ 
ğ‘“
âˆ—
(
ğ‘¥
)
=
âˆ‘
ğ‘†
ğ‘˜
ğ‘”
ğ‘˜
(
ğ‘¥
)
f 
âˆ—
 (x)=âˆ‘S 
k
â€‹
 g 
k
â€‹
 (x)

í…ŒìŠ¤í¬ (Task): ì–´ë–¤ skillì´ ë” ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ì— ë”°ë¼ ëª¨ë¸ì´ ê·¸ skillì„ ë¨¼ì € í•™ìŠµí•˜ê³ , ì ì€ ë¹ˆë„ì˜ skillì€ ë‚˜ì¤‘ì— í•™ìŠµí•¨ â†’ ì‹œê°„, ë°ì´í„°, íŒŒë¼ë¯¸í„° ì¦ê°€ì— ë”°ë¼ ëŒë°œì  ì„±ëŠ¥ í–¥ìƒ(emergence) ë°œìƒ

Grokking ì‹¤í—˜ ì˜ˆì‹œ

ì…ë ¥ (Input): MNIST ì´ë¯¸ì§€ ì¤‘ 1000ê°œ ìƒ˜í”Œ ì‚¬ìš©, 4-layer tanh MLP ëª¨ë¸ í•™ìŠµ

ì¶œë ¥ (Output): ìˆ«ì í´ë˜ìŠ¤ (0~9)

í…ŒìŠ¤í¬ (Task): ì‘ì€ ë°ì´í„°ë¡œ í›ˆë ¨í•  ë•Œ ì¼ë°˜í™”ê°€ ì§€ì—°ë˜ë‹¤ê°€ íŠ¹ì • ì‹œì  ì´í›„ì— ê°‘ìê¸° test accuracyê°€ ìƒìŠ¹ â†’ grokking í˜„ìƒ

ì¡°ì ˆ ì‹¤í—˜: ì´ˆê¸° weight í¬ê¸°, target ê°’ í¬ê¸°, ì…ë ¥ ìŠ¤ì¼€ì¼ ë“±ì„ ì¡°ì ˆí•˜ì—¬ grokkingì„ ì œê±°í•˜ê³  ë¹ ë¥¸ ì¼ë°˜í™”ë¥¼ ìœ ë„

Neural Collapse ì˜ˆì‹œ

ì…ë ¥ (Input): í´ë˜ìŠ¤ë³„ ì…ë ¥ ë°ì´í„° 
ğ‘¥
x

ì¶œë ¥ (Output): í´ë˜ìŠ¤ë³„ one-hot label

í…ŒìŠ¤í¬ (Task): í›ˆë ¨ì´ ì§„í–‰ë˜ë©° ë§ˆì§€ë§‰ í”¼ì²˜ ë²¡í„°ë“¤ì´ í´ë˜ìŠ¤ë³„ í‰ê· ìœ¼ë¡œ ëª¨ì´ê³ , ì„œë¡œ ì§êµí•˜ëŠ” simplex ETF êµ¬ì¡°ë¥¼ í˜•ì„±í•¨

ì´ëŸ¬í•œ ì˜ˆì‹œë“¤ì€ ëª¨ë‘ layerwise linear modelë¡œ ìˆ˜í•™ì ìœ¼ë¡œ ê¸°ìˆ  ê°€ëŠ¥í•˜ë©°, ReLU MLP ë“± ì‹¤ì œ ëª¨ë¸ê³¼ ë§¤ìš° ìœ ì‚¬í•œ ë™ì‘ì„ ë³´ì„ì„ ì‹¤í—˜ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.




The paper uses concrete example tasks and datasets to demonstrate that layerwise linear models can replicate complex neural phenomena:

Emergence Example

Input: Predefined skill functions 
ğ‘”
ğ‘˜
(
ğ‘¥
)
g 
k
â€‹
 (x) derived from a multitask sparse parity problem, with features following a power-law distribution

Output: Target function 
ğ‘“
âˆ—
(
ğ‘¥
)
=
âˆ‘
ğ‘†
ğ‘˜
ğ‘”
ğ‘˜
(
ğ‘¥
)
f 
âˆ—
 (x)=âˆ‘S 
k
â€‹
 g 
k
â€‹
 (x) as a weighted sum of skills

Task: The model learns frequently occurring skills earlier and rare ones later, leading to abrupt performance improvements (emergence) as time, data, or parameters increase

Grokking Example

Input: 1000 MNIST digit images used to train a 4-layer tanh MLP

Output: Digit classification labels (0â€“9)

Task: The model overfits training data but delays generalization, suddenly improving test accuracy after many epochs (grokking)

Intervention: By adjusting initial weight scale, target scaling, or input magnitude, the authors eliminate grokking and achieve early generalization

Neural Collapse Example

Input: Class-specific input vectors

Output: One-hot encoded class labels

Task: As training progresses, final-layer feature vectors collapse to class means forming a simplex ETF structure, matching phenomena observed in networks like ResNet18

These examples are all mathematically modeled using layerwise linear networks, and their behavior closely aligns with that of more complex ReLU-based networks in practice.




<br/>  
# ìš”ì•½   



ì´ ë…¼ë¬¸ì€ ë³µì¡í•œ ì‹ ê²½ë§ í˜„ìƒì„ í•´ì„í•˜ê¸° ìœ„í•´ **ì¸µë³„ ì„ í˜• ëª¨ë¸(layerwise linear models)**ê³¼ **ë™ì  í”¼ë“œë°± ì›ë¦¬(dynamical feedback principle)**ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ emergence, neural collapse, lazy/rich regime, grokking ë“± ë‹¤ì–‘í•œ í˜„ìƒì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •í™•íˆ ì„¤ëª…í•˜ê³ , ì‹¤ì œ ë¹„ì„ í˜• ì‹ ê²½ë§ê³¼ ìœ ì‚¬í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ˆì‹œë¡œëŠ” íŒ¨ë¦¬í‹° ê¸°ë°˜ ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµ, MNIST ìˆ«ì ë¶„ë¥˜, í´ë˜ìŠ¤ë³„ í”¼ì²˜ ë¶•ê´´ ë“±ì´ ì‚¬ìš©ë˜ë©°, ì„ í˜• ëª¨ë¸ë¡œë„ ì´ëŸ¬í•œ í˜„ìƒì´ ì¬í˜„ë¨ì„ ì…ì¦í•©ë‹ˆë‹¤.




This paper proposes layerwise linear models and the dynamical feedback principle to analyze complex neural network phenomena. These models successfully explain a range of behaviorsâ€”including emergence, neural collapse, lazy/rich regimes, and grokkingâ€”with exact mathematical solutions and empirical consistency with nonlinear networks. Example tasks include multitask parity learning, MNIST classification, and class-wise feature collapse, all of which are reproduced using linear models.



<br/>  
# ê¸°íƒ€  



Figure 1 (ì „ì²´ êµ¬ì¡°ë„)

ë…¼ë¬¸ì˜ íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ìš”ì•½í•œ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ, ê° í˜„ìƒì´ ì–´ë–¤ ì´ˆê¸° ì¡°ê±´ê³¼ ë™ì  ì›ë¦¬ë¡œë¶€í„° íŒŒìƒë˜ëŠ”ì§€ë¥¼ ìƒ‰ìƒ(ì´ˆë¡â€“í•µì‹¬ ì›ë¦¬, ë…¸ë‘â€“ì¡°ê±´, íŒŒë‘â€“ìˆ˜í•™ì  ì„±ì§ˆ, ë¹¨ê°•â€“ì‹¤ì œ í˜„ìƒ)ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬

ì¸ì‚¬ì´íŠ¸: **ë‹¨ì¼í•œ ì›ë¦¬(í”¼ë“œë°± ë™ì—­í•™)**ê°€ ë‹¤ì–‘í•œ í˜„ìƒì„ ê´€í†µí•œë‹¤ëŠ” ê²ƒì„ ê°•ì¡°

Figure 3 (ì„ í˜• vs. ì¸µë³„ ì„ í˜• ëª¨ë¸ì˜ í•™ìŠµ ê³¡ì„  ë¹„êµ)

ë™ì¼í•œ ì…ë ¥ ë¶„ì‚° ì¡°ê±´ì—ì„œ ì„ í˜• ëª¨ë¸ì€ ë¹ ë¥´ê²Œ í¬í™”ë˜ëŠ” ë°˜ë©´, ì¸µë³„ ì„ í˜• ëª¨ë¸ì€ ì§€ì—°ëœ ì‹œê·¸ëª¨ì´ë“œ í•™ìŠµì„ ë³´ì—¬ emergenceë¥¼ ì„¤ëª…

ì¸ì‚¬ì´íŠ¸: ì‹œê·¸ëª¨ì´ë“œ í•™ìŠµê³¼ ëª¨ë“œ ê°„ ì‹œê°„ì°¨ê°€ ëŒë°œì  ì„±ëŠ¥ í–¥ìƒê³¼ ê´€ë ¨ ìˆìŒ

Figure 4 (ìŠ¤í‚¬ í•™ìŠµ ê³¡ì„ )

multitask parity ë¬¸ì œì—ì„œ ê° skillì´ ì‹œê°„, ë°ì´í„°, íŒŒë¼ë¯¸í„°ì˜ ì¦ê°€ì— ë”°ë¼ ì–´ë–¤ ì‹œì ì—ì„œ ê¸‰ê²©íˆ í•™ìŠµë˜ëŠ”ì§€ë¥¼ ì‹œê°í™”

ì¸ì‚¬ì´íŠ¸: emergenceê°€ ê³„ë‹¨ì‹ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ì´ìœ ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤Œ

Figure 5 (Neural Collapse êµ¬ì¡° ì‹œê°í™”)

ë§ˆì§€ë§‰ ë ˆì´ì–´ í”¼ì²˜ë“¤ì´ í´ë˜ìŠ¤ í‰ê· ì„ ì¤‘ì‹¬ìœ¼ë¡œ simplex ETF í˜•íƒœë¡œ ìˆ˜ë ´í•˜ëŠ” êµ¬ì¡°ë¥¼ ë„ì‹í™”

ì¸ì‚¬ì´íŠ¸: ì´ ë‹¨ìˆœí•œ ê¸°í•˜ êµ¬ì¡°ê°€ í•™ìŠµëœ í”¼ì²˜ ë¶„ì‚°ì˜ ìµœì†Œí™” ë° ì¼ë°˜í™”ì™€ ì—°ê²°ë¨

Figure 6 & 7 (Lazy/Rich Regime ë° Grokking ì‹œê°í™”)

Layer imbalance ë° weight-to-target ratioë¥¼ ì¡°ì ˆí•˜ë©´ì„œ NTK ë³€í™”ë‚˜ í•™ìŠµ ì†ë„ì˜ ì°¨ì´ë¥¼ ì‹œê°í™”

ì¸ì‚¬ì´íŠ¸: ì‘ì€ ì´ˆê¸°í™” ë˜ëŠ” í° íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ì´ í”¼ë“œë°± íš¨ê³¼ë¥¼ ìœ ë„í•˜ì—¬ feature learningì„ ê°•í™”í•¨ â†’ grokking ì œê±° ê°€ëŠ¥

Figure 8 (MNISTì—ì„œ grokking ì œê±° ì‹¤í—˜)

weight/downscaling/input-scaling ë“± ë‹¤ì–‘í•œ ê¸°ë²•ì´ grokking ì§€ì—° ì—†ì´ ì¼ë°˜í™”ë¥¼ ìœ ë„í•¨ì„ í•™ìŠµê³¡ì„ ìœ¼ë¡œ ë³´ì—¬ì¤Œ

ì¸ì‚¬ì´íŠ¸: ì´ˆê¸° ì¡°ê±´ë§Œìœ¼ë¡œë„ rich regimeì„ ìœ ë„í•˜ì—¬ grokkingì„ ì œì–´í•  ìˆ˜ ìˆìŒ

Appendix Aâ€“H

ëª¨ë“  ì£¼ìš” ìˆ˜ì‹(ì˜ˆ: ì‹œê·¸ëª¨ì´ë“œ í•™ìŠµê³¡ì„ , ë³´ì¡´ëŸ‰, Î¸ ë³€í™” ë“±)ì˜ ìˆ˜í•™ì  ìœ ë„ ê³¼ì • í¬í•¨

ì¸ì‚¬ì´íŠ¸: ì‹¤í—˜ì´ ì•„ë‹Œ ì´ë¡ ì  ëª¨ë¸ë§ë§Œìœ¼ë¡œ ë³µì¡í•œ í˜„ìƒì„ ì˜ˆì¸¡ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì£¼ëŠ” í•µì‹¬ ë’·ë°›ì¹¨ ìë£Œ





Figure 1 (Paper Roadmap Diagram)

A color-coded diagram summarizes how each phenomenon arises from specific initial conditions and a common dynamical feedback principle

Insight: Reinforces the central claim that a single unified principle explains diverse neural behaviors

Figure 3 (Comparison of Dynamics)

Shows learning curves of linear vs. diagonal linear networks: linear models saturate quickly, whereas layerwise models show delayed sigmoidal growth

Insight: Highlights how delayed saturation of modes leads to emergence

Figure 4 (Skill Emergence Curves)

Tracks how individual skills in a multitask parity problem emerge abruptly as training time, data, or parameter count increases

Insight: Demonstrates that emergence is mathematically predictable and staged

Figure 5 (Neural Collapse Geometry)

Illustrates how final-layer features converge to class means forming a simplex ETF structure

Insight: Shows geometric organization of features that supports generalization

Figures 6 & 7 (Lazy/Rich Regime and Grokking Visualization)

Visualizes how layer imbalance or weight-to-target ratios affect training dynamics (e.g., NTK distances, learning speed)

Insight: Amplifying feedback induced by certain initializations leads to rich, feature-learning regimes and eliminates grokking delays

Figure 8 (Grokking Removal on MNIST)

Shows that techniques like weight or target scaling allow fast generalization without grokking on a 4-layer MLP

Insight: Proper initialization alone can place the model in a rich regime, avoiding overfitting phases

Appendices Aâ€“H

Contain full mathematical derivations for all key dynamics: sigmoidal learning, conservation laws, rank constraints, etc.

Insight: These provide theoretical rigor to show that complex DNN behavior can be captured by simple solvable models




<br/>
# refer format:     



@inproceedings{nam2025position,
  title={Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)},
  author={Nam, Yoonsoo and Lee, Seok Hyeong and Domine, Clementine Carla Juliette and Park, Yeachan and London, Charles and Choi, Wonyl and Goring, Niclas Alexander and Lee, Seungjai},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025},
  series={Proceedings of Machine Learning Research},
  volume={267},
  address={Vancouver, Canada},
  publisher={PMLR}
}




Nam, Yoonsoo, Seok Hyeong Lee, Clementine Carla Juliette Domine, Yeachan Park, Charles London, Wonyl Choi, Niclas Alexander Goring, and Seungjai Lee. â€œPosition: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking).â€ In Proceedings of the 42nd International Conference on Machine Learning, PMLR 267, Vancouver, Canada, 2025.





---
layout: post
title:  "[2025]HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning"
date:   2025-10-17 21:07:24 -0000
categories: study
---

{% highlight ruby %}

한줄 요약: HyMaTE 모델은 Mamba와 Transformer의 장점을 결합하여 전자 건강 기록(EHR) 데이터를 효과적으로 처리하는 하이브리드 아키텍처를 제안한다.


짧은 요약(Abstract) :



HyMaTE는 전자 건강 기록(EHR) 데이터를 효과적으로 표현하기 위해 설계된 하이브리드 모델입니다. EHR은 환자의 건강 진행 상황을 분석하는 데 중요한 역할을 하지만, 긴 다변량 시퀀스, 희소성 및 결측값으로 인해 전통적인 딥러닝 모델링에서 많은 도전 과제를 안고 있습니다. Transformer 기반 모델은 EHR 데이터 모델링과 임상 결과 예측에서 성공을 거두었지만, 이들의 이차 계산 복잡성과 제한된 컨텍스트 길이는 효율성과 실제 응용에 제약을 줍니다. 반면, Mamba와 같은 상태 공간 모델(SSM)은 긴 시퀀스를 처리하는 데 있어 선형 시간 모델링을 제공하지만, 주로 시퀀스 수준의 정보 혼합에 집중합니다. HyMaTE는 SSM의 강점과 고급 주의 메커니즘을 결합하여 이러한 문제를 해결하고, 여러 임상 데이터셋에서 예측 작업을 통해 EHR 데이터의 효과적이고 풍부한 통합 표현을 캡처할 수 있음을 입증합니다. 또한, 자기 주의 메커니즘에 의해 달성된 결과의 해석 가능성은 실제 의료 응용을 위한 확장 가능하고 일반화 가능한 솔루션으로서 모델의 효과성을 보여줍니다.




HyMaTE is a hybrid model designed for effectively representing electronic health record (EHR) data. EHRs play a crucial role in analyzing the progression of patient health; however, their complexity, characterized by long multivariate sequences, sparsity, and missing values, poses significant challenges in traditional deep learning modeling. While Transformer-based models have demonstrated success in modeling EHR data and predicting clinical outcomes, their quadratic computational complexity and limited context length hinder their efficiency and practical applications. On the other hand, State Space Models (SSMs) like Mamba offer linear-time sequence modeling for handling long sequences but primarily focus on mixing sequence-level information. HyMaTE addresses these challenges by combining the strengths of SSMs with advanced attention mechanisms, demonstrating its ability to capture an effective, richer, and more nuanced unified representation of EHR data through predictive tasks on multiple clinical datasets. Additionally, the interpretability of the outcomes achieved by self-attention illustrates the effectiveness of our model as a scalable and generalizable solution for real-world healthcare applications.


* Useful sentences :


{% endhighlight %}

<br/>

[Paper link]()
[~~Lecture link~~]()

<br/>

# 단어정리
*


<br/>
# Methodology



HyMaTE(하이브리드 Mamba 및 Transformer 모델)는 전자 건강 기록(EHR) 데이터를 효과적으로 표현하고 예측하기 위해 설계된 혁신적인 하이브리드 모델입니다. 이 모델은 두 가지 주요 아키텍처인 Mamba와 Transformer의 장점을 결합하여 EHR 데이터의 복잡성을 극복하고, 예측 성능을 향상시키는 것을 목표로 합니다.

#### 1. 모델 아키텍처
HyMaTE의 아키텍처는 크게 두 가지 주요 구성 요소로 나눌 수 있습니다: Mamba 블록과 Self-Attention 레이어입니다.

- **Mamba 블록**: Mamba는 상태 공간 모델(State Space Model, SSM)의 일종으로, 선형 시간 복잡도를 제공하여 긴 시퀀스를 효율적으로 처리할 수 있습니다. Mamba 블록은 입력 시퀀스를 처리하여 중요한 정보를 필터링하고, 이를 통해 EHR 데이터의 시퀀스 혼합을 수행합니다. Mamba는 각 입력 토큰을 상태 공간에 통합하여 컨텍스트 학습을 가능하게 합니다.

- **Self-Attention 레이어**: Mamba 블록을 통해 생성된 출력은 Self-Attention 레이어로 전달되어, 시퀀스 내의 지역적 의존성과 패턴을 추출합니다. 이 레이어는 입력 시퀀스의 각 요소가 다른 요소에 얼마나 주의를 기울여야 하는지를 결정하는 가중치를 계산합니다. 이를 통해 HyMaTE는 EHR 데이터의 복잡한 상호작용을 효과적으로 모델링할 수 있습니다.

- **Fusion Self-Attention**: Self-Attention 레이어의 출력은 Fusion Self-Attention 모듈로 전달되어, 지역적 및 전역적 컨텍스트를 통합합니다. 이 모듈은 최종 임베딩을 생성하는 데 사용되며, EHR 데이터의 다양한 특성을 종합적으로 반영합니다.

#### 2. 입력 임베딩
HyMaTE는 EHR 이벤트를 시간, 특성, 값의 세 가지 요소로 구성된 트리플렛으로 표현합니다. 각 트리플렛은 임베딩 레이어를 통해 고차원 벡터로 변환되며, 이 과정에서 각 요소는 개별적으로 처리됩니다. 이 임베딩은 Mamba 블록과 Self-Attention 레이어를 통해 더욱 정교한 표현으로 발전합니다.

#### 3. 트레이닝 방법
HyMaTE는 두 단계의 트레이닝 과정을 거칩니다:

- **자기 지도 학습(Self-Supervised Learning)**: 모델은 예측 작업을 통해 사전 학습을 수행합니다. 이 단계에서는 EHR 데이터의 결측값을 고려하여 예측 손실을 최소화하는 방식으로 학습합니다. 이를 통해 모델은 제한된 레이블 데이터로부터 강력한 표현을 학습할 수 있습니다.

- **지도 학습(Supervised Learning)**: 사전 학습 후, 모델은 특정 임상 예측 작업에 맞게 미세 조정됩니다. 이 단계에서는 최종 예측을 위해 추가적인 레이어가 추가됩니다.

#### 4. 해석 가능성
HyMaTE는 예측 결과의 해석 가능성을 높이기 위해 Fusion Self-Attention 레이어에서 파생된 주의 가중치를 분석합니다. 이를 통해 모델이 특정 예측을 내릴 때 어떤 변수와 시간대에 주목했는지를 파악할 수 있습니다. 이러한 해석 가능성은 임상 의사결정 지원에 중요한 역할을 합니다.



HyMaTE (Hybrid Mamba and Transformer Model) is an innovative hybrid model designed to effectively represent and predict electronic health record (EHR) data. This model aims to overcome the complexities of EHR data by combining the strengths of two primary architectures: Mamba and Transformer, thereby enhancing predictive performance.

#### 1. Model Architecture
The architecture of HyMaTE can be broadly divided into two main components: Mamba blocks and Self-Attention layers.

- **Mamba Blocks**: Mamba is a type of State Space Model (SSM) that provides linear time complexity, allowing for efficient processing of long sequences. The Mamba blocks process the input sequences to filter important information, performing sequence mixing of EHR data. Mamba integrates each input token into its state space, enabling context learning.

- **Self-Attention Layers**: The output generated by the Mamba blocks is passed to Self-Attention layers, which extract local dependencies and patterns within the sequence. These layers calculate weights that determine how much attention each element in the input sequence should pay to other elements. This allows HyMaTE to effectively model the complex interactions within EHR data.

- **Fusion Self-Attention**: The output from the Self-Attention layers is then sent to the Fusion Self-Attention module, which integrates both local and global contexts. This module is used to generate the final embedding, reflecting the diverse characteristics of EHR data comprehensively.

#### 2. Input Embedding
HyMaTE represents EHR events as triplets consisting of time, feature, and value. Each triplet is transformed into a high-dimensional vector through an embedding layer, where each component is processed individually. This embedding is further refined through the Mamba blocks and Self-Attention layers.

#### 3. Training Method
HyMaTE undergoes a two-phase training process:

- **Self-Supervised Learning**: The model performs pre-training through predictive tasks. In this phase, it minimizes prediction loss while considering missing values in the EHR data. This allows the model to learn robust representations from limited labeled data.

- **Supervised Learning**: After pre-training, the model is fine-tuned for specific clinical prediction tasks. Additional layers are attached for final predictions during this phase.

#### 4. Interpretability
HyMaTE enhances the interpretability of its predictions by analyzing the attention weights derived from the Fusion Self-Attention layer. This analysis helps identify which variables and time points the model focused on when making specific predictions. Such interpretability plays a crucial role in clinical decision support.


<br/>
# Results



HyMaTE 모델은 다양한 임상 예측 작업에서 우수한 성능을 보여주었으며, PhysioNet Challenge 2012, MIMIC-IV, 그리고 소아 비만 관리 데이터셋을 포함한 여러 EHR 데이터셋에서 평가되었습니다. 이 모델은 경쟁 모델들과 비교하여 다음과 같은 성과를 기록했습니다.

1. **PhysioNet 2012 데이터셋 - 사망 예측**
   - HyMaTE는 AUROC(Receiver Operating Characteristic 곡선 아래 면적) 0.868과 AUPRC(Precision-Recall 곡선 아래 면적) 0.602를 기록했습니다. 이는 DuETT 모델의 AUROC 0.857 및 AUPRC 0.598보다 우수한 성과입니다.

2. **MIMIC-IV 데이터셋 - 사망 예측**
   - HyMaTE는 AUROC 0.907과 AUPRC 0.648을 달성하여 EHR-Mamba 모델의 AUROC 0.881 및 AUPRC 0.637을 초과했습니다. 이는 HyMaTE가 MIMIC-IV 데이터셋에서 사망 예측 작업에서 가장 높은 성능을 보였음을 나타냅니다.

3. **MIMIC-IV 데이터셋 - 입원 기간 예측**
   - HyMaTE는 AUROC 0.802와 AUPRC 0.495를 기록하여 EHR-Mamba의 AUROC 0.781 및 AUPRC 0.428을 초과했습니다. 이는 HyMaTE가 입원 기간 예측에서도 우수한 성능을 발휘했음을 보여줍니다.

4. **MIMIC-IV 데이터셋 - 재입원 예측**
   - HyMaTE는 AUROC 0.798과 AUPRC 0.624를 기록하여 DuETT의 AUROC 0.772 및 AUPRC 0.592를 초과했습니다. 이는 HyMaTE가 재입원 예측에서도 가장 높은 성능을 보였음을 나타냅니다.

5. **소아 비만 관리 데이터셋 - 체중 감소 예측**
   - HyMaTE는 AUROC 0.752와 AUPRC 0.387을 기록하여 EHR-Mamba의 AUROC 0.704 및 AUPRC 0.284를 초과했습니다. 이는 HyMaTE가 소아 비만 관리 데이터셋에서도 우수한 성능을 발휘했음을 보여줍니다.

이러한 결과들은 HyMaTE 모델이 다양한 임상 예측 작업에서 경쟁 모델들보다 뛰어난 성능을 발휘하며, EHR 데이터의 복잡성을 효과적으로 처리할 수 있는 능력을 갖추고 있음을 보여줍니다. HyMaTE는 특히 긴 시퀀스를 효율적으로 처리할 수 있는 능력과 해석 가능성을 강조하며, 실제 의료 환경에서의 활용 가능성을 높이고 있습니다.

---




The HyMaTE model demonstrated superior performance across various clinical prediction tasks, evaluated on multiple EHR datasets, including the PhysioNet Challenge 2012, MIMIC-IV, and a pediatric weight management dataset. The model achieved the following results compared to competing models:

1. **PhysioNet 2012 Dataset - Mortality Prediction**
   - HyMaTE recorded an AUROC (Area Under the Receiver Operating Characteristic curve) of 0.868 and an AUPRC (Area Under the Precision-Recall curve) of 0.602, outperforming the DuETT model, which achieved an AUROC of 0.857 and an AUPRC of 0.598.

2. **MIMIC-IV Dataset - Mortality Prediction**
   - HyMaTE achieved an AUROC of 0.907 and an AUPRC of 0.648, surpassing the EHR-Mamba model's AUROC of 0.881 and AUPRC of 0.637. This indicates that HyMaTE performed the best in mortality prediction tasks on the MIMIC-IV dataset.

3. **MIMIC-IV Dataset - Length of Stay Prediction**
   - HyMaTE recorded an AUROC of 0.802 and an AUPRC of 0.495, exceeding EHR-Mamba's AUROC of 0.781 and AUPRC of 0.428. This shows that HyMaTE also excelled in predicting length of stay.

4. **MIMIC-IV Dataset - Readmission Prediction**
   - HyMaTE achieved an AUROC of 0.798 and an AUPRC of 0.624, outperforming DuETT's AUROC of 0.772 and AUPRC of 0.592. This indicates that HyMaTE had the highest performance in readmission prediction as well.

5. **Pediatric Weight Management Dataset - Weight Loss Prediction**
   - HyMaTE recorded an AUROC of 0.752 and an AUPRC of 0.387, surpassing EHR-Mamba's AUROC of 0.704 and AUPRC of 0.284. This demonstrates that HyMaTE also performed well on the pediatric weight management dataset.

These results illustrate that the HyMaTE model outperforms competing models across various clinical prediction tasks and possesses the capability to effectively handle the complexities of EHR data. HyMaTE emphasizes its ability to efficiently process long sequences and its interpretability, enhancing its potential for real-world healthcare applications.


<br/>
# 예제



HyMaTE 모델은 전자 건강 기록(EHR) 데이터를 처리하기 위해 설계된 하이브리드 모델로, 주로 두 가지 주요 단계로 구성됩니다: **자기 지도 사전 훈련**과 **감독된 미세 조정**입니다. 이 모델은 다양한 임상 예측 작업을 수행하기 위해 여러 데이터셋에서 훈련되고 평가됩니다.

#### 1. 데이터셋 및 입력 형식
HyMaTE는 세 가지 주요 데이터셋을 사용하여 평가됩니다:

- **PhysioNet Challenge 2012**: 이 데이터셋은 ICU 환자의 병원 내 사망 예측을 위한 데이터로, 11,988명의 환자와 42개의 변수(37개의 시계열 이벤트 포함)를 포함합니다. 입력 데이터는 환자의 첫 48시간 동안의 생체 신호 및 실험실 결과를 포함하며, 출력은 다음 2시간 이내의 사망 여부(0 또는 1)입니다.

- **MIMIC-IV**: 이 데이터셋은 Beth Israel Deaconess Medical Center의 퇴원 환자 데이터를 포함하며, 53,150명의 환자와 69,211회의 입원을 포함합니다. 입력은 환자의 ICU 체류 첫 48시간 동안의 시계열 데이터이며, 출력은 병원 내 사망, 1주 이상 체류, 1개월 이내 재입원 여부입니다.

- **Pediatric Weight Management**: 이 데이터셋은 대규모 소아 건강 관리 시스템에서 추출된 데이터로, 14,392명의 소아 환자 기록을 포함합니다. 입력은 환자의 시계열 데이터와 정적 변수(예: 나이, 성별)이며, 출력은 항비만 약물을 사용하는 환자에서 5% 이상의 체중 감소 여부입니다.

#### 2. 모델 아키텍처
HyMaTE의 아키텍처는 다음과 같은 주요 구성 요소로 이루어져 있습니다:

- **입력 임베딩**: EHR 이벤트는 시간, 변수, 값의 세 가지 요소로 구성된 트리플렛으로 표현됩니다. 각 트리플렛은 고차원 벡터로 변환되어 모델에 입력됩니다.

- **Mamba 블록**: 입력 임베딩은 여러 Mamba 블록을 통과하여 시퀀스 정보를 효율적으로 인코딩합니다. Mamba는 선형 시간 복잡도로 긴 시퀀스를 처리할 수 있는 장점이 있습니다.

- **자기 주의 레이어**: Mamba 블록의 출력을 바탕으로 자기 주의 레이어가 적용되어 시퀀스 내의 지역적 의존성을 캡처합니다.

- **융합 자기 주의 레이어**: 최종적으로, 융합 자기 주의 레이어가 지역적 및 전역적 정보를 통합하여 최종 임베딩을 생성합니다.

#### 3. 훈련 및 평가
모델은 두 단계로 훈련됩니다:

- **자기 지도 사전 훈련**: 이 단계에서는 예측 마스크를 사용하여 관찰된 변수만을 기반으로 예측을 수행합니다. 이 과정에서 모델은 제한된 레이블이 있는 EHR 데이터에서 강력한 표현을 학습합니다.

- **감독된 미세 조정**: 사전 훈련된 모델은 특정 임상 예측 작업에 맞게 미세 조정됩니다. 이 단계에서는 각 데이터셋에 맞는 출력 레이어가 추가되어 최종 예측을 생성합니다.

모델의 성능은 AUROC(Receiver Operating Characteristic 곡선 아래 면적) 및 AUPRC(Precision-Recall 곡선 아래 면적)와 같은 지표를 사용하여 평가됩니다. 예를 들어, PhysioNet 2012 데이터셋에서 HyMaTE는 AUROC 0.868과 AUPRC 0.602를 기록하여 다른 모델들보다 우수한 성능을 보였습니다.




The HyMaTE model is a hybrid architecture designed to process Electronic Health Record (EHR) data, primarily consisting of two main stages: **self-supervised pre-training** and **supervised fine-tuning**. This model is trained and evaluated on various clinical prediction tasks using multiple datasets.

#### 1. Datasets and Input Format
HyMaTE is evaluated using three main datasets:

- **PhysioNet Challenge 2012**: This dataset is used for predicting in-hospital mortality of ICU patients, containing data from 11,988 patients and 42 variables (including 37 time-series events). The input data includes vital signs and laboratory results from the first 48 hours of a patient's ICU stay, while the output is a binary label indicating whether the patient died within the next 2 hours (0 or 1).

- **MIMIC-IV**: This dataset consists of de-identified retrospective patient data from the Beth Israel Deaconess Medical Center, focusing on a derived ICU subset with 53,150 patients and 69,211 admissions. The input is the time-series data from the first 48 hours of a patient's stay, and the output includes predictions for in-hospital mortality, length of stay exceeding 1 week, and readmission within 1 month.

- **Pediatric Weight Management**: This dataset is extracted from the EHR of a large pediatric healthcare system, encompassing 14,392 individual pediatric records. The input consists of time-series data and static variables (e.g., age, gender), while the output is a binary label indicating whether the patient achieved a weight loss of more than 5% while using anti-obesity medications.

#### 2. Model Architecture
The architecture of HyMaTE consists of the following key components:

- **Input Embedding**: EHR events are represented as triplets consisting of time, feature, and value. Each triplet is transformed into a high-dimensional vector for input into the model.

- **Mamba Blocks**: The input embeddings pass through multiple Mamba blocks, which efficiently encode sequence information. Mamba offers the advantage of processing long sequences with linear time complexity.

- **Self-Attention Layer**: A self-attention layer is applied to the output of the Mamba blocks to capture local dependencies within the sequence.

- **Fusion Self-Attention Layer**: Finally, a fusion self-attention layer integrates local and global information to generate the final embedding.

#### 3. Training and Evaluation
The model is trained in two phases:

- **Self-Supervised Pre-Training**: In this phase, a forecasting task is performed using a mask to exclude unobserved variables. This process allows the model to learn robust representations from limited labeled EHR data.

- **Supervised Fine-Tuning**: The pre-trained model is fine-tuned for specific clinical prediction tasks by adding output layers tailored to each dataset.

The model's performance is evaluated using metrics such as AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall curve). For instance, on the PhysioNet 2012 dataset, HyMaTE achieved an AUROC of 0.868 and an AUPRC of 0.602, demonstrating superior performance compared to other models.

<br/>
# 요약

**한국어 요약:** HyMaTE 모델은 Mamba와 Transformer의 장점을 결합하여 전자 건강 기록(EHR) 데이터를 효과적으로 처리하는 하이브리드 아키텍처를 제안한다. 다양한 임상 예측 작업에서 HyMaTE는 PhysioNet 2012 및 MIMIC-IV 데이터셋에서 우수한 성능을 보였으며, 특히 MIMIC-IV에서 사망 예측에 대해 AUROC 0.907을 기록했다. 예를 들어, 82세 남성 환자의 사망 예측에서 모델은 NIMAP 데이터의 변동에 주목하여 높은 예측 확률을 도출하였다.

**English Summary:** The HyMaTE model proposes a hybrid architecture that combines the strengths of Mamba and Transformer to effectively process electronic health record (EHR) data. HyMaTE demonstrated superior performance across various clinical prediction tasks, achieving an AUROC of 0.907 for mortality prediction on the MIMIC-IV dataset. For instance, in predicting the mortality of an 82-year-old male patient, the model highlighted fluctuations in NIMAP data, leading to a high prediction probability.

<br/>
# 기타



1. **다이어그램 및 피규어**:
   - **모델 아키텍처 다이어그램**: HyMaTE의 아키텍처는 Mamba 블록과 Self-Attention 레이어를 결합하여 EHR 데이터를 처리하는 과정을 시각적으로 나타냅니다. 이 구조는 Mamba의 효율적인 시퀀스 인코딩과 Transformer의 강력한 주의 메커니즘을 통합하여, 긴 시퀀스를 효과적으로 처리할 수 있도록 설계되었습니다.
   - **AUROC vs. 시퀀스 길이 그래프**: 이 그래프는 시퀀스 길이에 따른 HyMaTE의 AUROC 성능을 보여줍니다. 시퀀스 길이가 증가함에 따라 성능 저하가 최소화되는 경향을 보이며, 이는 HyMaTE가 긴 시퀀스를 효과적으로 처리할 수 있는 능력을 강조합니다.

2. **테이블**:
   - **성능 비교 테이블**: HyMaTE는 PhysioNet 2012, MIMIC-IV, 그리고 소아 비만 예측 데이터셋에서 다양한 임상 예측 작업에 대해 다른 모델들과 비교하여 우수한 성능을 보였습니다. 예를 들어, MIMIC-IV의 사망 예측에서 HyMaTE는 AUROC 0.907과 AUPRC 0.648을 기록하여 가장 높은 성능을 달성했습니다. 이는 HyMaTE가 EHR 데이터의 복잡성을 효과적으로 처리할 수 있음을 나타냅니다.
   - **Ablation Study 테이블**: 이 테이블은 HyMaTE의 각 구성 요소를 제거했을 때의 성능 변화를 보여줍니다. 예를 들어, Self-Attention 레이어를 제거했을 때 성능이 크게 저하되었으며, 이는 HyMaTE의 주의 메커니즘이 모델의 성능에 필수적임을 시사합니다.

3. **어펜딕스**:
   - **기타 모델 성능 비교**: 어펜딕스에서는 HyMaTE와 비교한 다양한 베이스라인 모델의 성능을 상세히 나열하고 있습니다. 이 데이터는 HyMaTE의 성능이 다른 기존 모델들에 비해 얼마나 우수한지를 명확히 보여줍니다.




1. **Diagrams and Figures**:
   - **Model Architecture Diagram**: The architecture of HyMaTE visually represents the process of handling EHR data by combining Mamba blocks and Self-Attention layers. This structure integrates the efficient sequence encoding of Mamba with the powerful attention mechanisms of Transformers, designed to effectively process long sequences.
   - **AUROC vs. Sequence Length Graph**: This graph illustrates the performance of HyMaTE in terms of AUROC as the sequence length increases. It shows a trend of minimal performance degradation with longer sequences, emphasizing HyMaTE's capability to handle long sequences effectively.

2. **Tables**:
   - **Performance Comparison Table**: HyMaTE demonstrated superior performance across various clinical prediction tasks on datasets such as PhysioNet 2012, MIMIC-IV, and the pediatric weight management dataset when compared to other models. For instance, in the MIMIC-IV mortality prediction task, HyMaTE achieved an AUROC of 0.907 and an AUPRC of 0.648, marking the highest performance. This indicates HyMaTE's effectiveness in managing the complexities of EHR data.
   - **Ablation Study Table**: This table shows the performance changes when each component of HyMaTE is removed. For example, removing the Self-Attention layer resulted in a significant drop in performance, suggesting that the attention mechanisms are crucial for the model's effectiveness.

3. **Appendices**:
   - **Additional Model Performance Comparison**: The appendix provides a detailed list of various baseline models compared to HyMaTE, showcasing how HyMaTE's performance stands out against existing models. This data clearly illustrates the advantages of HyMaTE in terms of predictive accuracy and efficiency.

<br/>
# refer format:
### BibTeX 형식

```bibtex
@article{Mottalib2025,
  author = {Mottalib, Md Mozaharul and Phan, Thao-Ly T. and Beheshti, Rahmatollah},
  title = {HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning},
  journal = {arXiv preprint arXiv:2509.24118},
  year = {2025},
  url = {https://github.com/healthylaife/HyMaTE}
}
```

### 시카고 스타일

Mottalib, Md Mozaharul, Thao-Ly T. Phan, and Rahmatollah Beheshti. 2025. "HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning." arXiv preprint arXiv:2509.24118. https://github.com/healthylaife/HyMaTE.

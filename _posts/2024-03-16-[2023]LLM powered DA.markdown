---
layout: post
title:  "[2023]LLM-powered Data Augmentation for Enhanced Crosslingual Performance"  
date:   2024-03-16 22:16:29 -0400
categories: study
---

{% highlight ruby %}


한줄 요약: LLM을 이용한 Data Augmentation을 통해 데이터가 적은 언어에서 좋은 벤치마크 성능을 보임, 하지만 타밀어처럼 데이터 너무 적을 경우 효과 낮았는데 이는 토크나이저 학습이 매우 중요함을 의미    

짧은 요약(Abstract) :    
* 이 논문은 큰 언어 모델(Large Language Models, LLMs)을 활용하여 다국어 상식 추론 데이터셋에서 극도로 제한된 학습 데이터를 보강하는 가능성을 탐구함  
* 여러 LLMs, 즉 Dolly-v2, StableVicuna, ChatGPT, 그리고 GPT-4를 사용하여 세 가지 데이터셋인 XCOPA, XWinograd, 그리고 XStoryCloze를 보강함  
* 이후 합성 데이터를 사용하여 작은 다국어 모델인 mBERT와 XLMR을 미세 조정한 효과를 평가함  
* 영어로 생성된 데이터와 대상 언어로 생성된 데이터를 비교하며, LLM에 의해 생성된 데이터를 포함하는 것이 상당한 정확도 향상, 예를 들어 최상의 경우 13.4 정확도 점수 개선을 가져온다는 점에서 전반적인 이점을 보여줌  
* 또한, 다양한 언어로 생성된 예시의 자연스러움과 논리적 일관성을 평가하기 위해 원어민 평가자를 활용한 인간 평가를 수행함  
* 평가 결과, ChatGPT와 GPT-4와 같은 LLM은 대부분의 언어에서 자연스럽고 일관된 텍스트를 생성하는 데 뛰어난 능력을 보이지만, 타밀어와 같은 특정 언어에서 의미 있는 텍스트를 생성하는 데 어려움을 겪음  
* ChatGPT는 원본 데이터셋과 비교하여 타당한 대안을 생성하는 데 있어 부족함을 보이지만, GPT-4에서 생성된 예시는 논리적 일관성 측면에서 경쟁력을 보임  
* 생성된 데이터를 공개하여 재현성과 공공 사용을 가능하게 함  

Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1Uns3ACj46wgMzxu30a-L2-dWTA_M-bYo?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* costlier: costly의 비교급, 더 비싼  
* clear-cut: 명백한, 분명한 (Adjective): 무엇인가가 매우 분명하고 의심의 여지가 없을 때 사용. 예를 들어, “The evidence was clear-cut”는 “증거가 명백했다”라는 의미   
<br/>

# 1. Introduction  
* 자연어 처리(NLP) 모델의 성공은 훈련 데이터의 가용성과 품질에 크게 의존함  
* 이는 영어 이외의 언어에 대한 데이터가 일반적으로 제한되어 있다는 점에서 다국어 NLP에 있어 중요한 도전 과제임(Ponti et al. 2019; Joshi et al. 2020; Whitehouse et al. 2022)   
* 데이터 부족 문제를 해결하는 한 가지 방법은 제로샷 교차 언어 전이나 다양한 언어에서의 다중 과제 학습을 통해 접근하는 것이며, 이는 수동으로 생성된 영어 학습 데이터에서의 제로샷 전이 학습을 통해 수행될 수 있음  
* 저자들의 실험은 상대적으로 큰 규모의 합성 데이터셋으로 모델을 훈련시키는 것이 제한된 수동 생성 데이터셋으로 훈련시키는 것보다 더 나은 성능을 달성함을 보여줌  
* 이러한 발견은 LLM으로 생성된 합성 데이터가 하류 과제에 특화된 모델을 개선하는 데 유용함을 실증적으로 확인함  
* XCOPA 데이터셋에서 다국어 데이터 합성을 확장한 결과, ChatGPT와 GPT-4를 사용하여 다국어 데이터셋을 생성하는 것이 영어로 생성된 데이터에 대한 교차 언어 제로샷 전이의 효과를 일반적으로 능가함을 발견함  
* 저자들은 다양한 언어에서 생성된 데이터셋의 질을 평가하기 위해 원어민 평가자에게 생성된 데이터에 대한 자연스러움과 논리적 일관성을 평가하도록 요청함  
* 평가 결과, ChatGPT와 GPT-4가 생성한 데이터는 대부분의 언어에서 높은 자연스러움을 보이며 심지어 원본 데이터를 능가하는 경우도 있음을 보여줌  
* 그러나 타밀어와 같은 특정 언어에서는 두 모델 모두 자연스러운 텍스트를 생성하는 데 실패함   
* ChatGPT로 생성된 데이터셋 예시를 평가할 때 더 그럴듯한 옵션과 관련하여 원래 인간이 만든 데이터와 비교할 때 주목할 만한 일관성이 없음을 밝혀냄  
* 반면, GPT-4는 인간이 작성한 데이터와 동등한 수준의 성능을 보임  
* 요약하자면, LLM을 이용한 데이터 보강은 가능성이 있음을 보여주나, 데이터 생성에 사용된 LLM의 선택이 결과 데이터의 질과 고려되는 언어의 적용 가능성에 상당한 영향을 미침  
* 더 발전된 모델 예를 들어 GPT-4에 접근할 수 없는 상황에서는 다른 모델을 사용할 수 있으나, 이는 특정 비영어권 언어에서 성능이 떨어질 수 있는 문제 - GPT-4에도 존재하는 문제 -와 논리적 일관성에 대한 우려를 낳을 수 있음  
<br/>  

# 2. Related Work  
## Multilingual and Low-Resource NLP  
* 최근에는 NLP를 영어를 넘어 확장하는 데 더 많은 관심이 기울여지고 있으며, 이는 다국어 모델의 개발과 다국어 과제를 다루기 위한 벤치마크의 생성을 포함함  
* 여러 언어 간에 흔히 마주치는 주요 과제 중 하나는 사용 가능한 데이터의 부족임  
* 결과적으로, 데이터가 부족할 때 하나의 접근법은 제로샷 다국어 전이를 사용하는 것임  
* Winata 등에 의한 연구는 관련 언어 간의 제로샷 다국어 전이의 효과를 입증함  
* 또한, Muennighoff 등은 영어 지시 데이터로만 미세 조정된 모델이 다국어 지시를 이해할 수 있음을 보여줌  
* 이 연구에서는 데이터의 가용성이 제한된 유사한 상황을 다룸  


## Multilingual Data Augmentation  
* Lauscher et al. (2020)은 소수의 예시가 작은 모델의 다국어 성능을 크게 향상시킬 수 있음을 보여주어 다국어 데이터 보강이 효과적인 전략임을 증명함  
* 모델의 다국어 정확도를 측정 및 모델링(Xia et al. 2020)을 통해 다국어 데이터 보강 전략, 예를 들어 전달 언어 선택(Lin et al. 2019) 및 최적의 데이터 보강 접근 방식을 위한 다국어 소수샷 정확도 예측(Srinivasan et al. 2022)을 연구하는 일련의 작업들이 있음  
* 많은 연구들이 언어 이론(Lee et al. 2019; Pratapa et al. 2018), 기계 번역 모델(Tarunesh et al. 2021), 병렬 코퍼스 및 위키피디아(Winata et al. 2019; Whitehouse et al. 2022), 그리고 ChatGPT(Dai et al. 2023)를 활용하여 코드 혼합을 위한 합성 데이터 보강에 중점을 둠  
* 본 연구에서는 강력한 지시 조정된 LLMs를 사용한 다국어 상식 데이터셋에 대한 데이터 보강을 탐구함  



<br/>

# 3. Dataset Augmentation  
* 저자들의 실험은 학습 데이터의 제한된 가용성과 데이터 합성에 대한 상식 추론 데이터셋의 더 큰 도전으로 인해 선택된 XCOPA, XWinograd, 그리고 XStoryCloze를 사용함  
* XCOPA는 영어(EN) COPA의 검증 및 테스트 세트를 11개의 대상 언어로 번역하고 다시 주석을 달아 만든 가상 언어 선택 데이터셋임  
* XWinograd는 원본 영어 Winograd Schema Challenge(WSC)를 다른 5개 언어(FR: 프랑스어, JA: 일본어, PT: 포르투갈어, RU: 러시아어, 그리고 ZH: 중국어)로 확장하여, 기계의 상식 추론 능력을 평가하기 위한 대명사 해결 문제를 포함함  
* XStoryCloze는 원본 영어 StoryCloze 데이터셋의 검증 분할을 10개의 언어로 번역하여 수집한 것으로, 4문장으로 이루어진 상식 이야기와 정확한 결말 및 잘못된 결말을 포함함  

## 3.1 LLMs for Data Generation  
* 저자들의 예비 실험은 하류 NLP 작업에 특별히 미세 조정된 언어 모델이 복잡한 지시사항을 따르는 데 어려움을 겪는다는 것을 발견함  
* 반면에 Dolly-v2, StableVicuna, ChatGPT, 그리고 GPT-4와 같은 최신 LLM은 더 복잡하고 일반적인 목적의 지시사항을 처리하도록 설계되었으며 데이터 생성을 위한 저자들의 지시사항을 따르는 데 성공함  
* ChatGPT와 GPT-4는 특히 비영어 언어로 예시를 생성할 수 있는 능력이 두드러짐  

* 저자들은 개방형 모델과 폐쇄형 모델 사이의 균형을 맞추기 위해 앞서 언급된 네 가지 LLM을 탐색함  
* 특히 dolly-v2-12b5를 사용하며, 이는 EleutherAI의 Pythia-12b에서 파생되었고 Databricks 직원들이 생성한 약 15K의 지시사항에 미세 조정되었음  
* 그리고 다양한 대화 및 지시 데이터셋에 대해 RLHF(인간 피드백에서의 강화 학습)로 미세 조정된 Vicuna 모델인 StableVicuna-13B를 사용함  
* Vicuna는 ShareGPT에서 수집된 사용자 공유 대화에 미세 조정된 오픈 소스 LLaMA 모델임  


## 3.2 Instructions and Responses  
* 저자들은 데이터셋 문서의 설명을 참조하여 LLM을 이용해 모든 데이터셋에 대한 합성 예시를 생성하기 위한 지시사항을 구성함  
* 원본 데이터셋의 학습(+검증) 분할에서 무작위로 샘플링한 몇 가지 예시를 제공하고 LLM에 유사한 데이터 포인트를 생성하도록 요청함  
* 다양한 지시사항을 실험하고, 합성된 데이터를 소규모로 평가한 후, 오류를 기반으로 지시사항을 업데이트하고 최종 데이터셋을 생성하기 위한 최적의 지시사항을 선택함  
* 최종 지시사항과 응답은 표 2에 나와 있으며, 데이터 생성 과정에는 다음의 주요 단계가 포함됨:   
(1) 생성하려는 예시의 총 수를 결정함   
(2) 다음의 반복 프로세스를 통해 예시를 생성함:     
(a) 다양성을 보장하기 위해 훈련 데이터셋에서 n개의 예시를 무작위로 샘플링함     
(b) 이러한 샘플링된 예시를 지시사항에 추가하고 모델에 m개의 새로운 예시를 생성하도록 요청함   
(c) 이후에 유효하고 고유한 예시만 생성된 세트에 추가함  
* 저자들은 고정된 예산 시나리오에 초점을 맞추고 각 데이터셋에 대해 LLM을 이용하여 총 3-4K 데이터 포인트를 먼저 생성함  


<br/>
# 4. Experimental Setups  
* 저자들은 먼저 Dolly-v2, StableVicuna, ChatGPT, 그리고 GPT-4를 사용하여 XCOPA, XWinograd, 그리고 XStoryCloze에 대한 합성 영어 예시를 생성함    
* 최종적으로 필터링된 세 데이터셋의 합성 데이터 크기는 각각 3.7k, 2k, 그리고 1.7k임    
* 이후 저자들은 합성 데이터로 mBERT, XLMR-base, 그리고 XLMR-large를 미세 조정하고 대상 언어의 원래 검증 세트를 사용하여 다양한 언어에서의 제로샷 교차 언어 전이 성능을 비교함    
* XCOPA의 경우, 저자들은 대상 언어로 직접 데이터 포인트를 생성하기 위해 대상 언어의 예시를 제공하고 생성된 데이터에 대한 언어를 명시함(표 2 참조)    
* 하지만 TH와 TR 학습/검증 데이터에 원인에 대한 예시가 포함되지 않았기 때문에(테스트 분할에는 나타남), 저자들은 두 언어에 대한 XCOPA를 생성하지 않음    
* 다국어 합성 데이터 생성을 위해 저자들은 ChatGPT와 GPT-4를 사용함    

<br/>

# 5. Results and Discussion  

* 이 섹션은 세 데이터셋에 대한 미세 조정된 모델의 주요 결과를 제시하고 다양한 LLMs, 언어 및 규모에서 생성된 데이터와의 성능을 비교함   

## 5.1 General Results  
* 표 4는 세 데이터셋에서 모든 언어에 대해 미세 조정된 mBERT, XLMR-Base 및 XLMR-Large 모델의 평균 정확도를 제시함    
* 모델은 원본 데이터(ORI), 다양한 LLM에서 생성된 데이터(GEN), 그리고 두 소스의 조합(O+G)을 영어로 사용하여 훈련됨    
* 다양한 데이터셋, LLM 및 미세 조정된 모델에서 원본 및 LLM에서 생성된 데이터를 모두 사용할 때 일관된 개선이 관찰됨    
* 모델 중에서 Dolly-v2가 mBERT를 미세 조정할 때 Xingorad에서 가장 우수한 성능을 보임  

## 5.2 Multilingual Data Generation  
* 저자들은 합성된 다국어 데이터셋이 영어로만 훈련된 것보다 우수한 성능을 보이는지 조사함    
* XCOPA 데이터셋을 선택하고 LLM에게 대상 언어로 직접 응답을 생성하도록 요청하거나 Google Translate API를 사용하여 영어로 생성된 데이터를 대상 언어로 번역하는 두 가지 설정을 탐색함    
* Dolly-v2와 StableVicuna은 비영어 텍스트 생성에 효과적이지 않기 때문에 제외됨    
* GPT-4가 가장 유망한 성능을 보이지만 ChatGPT에 비해 비용이 훨씬 많이 듦   

## 5.3 Dataset Scaling Up  
* 저자들은 생성된 데이터의 대규모 학습이 모델 성능에 미치는 영향을 조사함  
* 특히, XCOPA 데이터셋에 초점을 맞추고 ChatGPT(예산 효율성이 더 높음)를 사용하여 영어로 생성된 데이터를 28.6k 예시로 확장함    
* 또한 영어로 생성된 데이터를 대상 언어로 번역한 결과와 비교함    
* 표 6에서 보이듯이 생성된 데이터를 28k 이상으로 확장할 때 모델 성능에 긍정적인 영향을 미침을 보여줌    
* 특히 XLMR-Large가 가장 눈에 띄는 개선을 보임    
* 또한, 저자들은 원본 데이터셋의 고정된 비율로 데이터를 생성하는 실험을 수행하고, 그 결과를 부록 C에 포함함    

<br/>

# 6. Human Evaluation  
* 이 연구에서는 생성된 데이터셋의 질을 더 잘 평가하고 인간이 만든 데이터와 비교하기 위해 ChatGPT와 GPT-4로 생성된 다국어 데이터에 대해 원어민 평가자들에게 주석을 달도록 요청함  
* 각 데이터셋에 대해 영어로 생성된 50개의 예시를 먼저 선택한 후, 두 가지 범주에서 예시를 평가할 두 명의 주석자를 요청함:  

## 6.1 Text Naturalness  
* 텍스트 자연스러움: 주석자들에게 각 예시에 대해 다음 옵션 중 하나를 선택하도록 요청함: "텍스트가 자연스럽게 들림", "텍스트가 어색하지만 이해할 수 있음", 또는 "텍스트가 이해할 수 없음"  

### Issue with Tamil  
* 반면 타밀어 데이터셋의 성능은 예상외로 낮았으며, 대부분의 예시가 '이해할 수 없음'으로 분류됨  
* 언어 전문가와 상의한 결과 타밀어의 주요 문제는 여러 가지로 확인됨  
* 첫째, '다시 시도해볼 거야'와 같이 같은 의미의 중복된 단어 삽입, 둘째, 동사 일치 오류, 그리고 셋째, 상황에 맞지 않는 흔하지 않은 단어의 존재가 있음  
* 타밀어를 생성하는 것이 GPT-4를 사용하여 느리고 비용이 많이 든다는 사실을 발견함  
* 이는 타밀어뿐만 아니라 텔루구어와 칸나다어와 같은 유사 언어에 대한 토크나이저가 제대로 훈련되지 않았기 때문에 이러한 언어에서 사용할 수 없는 생성을 초래하는 것으로 의심됨  
* 생성된 데이터의 낮은 질이 XLMR-Large 모델을 ChatGPT로 생성된 타밀어 데이터로 미세 조정할 때 성능이 눈에 띄게 감소하는 이유를 설명할 수 있음  
* 그럼에도 불구하고, GPT-4로 생성된 타밀어 데이터로 훈련된 모델은 기준선을 상회하는 개선을 보임 
* 이는 추가적인 조사가 필요한 흥미로운 현상임  

## 6.2 Logic Soundness  
* 논리적 타당성: 이 범주는 예시의 상식적 측면에 초점을 맞춤   
* 주석자들은 "올바른 옵션이 (분명히) 더 타당함", "두 옵션이 모두 동등하게 타당함", "두 옵션이 모두 타당하지 않음", 또는 "잘못된 옵션이 실제로 더 타당함" 중에서 가장 적절한 설명을 선택해야 함  
* 텍스트가 적어도 이해할 수 있는 수준일 경우에만 주석자들에게 논리를 평가하도록 요청함  
* XWinograd에 대해서는 추가 평가 기준을 도입함  
* 주석자들에게 예시에서 두 명사구를 동일한 대명사로 대체할 수 있는지 판단하도록 요청함(§3.2 참조)  
* XCOPA의 경우, 가장 주목할 만한 개선을 보인 두 언어, 즉 ZH와 ID뿐만 아니라 가장 개선이 적거나 후퇴한 두 언어에 대한 주석을 영어가 아닌 언어로 확장함​  


<br/>
# 7. Conclusions  
* 이 논문에서는 큰 언어 모델(Large Language Models, LLMs)을 활용하여 교차 언어 데이터셋에서 제한된 훈련 데이터의 효과를 향상시킬 수 있는 가능성을 탐구함  
* 특히 상식 추론 작업에 대한 데이터 합성에 있어서 도전적인 문제를 다룸  
* 다양한 LLMs를 사용하여 세 가지 데이터셋을 보강하고, 합성된 데이터를 사용하여 작은 다국어 모델들을 미세 조정한 결과를 비교함  
* LLMs, 특히 GPT-4가 생성한 데이터는 대부분의 언어에서 상당한 성능 향상을 보여줌에도 불구하고, 일부 저자원 언어에서는 이러한 모델들이 효과적인 데이터를 생성하는데 어려움을 겪을 수 있음  
* 최적의 성능을 달성하기 위해서는 대상 언어의 몇 가지 예시가 여전히 필요함  
* 저자들이 사용한 폐쇄된 모델들, 예를 들어 GPT-4는 라이선스 제한으로 인해 접근이 제한되어 있으며, 이로 인해 얻은 결과는 재현이 불가능할 수 있음  
* 그럼에도 불구하고, LLMs를 활용한 다국어 데이터셋 보강의 잠재적 이점을 시험한 이 연구는 유망함  
* LLMs를 활용한 합성 데이터 생성, 특히 다국어 데이터에 대해 접근하는 것은 감수성과 존중을 요구함  
* LLMs가 웹 데이터에 기반하여 훈련되었기 때문에, 특정 언어나 커뮤니티에 대한 편견이나 스테레오타입을 전파할 위험이 있음  
* 따라서, 특정 언어나 커뮤니티를 대표하는 문화적 정체성을 반영할 때는 언어학자, 언어 전문가, 커뮤니티 대표와 협력하여 스테레오타입의 전파나 문화적 민감성을 피할 필요가 있음  


<br/>  
# 요약  
* 이 논문은 큰 언어 모델(LLMs)을 활용하여 다국어 상식 추론 데이터셋에서 극도로 제한된 학습 데이터를 보강하는 가능성을 탐구함
* 여러 LLMs, 즉 Dolly-v2, StableVicuna, ChatGPT, 그리고 GPT-4를 사용하여 세 가지 데이터셋인 XCOPA, XWinograd, 그리고 XStoryCloze를 보강함  
** XCOPA (Cross-lingual Choice of Plausible Alternatives): COPA (Choice of Plausible Alternatives)의 확장으로, 상황과 질문이 주어지며, 주어진 두 가지 선택지 중에서 가장 적합한 하나를 선택해야 하는 과제로 XCOPA는 이를 11개의 다양한 언어로 확장하여 다국어 상식 추론 능력을 평가  
** XWinograd (Cross-lingual Winograd Schema Challenge): Winograd Schema Challenge (WSC)는 고전적인 자연어 이해 과제로, 대명사의 모호성을 해결하기 위해 상식과 문맥적 이해가 필요한 문제들을 포함, XWinograd는 WSC를 여러 언어로 확장하여 다국어 상황에서의 언어 이해와 상식 추론 능력을 평가  
** XStoryCloze: StoryCloze 테스트는 일련의 문장으로 이루어진 짧은 이야기가 주어지고, 이야기의 결말로 가장 적합한 문장을 선택하는 과제, 이 테스트는 언어 이해와 논리적 추론 능력을 평가하는 데 사용,  XStoryCloze는 이 과제를 다양한 언어로 확장하여 다국어 이해 및 추론 능력을 평가   
* 영어로 생성된 데이터와 대상 언어로 생성된 데이터를 비교하며, LLM에 의해 생성된 데이터를 포함하는 것이 상당한 정확도 향상을 가져온다는 점에서 전반적인 이점을 보여줌  
* 평가 결과, ChatGPT와 GPT-4와 같은 LLM은 대부분의 언어에서 자연스럽고 일관된 텍스트를 생성하는 데 뛰어난 능력을 보이지만, 타밀어와 같은 특정 언어에서 의미 있는 텍스트를 생성하는 데 어려움 을 겪음  
** 타밀어뿐만 아니라 텔루구어와 칸나다어와 같은 유사 언어에 대한 토크나이저가 제대로 훈련되지 않았기 때문에 이러한 언어에서 사용할 수 없는 생성을 초래하는 것으로 의심됨    
** 토크나이저 학습이 매우 중요함을 의미  

* This paper explores the possibility of enhancing limited training data in multilingual common sense reasoning datasets using large language models
* The authors use various LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4 to augment three datasets like XCOPA, XWinograd, and XStoryCloze  
** XCOPA (Cross-lingual Choice of Plausible Alternatives) is an extension of COPA (Choice of Plausible Alternatives)  
*** It involves choosing the most plausible option out of two given choices based on a situation and a question  
*** XCOPA expands this to 11 different languages to assess multilingual common sense reasoning ability    
** XWinograd (Cross-lingual Winograd Schema Challenge) is based on the Winograd Schema Challenge (WSC)  
*** WSC is a classic natural language understanding task that requires common sense and contextual understanding to resolve pronoun ambiguity  
*** XWinograd extends WSC to several languages to evaluate language understanding and common sense reasoning in a multilingual context  
** XStoryCloze involves selecting the most suitable sentence as the ending of a short story made up of a series of sentences  
*** This task tests language comprehension and logical reasoning abilities   
*** XStoryCloze extends this task to various languages to assess multilingual understanding and reasoning capabilities   
* Comparing data generated in English with data generated in target languages shows that including data created by LLMs can significantly improve accuracy, demonstrating a general benefit    
* The evaluation results show that LLMs like ChatGPT and GPT-4 are excellent at producing natural and consistent text in most languages, but they struggle to generate meaningful text in certain languages, such as Tamil  
** The lack of proper tokenizer training for similar languages like Telugu and Kannada is suspected to cause the inability to generate content in these languages, highlighting the importance of tokenizer training  
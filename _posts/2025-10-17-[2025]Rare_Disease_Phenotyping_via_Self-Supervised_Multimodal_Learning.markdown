---
layout: post
title:  "[2025]Rare Disease Phenotyping via Self-Supervised Multimodal Learning"
date:   2025-10-17 16:39:34 -0000
categories: study
---

{% highlight ruby %}

í•œì¤„ ìš”ì•½: ì´ ë…¼ë¬¸ì—ì„œëŠ” í¬ê·€ ì§ˆí™˜ì˜ í‘œí˜„í˜•ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ìœ ì „ì, 3D ì´ë¯¸ì§•, ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°ë¥¼ í†µí•©í•œ ë©€í‹°ëª¨ë‹¬ ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.


ì§§ì€ ìš”ì•½(Abstract) :


ì´ ë…¼ë¬¸ì—ì„œëŠ” í¬ê·€ ìœ ì „ ì§ˆí™˜ í™˜ìë“¤ì´ ê²ªëŠ” ì§„ë‹¨ ê³¼ì •ì˜ ë³µì¡ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ì „ì²´ ì—‘ì†œ ì‹œí€€ì‹±(Whole Exome Sequencing, WES), 3D MRI ì˜ìƒ, ê·¸ë¦¬ê³  ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ í¬ê·€ ì§ˆí™˜ì˜ í‘œí˜„í˜•ì„ íŒŒì•…í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°œë³„ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ê³¼ëŠ” ë‹¬ë¦¬, ì´ ì ‘ê·¼ë²•ì€ ì„¸ ê°€ì§€ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¨ì¼ ë³€í™˜ê¸° ê¸°ë°˜ì˜ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í™˜ìë³„ ë‹¤ì¤‘ ëª¨ë‹¬ íŠ¹ì§•ì„ ì •ë ¬í•˜ê¸° ìœ„í•´ êµì°¨ ëª¨ë‹¬ ëŒ€ì¡° ì†ì‹¤ì„ ìµœì í™”í•˜ê³ , ìƒë¬¼ ì˜í•™ ì§€ì‹ ê·¸ë˜í”„ì— ê¸°ë°˜í•œ ì˜¨í†¨ë¡œì§€ ì†ì‹¤ì„ í†µí•´ í‘œí˜„ì„ ê°•í™”í•©ë‹ˆë‹¤. ì œë¡œìƒ· ê²€ìƒ‰ ë° ë¶„ë¥˜ ì‘ì—…ì—ì„œ, ìš°ë¦¬ì˜ ëª¨ë¸ì€ ìµœì‹  ì „ë¬¸í™”ëœ ê¸°ì¤€ì„ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì´ì „ ë°©ë²•ë³´ë‹¤ +5.6% AUROC í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ì—¬ë¡œëŠ” (i) ìœ ì „ì  ë³€ì´, 3D ì´ë¯¸ì§€, ì‹œê°„ ì‹œê³„ì—´ ì‹ í˜¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” í†µí•© ë‹¤ì¤‘ ëª¨ë‹¬ ë³€í™˜ê¸°, (ii) í™˜ì ë°ì´í„°ë¥¼ ì¸ê°„ í‘œí˜„í˜• ì˜¨í†¨ë¡œì§€(HPO) ìš©ì–´ì™€ í•¨ê»˜ ì„ë² ë”©í•˜ëŠ” ì˜¨í†¨ë¡œì§€ ì •ë ¬ ì ì¬ ê³µê°„, (iii) ì œë¡œìƒ· í¬ê·€ ì§ˆí™˜ ê²€ìƒ‰ì—ì„œ í–¥ìƒëœ ì¬í˜„ìœ¨ê³¼ ì„ìƒì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ì£¼ì˜ ê¸°ë°˜ ì„¤ëª…ì„ ë³´ì—¬ì£¼ëŠ” ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì´ í¬í•¨ë©ë‹ˆë‹¤.



This paper proposes a multimodal self-supervised learning model to phenotype rare diseases by integrating whole exome sequencing (WES), volumetric MRI, and wearable sensor data, addressing the complexity of the diagnostic process faced by patients with rare genetic disorders. Unlike traditional siloed machine learning pipelines, this approach employs a single transformer-based encoder-decoder architecture capable of handling all three data modalities. We optimize a cross-modal contrastive loss to align patient-specific multimodal features, alongside an ontology-based loss that grounds representations in biomedical knowledge graphs. Across zero-shot retrieval and classification tasks, our model outperforms state-of-the-art specialized baselines, achieving a +5.6% AUROC gain over the best prior method. Key contributions include: (i) a unified multimodal transformer that processes genomic variants, 3D images, and time-series signals, (ii) an ontology-aligned latent space that embeds patient data alongside Human Phenotype Ontology (HPO) terms, and (iii) extensive experiments demonstrating improved recall in zero-shot rare disease retrieval and clinically meaningful attention-based explanations.


* Useful sentences :


{% endhighlight %}

<br/>

[Paper link]()
[~~Lecture link~~]()

<br/>

# ë‹¨ì–´ì •ë¦¬
*


<br/>
# Methodology



ì´ ë…¼ë¬¸ì—ì„œëŠ” í¬ê·€ ì§ˆë³‘ì˜ í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ í†µí•©ëœ ë‹¤ì¤‘ ëª¨ë‹¬ ìê¸° ì§€ë„ í•™ìŠµ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ìœ ì „ì ë³€ì´, 3D ì´ë¯¸ì§•, ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì„¸ ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í†µí•©ëœ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

#### ëª¨ë¸ ì•„í‚¤í…ì²˜
ëª¨ë¸ì€ ì„¸ ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°(ìœ ì „ì²´, ì´ë¯¸ì§•, ìƒì²´ ì‹ í˜¸)ì— ëŒ€í•´ ê°ê°ì˜ ì¸ì½”ë”ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì´ ì¸ì½”ë”ë“¤ì€ ê³µí†µì˜ ì ì¬ ê³µê°„ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤. ê° ì¸ì½”ë”ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

1. **ìœ ì „ì²´ ì¸ì½”ë” (ğ¸ğ‘”)**: ì´ ì¸ì½”ë”ëŠ” ì „ì²´ ì—‘ì†œ ì‹œí€€ì‹±(Whole Exome Sequencing, WES)ì—ì„œ ìƒì„±ëœ ë³€ì´ í˜¸ì¶œ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. BERT ìŠ¤íƒ€ì¼ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ì´ í† í°ì„ ì¸ì½”ë”©í•˜ë©°, í•˜í”Œë¡œíƒ€ì… ë§ˆìŠ¤í‚¹ ê¸°ë²•ì„ ë„ì…í•˜ì—¬ ì—°ê´€ëœ ë³€ì´ ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.

2. **ì´ë¯¸ì§• ì¸ì½”ë” (ğ¸ğ‘£)**: ì´ ì¸ì½”ë”ëŠ” T1 ê°€ì¤‘ì¹˜ ë‡Œ MRIì™€ ê°™ì€ 3D ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ì§€ê³ , ê° íŒ¨ì¹˜ëŠ” 3D ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì— ì…ë ¥ë©ë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ëœ ë³¼ë¥¨ ëª¨ë¸ë§ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë¶€ íŒ¨ì¹˜ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê³  ì´ë¥¼ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

3. **ìƒì²´ ì‹ í˜¸ ì¸ì½”ë” (ğ¸ğ‘ )**: ì´ ì¸ì½”ë”ëŠ” ë‹¤ë³€ëŸ‰ ìƒì²´ ì‹ í˜¸(ì˜ˆ: ECG, PPG)ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‹ í˜¸ëŠ” 1D CNNì„ í†µí•´ ì§€ì—­ì  íŒ¨í„´ì„ ì¶”ì¶œí•œ í›„, íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í†µí•´ ì¥ê¸° ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤.

#### í•™ìŠµ ê¸°ë²•
ëª¨ë¸ì€ ë‘ ê°€ì§€ ì£¼ìš” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤:

1. **êµì°¨ ëª¨ë‹¬ ëŒ€ì¡° ì†ì‹¤ (LMMCL)**: ì´ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ë™ì¼í•œ í™˜ìì˜ ì„ë² ë”©ì´ ìœ ì‚¬í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ê³µí†µëœ ì§ˆë³‘ ê´€ë ¨ ì‹ í˜¸ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

2. **ì˜¨í†¨ë¡œì§€ ì •ë ¬ ì†ì‹¤ (LKG)**: ì´ ì†ì‹¤ í•¨ìˆ˜ëŠ” í™˜ìì˜ ì ì¬ í‘œí˜„ì´ ì„ìƒì  í‘œí˜„ê³¼ ì¼ì¹˜í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. Human Phenotype Ontology (HPO)ì™€ Unified Medical Language System (UMLS)ì„ í™œìš©í•˜ì—¬ í™˜ì ì„ë² ë”©ì„ ì „ë¬¸ê°€ê°€ ì •ì˜í•œ í‘œí˜„ê³¼ ì •ë ¬í•©ë‹ˆë‹¤.

#### ë°ì´í„° ì²˜ë¦¬
ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤. ìœ ì „ì²´ ë°ì´í„°ëŠ” TCGA-GBM ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê³ , ì´ë¯¸ì§• ë°ì´í„°ëŠ” ADNI ë°ì´í„°ì…‹ì„, ìƒì²´ ì‹ í˜¸ ë°ì´í„°ëŠ” UK Biobankì˜ ì›¨ì–´ëŸ¬ë¸” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì€ ì‚¬ì „ ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì³ ëª¨ë¸ì— ì…ë ¥ë©ë‹ˆë‹¤.

ì´ëŸ¬í•œ í†µí•©ëœ ì ‘ê·¼ ë°©ì‹ì€ í¬ê·€ ì§ˆë³‘ì˜ ì§„ë‹¨ì„ ìœ„í•œ AI ê¸°ë°˜ì˜ ì°¨ë³„ ì§„ë‹¨ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, í™˜ìì—ê²Œ ë³´ë‹¤ ì‹ ì†í•˜ê³  ì •í™•í•œ ì¹˜ë£Œë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

---




This paper proposes a unified multimodal self-supervised learning approach for rare disease phenotyping. The model utilizes an integrated encoder-decoder architecture capable of processing three different data modalities: genomic variants, 3D imaging, and wearable sensor data.

#### Model Architecture
The model consists of three modality-specific encoders, each corresponding to genomics, imaging, and biosignals, which map to a common latent space. Each encoder is structured as follows:

1. **Genomic Encoder (ğ¸ğ‘”)**: This encoder takes variant call data from Whole Exome Sequencing (WES) as input. It employs a BERT-style transformer to encode variant tokens and introduces haplotype masking techniques to preserve information about linked variants.

2. **Imaging Encoder (ğ¸ğ‘£)**: This encoder processes 3D images such as T1-weighted brain MRIs. The images are divided into patches, and each patch is fed into a 3D vision transformer. A masked volume modeling technique is used, where some patches are masked, and the model is tasked with reconstructing them.

3. **Biosignal Encoder (ğ¸ğ‘ )**: This encoder handles multivariate biosignals (e.g., ECG, PPG). The signals are first processed through a 1D CNN to extract local patterns, followed by a transformer to model long-range dependencies.

#### Training Techniques
The model is trained using two main loss functions:

1. **Cross-Modal Contrastive Loss (LMMCL)**: This loss function encourages the embeddings of the same patient from different modalities to be similar. This allows the model to learn common disease-relevant signals across various modalities.

2. **Ontology Alignment Loss (LKG)**: This loss function encourages the model's latent representation to align with clinical phenotypes. It leverages the Human Phenotype Ontology (HPO) and the Unified Medical Language System (UMLS) to align patient embeddings with expert-defined phenotypic profiles.

#### Data Processing
The model is trained on various datasets. Genomic data is sourced from the TCGA-GBM dataset, imaging data from the ADNI dataset, and biosignal data from the UK Biobank wearable dataset. Each dataset undergoes preprocessing before being input into the model.

This integrated approach enables AI-driven differential diagnosis for rare diseases, potentially providing patients with timely and accurate treatments.


<br/>
# Results



ì´ ì—°êµ¬ì—ì„œëŠ” í¬ê·€ ì§ˆí™˜ì˜ í‘œí˜„ í•™ìŠµì„ ìœ„í•œ í†µí•© ë‹¤ì¤‘ ëª¨ë‹¬ ìê¸° ê°ë… í•™ìŠµ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°, ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ê²½ìŸ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤. ì£¼ìš” ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

1. **í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹**:
   - **ìœ ì „ì²´ ë°ì´í„°**: TCGA-GBM ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 250ëª…ì˜ ì‹ ê²½êµì¢… í™˜ìì™€ 50ëª…ì˜ ì •ìƒ ëŒ€ì¡°êµ°ì„ í¬í•¨í•˜ì˜€ë‹¤.
   - **ì‹ ê²½ì˜ìƒ ë°ì´í„°**: ADNI ë°ì´í„°ì…‹ì—ì„œ ì•Œì¸ í•˜ì´ë¨¸ í™˜ì 200ëª…ê³¼ ì—°ë ¹ì´ ì¼ì¹˜í•˜ëŠ” ëŒ€ì¡°êµ° 200ëª…ì˜ T1 ê°€ì¤‘ì¹˜ ë‡Œ MRIë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.
   - **ìƒì²´ ì‹ í˜¸ ë°ì´í„°**: UK Biobankì—ì„œ 100ëª…ì˜ í”¼í—˜ìë¡œë¶€í„° 500ì‹œê°„ì˜ PPG ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì˜€ë‹¤.

2. **ê²½ìŸ ëª¨ë¸**:
   - **ElasticNet (ì¡°ê¸° ìœµí•©)**: ìœ ì „ì²´, ì˜ìƒ ë° ì‹ í˜¸ ë°ì´í„°ë¥¼ ìˆ˜ì‘ì—…ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤.
   - **ë‹¨ì¼ ëª¨ë‹¬ CNN**: ê° ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•´ ë³„ë„ë¡œ í›ˆë ¨ëœ ëª¨ë¸ë¡œ, ìœ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ MLP, MRIì— ëŒ€í•´ 3D ResNet, ì‹ í˜¸ì— ëŒ€í•´ 1D CNNì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
   - **BioGPT-X**: ìƒë¬¼ ì˜í•™ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì‚¬ì „ í›ˆë ¨ëœ ê°•ë ¥í•œ ë³€í™˜ê¸° ëª¨ë¸ë¡œ, ìœ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •í•˜ì˜€ë‹¤.
   - **MedCLIP**: ì˜ë£Œ ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ì™€ ëŒ€ì¡°í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ë¡œ, ë‘ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤.

3. **ë©”íŠ¸ë¦­**:
   - **AUROC (Receiver Operating Characteristic Curveì˜ ë©´ì )**: ê° ëª¨ë¸ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤.
   - **F1 ì ìˆ˜**: ëª¨ë¸ì˜ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ì˜€ë‹¤.
   - **Recall@K**: ì£¼ì–´ì§„ K ê°’ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤.
   - **Mean Reciprocal Rank (MRR)**: ê²€ìƒ‰ëœ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤.

4. **ë¹„êµ ê²°ê³¼**:
   - **ìœ ì „ì²´ ë°ì´í„° (GBM vs. ì •ìƒ)**: ì œì•ˆëœ ëª¨ë¸ì€ AUROC 0.93Â±0.02ë¥¼ ê¸°ë¡í•˜ì—¬ BioGPT-Xì˜ 0.88Â±0.03ì„ ì´ˆê³¼í•˜ì˜€ë‹¤. ì´ëŠ” 5%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‚˜íƒ€ë‚¸ë‹¤.
   - **ì‹ ê²½ì˜ìƒ ë°ì´í„° (AD vs. ì •ìƒ)**: ì œì•ˆëœ ëª¨ë¸ì€ AUROC 0.89Â±0.01ì„ ê¸°ë¡í•˜ì—¬ ResNet-18ì˜ 0.81Â±0.02ë¥¼ ì´ˆê³¼í•˜ì˜€ë‹¤.
   - **ìƒì²´ ì‹ í˜¸ ë°ì´í„° (AF vs. ì •ìƒ)**: ì œì•ˆëœ ëª¨ë¸ì€ AUROC 0.94Â±0.01ì„ ê¸°ë¡í•˜ì—¬ ë‹¨ì¼ ëª¨ë‹¬ CNNì˜ 0.87Â±0.02ë¥¼ ì´ˆê³¼í•˜ì˜€ë‹¤.
   - **í¬ê·€ ì§ˆí™˜ ê²€ìƒ‰**: ì œì•ˆëœ ëª¨ë¸ì€ 30ê°œì˜ ì‹œë®¬ë ˆì´ì…˜ëœ í™˜ì ë°ì´í„°ì—ì„œ 93.3%ì˜ ì •í™•ë„ë¡œ ì˜¬ë°”ë¥¸ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•˜ì˜€ë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ê²½ìŸ ëª¨ë¸ë“¤ì´ ìˆ˜í–‰í•  ìˆ˜ ì—†ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì˜€ë‹¤.

ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì œì•ˆëœ ëª¨ë¸ì´ ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ í¬ê·€ ì§ˆí™˜ì˜ ì§„ë‹¨ì„ ì§€ì›í•˜ëŠ” ë° ìˆì–´ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤€ë‹¤. íŠ¹íˆ, ì œì•ˆëœ ëª¨ë¸ì€ ê¸°ì¡´ì˜ ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì´ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµì˜ ì´ì ì„ ì˜ ë‚˜íƒ€ë‚¸ë‹¤.

---




This study proposed a unified multimodal self-supervised learning approach for rare disease representation learning and evaluated its performance against competitive models across various test datasets. The key results are as follows:

1. **Test Datasets**:
   - **Genomic Data**: The TCGA-GBM dataset was used, which includes 250 glioblastoma patients and 50 normal controls.
   - **Neuroimaging Data**: The ADNI dataset was utilized, comprising T1-weighted brain MRIs from 200 Alzheimerâ€™s patients and 200 age-matched controls.
   - **Biosignal Data**: A subset of the UK Biobank was used, collecting 500 hours of PPG data from 100 subjects.

2. **Competitive Models**:
   - **ElasticNet (Early Fusion)**: A logistic regression model that combined handcrafted features from genomic, imaging, and signal data.
   - **Single-Modality CNNs**: Models trained separately for each modality, including MLP for genomic data, 3D ResNet for MRI, and 1D CNN for signals.
   - **BioGPT-X**: A strong transformer model pre-trained on biomedical text, fine-tuned on genomic data.
   - **MedCLIP**: A vision-language model that aligns medical images with text, adapted for the tri-modal problem.

3. **Metrics**:
   - **AUROC (Area Under the Receiver Operating Characteristic Curve)**: Used to evaluate the classification performance of each model.
   - **F1 Score**: A comprehensive measure of the model's precision and recall.
   - **Recall@K**: Evaluated the performance of retrieving correct cases for a given K value.
   - **Mean Reciprocal Rank (MRR)**: Used to assess the ranking of retrieved results.

4. **Comparison Results**:
   - **Genomic Data (GBM vs. Normal)**: The proposed model achieved an AUROC of 0.93Â±0.02, surpassing BioGPT-X's 0.88Â±0.03, indicating a 5% performance improvement.
   - **Neuroimaging Data (AD vs. Normal)**: The proposed model recorded an AUROC of 0.89Â±0.01, significantly higher than ResNet-18's 0.81Â±0.02.
   - **Biosignal Data (AF vs. Normal)**: The proposed model achieved an AUROC of 0.94Â±0.01, outperforming the single-modality CNN's 0.87Â±0.02.
   - **Rare Disease Retrieval**: The proposed model correctly retrieved matching cases in 93.3% of instances from a simulated dataset of 30 patients, demonstrating capabilities for cross-modal retrieval that existing models could not achieve.

These results indicate that the proposed model effectively supports the diagnosis of rare diseases by integrating multimodal data. Notably, the model outperformed existing single-modality models, highlighting the advantages of multimodal learning.


<br/>
# ì˜ˆì œ



ì´ ë…¼ë¬¸ì—ì„œëŠ” í¬ê·€ ì§ˆë³‘ì˜ í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ëª¨ë‹¬ ìê¸° ì§€ë„ í•™ìŠµ(self-supervised learning) ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ìœ ì „ì²´ ë°ì´í„°(Whole Exome Sequencing, WES), 3D MRI ì´ë¯¸ì§€, ê·¸ë¦¬ê³  ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°(ì˜ˆ: ECG, PPG)ë¥¼ í†µí•©í•˜ì—¬ í™˜ìì˜ í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤. 

#### íŠ¸ë ˆì´ë‹ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°

1. **íŠ¸ë ˆì´ë‹ ë°ì´í„°**:
   - **ìœ ì „ì²´ ë°ì´í„°**: TCGA-GBM ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 250ëª…ì˜ ì‹ ê²½êµì¢…(GBM) í™˜ìì™€ 50ëª…ì˜ ì •ìƒ ëŒ€ì¡°êµ°ì˜ WES ë³€ì´ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ê° ë³€ì´ëŠ” VCF í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ë©°, ë‹¨ì¼ ë‰´í´ë ˆì˜¤íƒ€ì´ë“œ ë³€ì´(SNV)ì™€ ì‘ì€ ì¸ë¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
   - **MRI ë°ì´í„°**: ADNI ë°ì´í„°ì…‹ì—ì„œ 200ëª…ì˜ ì•Œì¸ í•˜ì´ë¨¸ í™˜ìì™€ 200ëª…ì˜ ì—°ë ¹ëŒ€ê°€ ì¼ì¹˜í•˜ëŠ” ëŒ€ì¡°êµ°ì˜ T1 ê°€ì¤‘ì¹˜ ë‡Œ MRI ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ì´ë¯¸ì§€ëŠ” 1mmÂ³ì˜ ë“±ë°©ì„± ë³µì…€ë¡œ ì¬ìƒ˜í”Œë§ë˜ê³ , ë‘ê°œê³¨ì´ ì œê±°ë˜ë©°, ê°•ë„ ì •ê·œí™”ê°€ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
   - **ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°**: UK Biobankì—ì„œ 100ëª…ì˜ í”¼í—˜ìë¡œë¶€í„° ìˆ˜ì§‘ëœ 500ì‹œê°„ì˜ PPG ì‹ í˜¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ì‹¬ì¥ ë¶€ì •ë§¥ ì§„ë‹¨ì„ ë°›ì€ í”¼í—˜ìì™€ ê±´ê°•í•œ í”¼í—˜ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

2. **í…ŒìŠ¤íŠ¸ ë°ì´í„°**:
   - **í¬ê·€ ì§ˆë³‘ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°**: 30ëª…ì˜ "ì‹œë®¬ë ˆì´ì…˜ í™˜ì"ë¥¼ êµ¬ì„±í•˜ì—¬ ê° í™˜ìì— ëŒ€í•´ 10ê°œì˜ TCGA-GBM ìœ ì „ì²´, 10ê°œì˜ ADNI MRI ìŠ¤ìº”, 10ê°œì˜ UK Biobank ì‹ í˜¸ë¥¼ ë§¤ì¹­í•˜ì—¬ 10ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í¬ê·€ ì§ˆë³‘ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ëª¨ë¸ì˜ ì œë¡œìƒ·(zero-shot) ê²€ìƒ‰ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

#### êµ¬ì²´ì ì¸ í…ŒìŠ¤í¬

- **ì œë¡œìƒ· í¬ê·€ ì§ˆë³‘ ê²€ìƒ‰**: ì£¼ì–´ì§„ ìœ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ìœ ì „ì ë³€ì´ë¥¼ ê°€ì§„ í™˜ìì˜ ìœ ì „ì²´ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì œê³µí•˜ë©´, ëª¨ë¸ì€ í•´ë‹¹ í™˜ìì™€ ìœ ì‚¬í•œ MRI ë˜ëŠ” PPG ê¸°ë¡ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
- **ë¶„ë¥˜ ì‘ì—…**: ëª¨ë¸ì€ í•™ìŠµëœ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì§ˆë³‘(ì˜ˆ: GBM vs. ì •ìƒ, AD vs. ì •ìƒ, ì‹¬ë°©ì„¸ë™ vs. ì •ìƒ)ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” AUROC(Receiver Operating Characteristic Area Under the Curve)ì™€ F1 ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ, ëª¨ë¸ì€ ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ í¬ê·€ ì§ˆë³‘ì˜ ì§„ë‹¨ì„ ì§€ì›í•˜ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---




This paper proposes a multimodal self-supervised learning approach for phenotyping rare diseases. The model integrates genomic data (Whole Exome Sequencing, WES), 3D MRI images, and wearable sensor data (e.g., ECG, PPG) to learn patient representations.

#### Training Data and Test Data

1. **Training Data**:
   - **Genomic Data**: The TCGA-GBM dataset is used, which includes WES variant data from 250 glioblastoma (GBM) patients and 50 normal controls. Each variant is provided in VCF format and includes single nucleotide variants (SNVs) and small indels.
   - **MRI Data**: The ADNI dataset provides T1-weighted brain MRI images from 200 Alzheimer's patients and 200 age-matched controls. Each image is resampled to 1mmÂ³ isotropic voxels, skull-stripped, and intensity-normalized.
   - **Wearable Sensor Data**: The UK Biobank dataset includes 500 hours of PPG signal data collected from 100 subjects, some diagnosed with cardiac arrhythmias and others healthy.

2. **Test Data**:
   - **Rare Disease Simulation Data**: A set of 30 "simulated patients" is constructed, pairing 10 TCGA-GBM genomes, 10 ADNI MRI scans, and 10 UK Biobank signals to represent 10 distinct rare conditions. This data is used to evaluate the model's zero-shot retrieval performance.

#### Specific Tasks

- **Zero-Shot Rare Disease Retrieval**: This task involves retrieving the most similar case given a genomic input. For example, when provided with the genomic data of a patient with a specific genetic variant, the model retrieves the corresponding MRI or PPG record of that patient.
- **Classification Tasks**: The model performs classification tasks using the learned embeddings to distinguish between specific diseases (e.g., GBM vs. control, AD vs. control, atrial fibrillation vs. normal). Performance is evaluated using metrics such as AUROC (Area Under the Receiver Operating Characteristic Curve) and F1 score.

In this way, the model aims to contribute to the diagnosis of rare diseases by integrating data from various modalities.

<br/>
# ìš”ì•½


ì´ ë…¼ë¬¸ì—ì„œëŠ” í¬ê·€ ì§ˆí™˜ì˜ í‘œí˜„í˜•ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ìœ ì „ì, 3D ì´ë¯¸ì§•, ì›¨ì–´ëŸ¬ë¸” ì„¼ì„œ ë°ì´í„°ë¥¼ í†µí•©í•œ ë©€í‹°ëª¨ë‹¬ ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì´ ëª¨ë¸ì€ ì œë¡œìƒ· í™˜ê²½ì—ì„œ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ ë°©ë²•ë³´ë‹¤ 5% ì´ìƒì˜ AUROC í–¥ìƒì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ë°ì´í„° ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì§„ë‹¨ ì •í™•ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ìœ ì „ì ë³€ì´ë¥¼ ê°€ì§„ í™˜ìì˜ MRI ì´ë¯¸ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë§¤ì¹­í•˜ì—¬ ì§ˆë³‘ì˜ í‘œí˜„í˜•ì„ íŒŒì•…í•˜ëŠ” ë° ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.

---

In this paper, a multimodal self-supervised learning model is proposed to phenotype rare diseases by integrating genomic, 3D imaging, and wearable sensor data. Experimental results show that this model achieves over a 5% AUROC improvement compared to state-of-the-art methods in a zero-shot setting, effectively leveraging correlations across different data modalities to enhance diagnostic accuracy. For instance, it successfully matched MRI images of patients with specific genetic variants, aiding in the identification of disease phenotypes.

<br/>
# ê¸°íƒ€



1. **ë‹¤ì´ì–´ê·¸ë¨ ë° í”¼ê·œì–´**
   - **Figure 1**: í†µí•©ëœ ìê¸° ì§€ë„ í•™ìŠµ ì•„í‚¤í…ì²˜ì˜ ê°œìš”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ëª¨ë‹¬ë¦¬í‹°(ìœ ì „ì²´, ì˜ìƒ, ì‹ í˜¸)ì— ëŒ€í•œ ì¸ì½”ë”ê°€ ê³µìœ  ì ì¬ ê³µê°„ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ê³¼ì •ì„ ì‹œê°í™”í•˜ì—¬, ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì´ ì–´ë–»ê²Œ í†µí•©ë˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
   - **Figure 2**: t-SNE ì‹œê°í™”ë¡œ, í™˜ì ì„ë² ë”©ì´ ì§ˆë³‘ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ë§ë˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„° ì†ŒìŠ¤ê°€ ì•„ë‹Œ ì§ˆë³‘ ìƒíƒœì— ë”°ë¼ ì„ë² ë”©ì„ ê·¸ë£¹í™”í•˜ê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
   - **Figure 3**: ìˆ˜ì‹ ì ì¡°ì‘ íŠ¹ì„±(ROC) ê³¡ì„ ì„ í†µí•´ ëª¨ë¸ê³¼ ë² ì´ìŠ¤ë¼ì¸ ê°„ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ì‹œê°ì ìœ¼ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ëª¨ë¸ì´ ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ë†’ì€ AUROCë¥¼ ë‹¬ì„±í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
   - **Figure 4**: ì‹ ë¢°ì„± ê³¡ì„ ìœ¼ë¡œ, ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ ë¹ˆë„ì™€ ì–¼ë§ˆë‚˜ ì˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì˜ ë³´ì •ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
   - **Figure 6**: ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ê±°ë¦¬ì™€ ì„ë² ë”© ê³µê°„ ê±°ë¦¬ ê°„ì˜ ê´€ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì˜¨í†¨ë¡œì§€ì˜ ì˜ë¯¸ì  ê·¼ì ‘ì„±ì´ í•™ìŠµëœ í‘œí˜„ì— ë°˜ì˜ë˜ê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

2. **í…Œì´ë¸”**
   - **Table 1**: í‰ê°€ ë°ì´í„°ì…‹ê³¼ ê° ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ì„ ìš”ì•½í•©ë‹ˆë‹¤. ê° ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ê¸ì • í´ë˜ìŠ¤ì™€ ì‘ì—…ì„ ëª…ì‹œí•˜ì—¬, ëª¨ë¸ì˜ í‰ê°€ ê¸°ì¤€ì„ ëª…í™•íˆ í•©ë‹ˆë‹¤.
   - **Table 2**: ëª¨ë¸ ì„±ëŠ¥ì„ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í†µí•©ëœ ë‹¤ì¤‘ ëª¨ë‹¬ ëª¨ë¸ì´ ê±°ì˜ ëª¨ë“  ë©”íŠ¸ë¦­ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆìŒì„ ê°•ì¡°í•©ë‹ˆë‹¤.
   - **Table 3**: ê° êµ¬ì„± ìš”ì†Œ ë˜ëŠ” ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì œê±°í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ëŠ” ì ˆë‹¨ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ëŠ” ê° êµ¬ì„± ìš”ì†Œì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

3. **ì–´íœë”•ìŠ¤**
   - **Appendix A**: ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ê·¸ ê°’ì„ ë‚˜ì—´í•˜ì—¬, ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©ëœ ì„¤ì •ì„ ëª…í™•íˆ í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì¬í˜„ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.




1. **Diagrams and Figures**
   - **Figure 1**: Provides an overview of the unified self-supervised learning architecture, illustrating how modality-specific encoders converge into a shared latent space. This visualizes the integration of diverse data types.
   - **Figure 2**: Shows a t-SNE visualization of patient embeddings, indicating that the model clusters embeddings primarily by disease rather than by data source, demonstrating effective cross-modal alignment.
   - **Figure 3**: Displays receiver operating characteristic (ROC) curves comparing the model's performance against baselines across modalities, highlighting the model's superior AUROC scores.
   - **Figure 4**: Reliability curves illustrate how well the model's predicted probabilities align with observed frequencies, indicating good calibration of the model's predictions.
   - **Figure 6**: Shows the relationship between ontology graph distance and embedding space distance, suggesting that semantic proximity in the ontology is reflected in the learned representations.

2. **Tables**
   - **Table 1**: Summarizes the evaluation datasets and their characteristics, clarifying the positive classes and tasks for each modality, which helps define the evaluation criteria for the model.
   - **Table 2**: Presents performance metrics of the model compared to baselines, emphasizing that the unified multimodal model achieves the highest scores across nearly all metrics.
   - **Table 3**: Provides results from ablation studies that quantify the contribution of each component or modality, underscoring the importance of each part of the model.

3. **Appendix**
   - **Appendix A**: Lists key hyperparameters and their values, clarifying the settings used for model training, which contributes to the reproducibility of the model.

<br/>
# refer format:
### BibTeX í˜•ì‹

```bibtex
@inproceedings{Uppalapati2025,
  author = {Khartik Uppalapati and Bora Yimenicioglu and Shakeel Abdulkareem and Adan Eftekhari},
  title = {Rare Disease Phenotyping via Self-Supervised Multimodal Learning},
  booktitle = {Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics (BCB '25)},
  year = {2025},
  month = {October},
  location = {Philadelphia, PA, USA},
  publisher = {ACM},
  pages = {1--12},
  doi = {10.1145/3765612.3767304}
}
```

### ì‹œì¹´ê³  ìŠ¤íƒ€ì¼

Uppalapati, Khartik, Bora Yimenicioglu, Shakeel Abdulkareem, and Adan Eftekhari. 2025. "Rare Disease Phenotyping via Self-Supervised Multimodal Learning." In *Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics (BCB '25)*, 1-12. Philadelphia, PA, USA: ACM. https://doi.org/10.1145/3765612.3767304.

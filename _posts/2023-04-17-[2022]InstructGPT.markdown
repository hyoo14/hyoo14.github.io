---
layout: post
title:  "[2022]Training language models to follow instructions with human feedback"
date:   2023-04-17 03:59:20 +0900
categories: study
---






{% highlight ruby %}


짧은 요약(Abstract) :    
* uset intent 파악위해 사람의 피드백으로 파인튜닝(F-T, Fine Tuning)




{% endhighlight %}  

<br/>


[Paper with my notes](https://drive.google.com/drive/folders/1VRg6Jnv2gBQmxx4kE__NQElIreVw48fy?usp=sharing)  


[~~Lecture link~~]()  

<br/>

# 단어정리  
* hedging: 대비책
* reliability: 믿을 수 있는, 믿을 만한  
* mitigate: 완화시키다, 경감시키다  
* bandit: 강도  
* bandit environment: Bandit environment는 강화 학습에서 사용되는 용어 중 하나로, 에이전트가 행동을 선택할 수 있는 환경  
* preformance regression: 성능이 이전 버전에 비해 감소하는 현상  
* likert scale: 좋아요 점수 스케일 같이 일반적으로 숫자로 표현되는 등간 척도로 구성되어 있으며, 보통 5점 척도나 7점 척도를 사용   
* held-out labels: Held-out labels은 검증 데이터 또는 테스트 데이터에 속한 레이블(정답)을 의미
* overly hedge: 과도한 리스크 관리  
* epistemic humility: 지식에 대한 겸손함이라는 의미로, 인간의 지식과 이해력에 대한 한계를 인식하고 수용하는 태도  





<br/>

# 1 Introduction  
* LM이 의도와 다른 행동 보임  
** next predict라서 의도파악과는 다름  
** 본 논문에서 F-T + RL(사람 선호 맞출 경우 보상하는 강화학습) 제안  
** RM(Reward Model)학습, RM으로부터 F-T, 이 때, PPO 알고리즘 사용  


### Labelers significantly prefer InstructGPT outputs over outputs from GPT-3  
* 레이블을 다는 사람들은 InstructGPT의 결과를 선호함  
** 1.3B param의 InstructGPT를 175B param의 GPT3보다 더 선호  
** 구조는 같되, 사람 data로 학습  
** 선호도 InstructGPT 85 +- 3%, GPT3 71+-4%  


### InstructGPT models show improvements in truthfulness over GPT-3  
* 없던 정보 생성 줄어듦 41%->21%  


### InstructGPT shows small improvements in toxicity over GPT-3, but not bias  
* 유동성 25% 줄임  


### We can minimize performance regression on public NLP datasets by modifying our RLHF fine-tuning procedure.  
* performance regression ( preformance regression: 성능이 이전 버전에 비해 감소하는 현상  ) 줄임  
** RLHF 파인튜닝 -> PPO update  


### Our models generalize to the preferences of "held-out" labelers that did not produce any training data.  
(held-out labels: Held-out labels은 검증 데이터 또는 테스트 데이터에 속한 레이블(정답)을 의미)  
* InstructGPT 레이블 성능 확인  
** 더 연구 필요  


### Public NLP datasets are not reflective of how our language models are used.  
* public data 본 LM 잘 반영 못 함  
** InstructGPT 선호  


### InstructGPT models show promising generalization to instructions outside of the RLHF fine-tuning distribution  
* 파인튜닝분포서 일반화 가능  
** 코드 QA, 다른 언어도 가능  


### InstructGPT still makes simple mistakes.  
** 아직 완전한 모델은 아님  
** 긴 hedging 문장(보험? 대비용? 문장)  
** 잘못된 전제 문제  
** 안정성, 활용성관련 전진 필요  
<br/>


# 2 Related work  
### Research on alignment and learning from human feedbak.  
* 사람의 의도, 사람의 피드백 강화학습으로 반영하는 배치기술 기반(RLHF-Research and Learning from Human Feedback)  
[Figure 2] 본 모델 3steps  
(1) SFT(Supervised Fine Tuning)  
(2) RM(Reward Model)  
(3) PPO(RL, Proximal Policy Optimization)  
** RL 피드백 : award  
*** 로봇, 아타리 게임의 강화학습에서 사용되던 것  
*** 파인튜닝에 이용(대화, 번역, 의미파싱, 스로리생성, 리뷰생성, 근거추출)  
*** 언어 어시스턴트  


### Training language models to follow instructions.  
** cross domain generalization LM과 연관  
*** 다른 도메인 데이터로 각각 train/test  


### Mitigating the harms of language models.  
** 유해요소 from LM 경감   
*** 파인튜닝 small value target 데이터  
*** 프리트레이닝 데이터 필터링  
<br/>

# 3 Models and experimental details  
* 방법론, 실험 상세  
## 3.1 High-level methodology  
* 고차원 방법론  
** 스타일 지속& 요약 도메인서 적용  
*** PLM부터 시작  
*** prompt 분포로 aligned 결과 생성  
*** 사람이 레이블  
*** 아래 3step 따름  


### Step 1: Collect demonstration data, and train a supervised policy.  
* 시연, 검증 프롬프트로 파인튜닝  


### Step 2: Collect comparison data, and train a reward model.  
* 결과와 대조 입력으로 RM 학습  


### Step 3: Optimize a policy against the reward model using PPO.  
* RM 결과를 스칼라 보상으로 사용, PPO로 파인튜닝  


** Step 2, 3 반복  


## 3.2 Dataset  
* 데이터셋  
** 상업 AI  
** 사람 프롬프트  
(1) SFT 시연 13K  
(2) RM 시연 대조 33K  
(3) PPO 31K  


## 3.3 Human data collection  
* 사람의 자료 수집  
** 40명 계약  
*** 이전보다 넓은 범위의 task(논란적, 민감 토픽)  
*** 계약자는 harmful 잘 인지  
*** 동의율(학습: 72.6+-1.6%, 선별: 77.3+-1.3%, 요약: 73+-4%)  


## 3.4 Models  
* 모델  
** GPT3로 시작  


### Supervised fine-tuning (SFT).   
** SFT: 16에폭, 코사인러닝decay, dropout0.2, RM score기반 검증, 에폭 상승/RM 상승 시 사람의 선호도 증가  


### Reward Modeling (RM).  
** RM-6B 모델(작지만 안정적), CEL사용, rank 해줄 시 속도 상승  


### Reinforcement learning (RL).  
* PPO 사용 => SFT  
** bandit env  
*** 랜덤 user 프롬프트  
*** 프롬프트와 답변 with reward  
** KL 페널티(SFT에서)로 RM의 과최적화 방지  
** PPO-ptx(mix) test  


### Baselines.  
* 베이스라인: GPT3, F-T GPT3, FLAN  


## 3.5 Evaluation  
### Evaluations on API distribution.  
* API 분포기반 평가  
** 사람의 선호 점수  
*** 1-7점  
** 유해탐지 데이터 수집  
*** 이것도 레이블링  


### Evaluations on public NLP datasets  
* 공용 data 평가  
** LM 안정성  
** Zero Shot 성능  
<br/>

# 4 Results  
## 4.1 Results on the API distribution  
### Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.  
* Labelers: InstructGPT를 GPT3 보다 더 선호  

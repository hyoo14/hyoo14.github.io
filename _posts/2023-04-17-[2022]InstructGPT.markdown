---
layout: post
title:  "[2022]Training language models to follow instructions with human feedback"
date:   2023-04-17 03:59:20 +0900
categories: study
---






{% highlight ruby %}


짧은 요약(Abstract) :    
* uset intent 파악위해 사람의 피드백으로 파인튜닝(F-T, Fine Tuning)




{% endhighlight %}  

<br/>


[Paper with my notes](https://drive.google.com/drive/folders/1VRg6Jnv2gBQmxx4kE__NQElIreVw48fy?usp=sharing)  


[~~Lecture link~~]()  

<br/>

# 단어정리  
* hedging: 대비책
* reliability: 믿을 수 있는, 믿을 만한  
* mitigate: 완화시키다, 경감시키다  
* bandit: 강도  
* bandit environment: Bandit environment는 강화 학습에서 사용되는 용어 중 하나로, 에이전트가 행동을 선택할 수 있는 환경  
* preformance regression: 성능이 이전 버전에 비해 감소하는 현상  
* likert scale: 좋아요 점수 스케일 같이 일반적으로 숫자로 표현되는 등간 척도로 구성되어 있으며, 보통 5점 척도나 7점 척도를 사용   
* held-out labels: Held-out labels은 검증 데이터 또는 테스트 데이터에 속한 레이블(정답)을 의미
* overly hedge: 과도한 리스크 관리  
* epistemic humility: 지식에 대한 겸손함이라는 의미로, 인간의 지식과 이해력에 대한 한계를 인식하고 수용하는 태도  





<br/>

# 1 Introduction  
* LM이 의도와 다른 행동 보임  
** next predict라서 의도파악과는 다름  
** 본 논문에서 F-T + RL(사람 선호 맞출 경우 보상하는 강화학습) 제안  
** RM(Reward Model)학습, RM으로부터 F-T, 이 때, PPO 알고리즘 사용  


### Labelers significantly prefer InstructGPT outputs over outputs from GPT-3  
* 레이블을 다는 사람들은 InstructGPT의 결과를 선호함  
** 1.3B param의 InstructGPT를 175B param의 GPT3보다 더 선호  
** 구조는 같되, 사람 data로 학습  
** 선호도 InstructGPT 85 +- 3%, GPT3 71+-4%  


### InstructGPT models show improvements in truthfulness over GPT-3  
* 없던 정보 생성 줄어듦 41%->21%  


### InstructGPT shows small improvements in toxicity over GPT-3, but not bias  
* 유동성 25% 줄임  


### We can minimize performance regression on public NLP datasets by modifying our RLHF fine-tuning procedure.  
* performance regression ( preformance regression: 성능이 이전 버전에 비해 감소하는 현상  ) 줄임  
** RLHF 파인튜닝 -> PPO update  


### Our models generalize to the preferences of "held-out" labelers that did not produce any training data.  
(held-out labels: Held-out labels은 검증 데이터 또는 테스트 데이터에 속한 레이블(정답)을 의미)  
* InstructGPT 레이블 성능 확인  
** 더 연구 필요  


### Public NLP datasets are not reflective of how our language models are used.  
* public data 본 LM 잘 반영 못 함  
** InstructGPT 선호  


### InstructGPT models show promising generalization to instructions outside of the RLHF fine-tuning distribution  
* 파인튜닝분포서 일반화 가능  
** 코드 QA, 다른 언어도 가능  


### InstructGPT still makes simple mistakes.  
** 아직 완전한 모델은 아님  
** 긴 hedging 문장(보험? 대비용? 문장)  
** 잘못된 전제 문제  
** 안정성, 활용성관련 전진 필요  
<br/>


# 2 Related work  
### Research on alignment and learning from human feedbak.  
* 사람의 의도, 사람의 피드백 강화학습으로 반영하는 배치기술 기반(RLHF-Research and Learning from Human Feedback)  
[Figure 2] 본 모델 3steps  
(1) SFT(Supervised Fine Tuning)  
(2) RM(Reward Model)  
(3) PPO(RL, Proximal Policy Optimization)  
** RL 피드백 : award  
*** 로봇, 아타리 게임의 강화학습에서 사용되던 것  
*** 파인튜닝에 이용(대화, 번역, 의미파싱, 스로리생성, 리뷰생성, 근거추출)  
*** 언어 어시스턴트  


### Training language models to follow instructions.  
** cross domain generalization LM과 연관  
*** 다른 도메인 데이터로 각각 train/test  


### Mitigating the harms of language models.  
** 유해요소 from LM 경감   
*** 파인튜닝 small value target 데이터  
*** 프리트레이닝 데이터 필터링  
<br/>

# 3 Models and experimental details  
* 방법론, 실험 상세  
## 3.1 High-level methodology  
* 고차원 방법론  
** 스타일 지속& 요약  



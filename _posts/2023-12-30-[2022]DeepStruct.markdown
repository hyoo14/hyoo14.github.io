---
layout: post
title:  "[2022]DeepStruct: Pretraining of Language Models for Structure Prediction"
date:   2023-12-31 12:57:11 -0400
categories: study
---

{% highlight ruby %}


짧은 요약(Abstract) :    
* 이 논문은 언어 모델의 구조적 이해 능력을 향상시키는 방법을 소개합니다. 이 방법은 이전 방법들과 달리 특정 작업에 대한 세부 조정을 통해 모델을 향상시키는 것이 아니라, 텍스트에서 구조를 생성하기 위해 작업과 무관한 코퍼스의 집합에 언어 모델을 사전 훈련하는 것을 포함합니다. 이 구조 사전 훈련은 모델이 구조적 작업에 대해 가진 지식의 제로샷 전이를 가능하게 합니다. 이 방법의 성능은 열린 정보 추출, 공동 실체 및 관계 추출, 명명된 실체 인식, 관계 분류, 의미 역할 라벨링, 이벤트 추출, 공동 참조 해결, 사실적 탐사, 의도 탐지, 대화 상태 추적을 포함한 10가지 구조 예측 작업에 대한 28개 데이터셋에서 평가됩니다. 이 접근법은 작업별 특정 훈련 세트로 더욱 강화됩니다. 이는 10B 파라미터 언어 모델이 대부분의 작업에 비중이 있는 지식을 전달할 수 있으며, 평가된 28개 데이터셋 중 21개에서 최신 성능을 달성한다는 것을 보여줍니다.  

Useful sentences :  
*   

{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1hdW96_pkwdPshi-MrpA-oZkN6fjDwQ_U?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 



<br/>

# 1 Introduction  
*  

# 2 Structure Pretraining  
## 2.1 Generative Pretraining  
### Pretraining Data  
## 2.2 Tasks  
## 2.3 Zero-Shot  
## 2.4 Multi-Task  
# 3 Experiments  
## 3.1 Main Results  
## 3.2 Ablation Studies  
### Pretraining Strategies  
### Scaling Laws  
# 4 Related Work  
# 5 Discussion  
## Related Models  
## Zero-Shot Setup  
# 6 Conclusion  
# 7 Ethical Considerations  
# 8 Environmental Considerations  
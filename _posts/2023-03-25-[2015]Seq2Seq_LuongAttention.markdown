---
layout: post
title:  "[2015]Effective Approaches to Attention-based Neural Machine Translation"
date:   2023-03-25 19:59:30 +0900
categories: study
---






{% highlight ruby %}


짧은 요약(Abstract) :    
* 어텐션이 NMT 향상  
* 근데 아직 연구/접근법 적음  
* 간단, 효과적 2가지 방법 소개  
	* global: 전체 소스 문장 단어 다 봄  
	* local: subset만 봄  
* WMT 영/독 번역으로 실험  
	* 5.0 BLEU 향상 with dropout  
* 앙상블 모델로 SOTA 달성  
	* 25.9 BLEU로 기존 n-gram reranker에 1.0BLEU 앞섬    




{% endhighlight %}  

<br/>


[Paper with my notes](https://drive.google.com/drive/folders/1pGtzNyK5IkgwWkjayZVsoc6ryOpl986-?usp=sharing)  


[Lecture link](https://vimeo.com/162101582)  

<br/>

# 단어정리  
* .   

   

# 1 Introduction  
* .   
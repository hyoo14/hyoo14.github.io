---
layout: post
title:  "[2023]Retentive Network: A Successor to Transformer for Large Language Models"
date:   2023-07-30 23:46:24 +0900
categories: study
---






{% highlight ruby %}


짧은 요약(Abstract) :    
* Recursive + Attention 구조 제안   
** LLM에 좀 더 친화적  
** 병렬, 반복, chunkwise recurrent 제안(병렬+반복의 중간정도?)    
** 매우 효율적으로 추론 복잡도 O(1)  
** 스케일링(up), 병렬, cost efficientcy 매우 좋음  


{% endhighlight %}  

<br/>


[Paper with my notes]()  


[~~Lecture link~~]()  

<br/>

# 단어정리  
* chunkwise: 조각 단위로, 부분 별로, chunk는 대개 큰 데이터나 정보의 한 조각을 의미.. 그러므로 chunkwise는 데이터나 정보를 조각 단위로 처리하거나 분석하는 방식  
* throughtput: 처리량, 초당 비트수 또는 초당 작업 수  
* latency: 대기시간, 지연시간, 특정 작업을 시작한 시점부터 완료될 때까지의 시간,  주로 밀리초(ms)나 마이크로초(μs)와 같은 시간 단위로 표시  







<br/>

# 1 Introduction  
*  .  

---
layout: post
title:  "[2025]LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models"  
date:   2025-07-25 13:59:40 +0900
categories: study
---

{% highlight ruby %}


í•œì¤„ ìš”ì•½: 


ì¶”ë¡ ìœ¼ë¡œ ë°©ì •ì‹ ë„ì¶œí•˜ê²Œí•˜ëŠ” ë°´ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì œì•ˆ(ê¸°ì¡´ì€ ì•”ê¸°ìœ„ì£¼)  


ì§§ì€ ìš”ì•½(Abstract) :    



ì´ ë…¼ë¬¸ì€ ê³¼í•™ ë°©ì •ì‹ ë°œê²¬(task of scientific equation discovery)ì„ ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ LLM-SRBenchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë“¤ì€ ìœ ëª…í•œ ë°©ì •ì‹ì„ ì‚¬ìš©í•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ë‹¨ìˆœíˆ ì•”ê¸°í•œ ë‚´ìš©ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì´ì§€ ì§„ì •í•œ â€˜ë°œê²¬â€™ì„ í•˜ëŠ”ì§€ íŒë‹¨í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ì´ 239ê°œì˜ ë¬¸ì œë¡œ êµ¬ì„±ëœ ë‘ ê°€ì§€ ìœ í˜•ì˜ ê³¼ì œë¥¼ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤:

LSR-Transform: ì˜ ì•Œë ¤ì§„ ë¬¼ë¦¬ ëª¨ë¸ì„ ë³€í˜•í•˜ì—¬ ë“œë¬¸ ìˆ˜í•™ì  í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê³ , LLMì´ ì•”ê¸°ê°€ ì•„ë‹Œ ì¶”ë¡ ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.

LSR-Synth: ì¸ê³µì ìœ¼ë¡œ ìƒì„±ëœ, ì°¸ì‹ í•˜ê³  ë°œê²¬ ì§€í–¥ì ì¸ ë¬¸ì œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ìˆ˜ì‹ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.

ì‹¤í—˜ ê²°ê³¼, ê°€ì¥ ì˜ ìˆ˜í–‰ëœ ì‹œìŠ¤í…œì¡°ì°¨ë„ ìƒì§•ì  ì •í™•ë„(symbolic accuracy)ê°€ **31.5%**ì— ê·¸ì³¤ìŠµë‹ˆë‹¤. ì´ëŠ” ì´ ê³¼ì œê°€ ë§¤ìš° ì–´ë µê³ , ì œì•ˆëœ ë²¤ì¹˜ë§ˆí¬ê°€ LLM ê¸°ë°˜ ë°©ì •ì‹ ë°œê²¬ì˜ í•œê³„ë¥¼ í‰ê°€í•˜ê³  í–¥í›„ ì—°êµ¬ë¥¼ ì´ë„ëŠ” ë° ìœ ìš©í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


Abstract
Scientific equation discovery has long been a cornerstone of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, it is difficult to assess the true discovery capabilities of these methods because existing benchmarks often use well-known equations. This makes them vulnerable to memorization by LLMs and results in inflated performance metrics that do not reflect genuine discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories:

LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and

LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning.
Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.




* Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()   

<br/>

# ë‹¨ì–´ì •ë¦¬  
*  







 
<br/>
# Methodology    



ì´ ë…¼ë¬¸ì€ **LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)**ì„ í™œìš©í•œ ê³¼í•™ ë°©ì •ì‹ ë°œê²¬(equation discovery)ì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ ë²¤ì¹˜ë§ˆí¬ LLM-SRBenchë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. LLM-SRBenchëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œ ìœ í˜•ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

LSR-Transform
ê¸°ì¡´ì˜ ìœ ëª… ë¬¼ë¦¬ ë°©ì •ì‹(Feynman equations)ì„ ë³€í˜•í•˜ì—¬ ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ ìˆ˜í•™ì  í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê³ , ì…ë ¥-ì¶œë ¥ ë³€ìˆ˜ì˜ ì—­í• ì„ ë°”ê¾¸ì–´ ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ íŒŒì´ì¬ì˜ SymPy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ìˆ˜ì‹ì„ ê¸°í˜¸ì ìœ¼ë¡œ ì¬ì •ì˜í•˜ê³ , ìƒˆë¡œìš´ ë¬¸ì œ ì„¤ëª…ì€ GPT-4oë¥¼ í™œìš©í•´ ìì—°ì–´ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ëª¨ë¸ì´ ì•”ê¸°í•œ ê³µì‹ì´ ì•„ë‹Œ, ë‚¯ì„  ìˆ˜ì‹ í˜•íƒœì— ëŒ€í•´ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ëª©ì ì´ ìˆìŠµë‹ˆë‹¤.

LSR-Synth
ê³¼í•™ ë¶„ì•¼ë³„ ëŒ€í‘œ ë¬¸ì œ(ì˜ˆ: í™”í•™ ë°˜ì‘ ì†ë„, ìƒë¬¼í•™ì  ê°œì²´ ìˆ˜ ì¦ê°€ ë“±)ì— ëŒ€í•´ ê¸°ì¡´ì— ì•Œë ¤ì§„ ìˆ˜í•™ì  í•­ê³¼ LLMì´ ìƒì„±í•œ ì°¸ì‹ í•œ synthetic termì„ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ ë°©ì •ì‹ì„ ë§Œë“­ë‹ˆë‹¤. ì´í›„ **ìˆ˜ì¹˜ í•´ì„ ê¸°ë²•(numerical solver)**ì„ ì´ìš©í•´ ë¬¸ì œì˜ í•´ ì¡´ì¬ì„±ì„ ê²€ì¦í•˜ê³ , GPT-4oë¥¼ ì´ìš©í•œ novelty í‰ê°€ì™€ ì „ë¬¸ê°€ ê²€ì¦ì„ í†µí•´ ë¬¸ì œì˜ ë‚œì´ë„ì™€ ì‹ ë¢°ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” íŠ¹ë³„í•œ ì•„í‚¤í…ì²˜ë³´ë‹¤ëŠ” ë‹¤ì–‘í•œ LLM ê¸°ë°˜ ë°©ë²•ë“¤ì˜ ë¹„êµ ì‹¤í—˜ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ëŒ€í‘œì ì¸ ëª¨ë¸/ê¸°ë²•ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

Direct Prompting (DataBlind): ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë¬¸ì œ ì„¤ëª…ë§Œìœ¼ë¡œ ìˆ˜ì‹ì„ ìƒì„±í•˜ëŠ” ë² ì´ìŠ¤ë¼ì¸.

LLM-SR (Shojaee et al., 2024b): LLMì´ ìƒì„±í•œ ìˆ˜ì‹ skeletonì„ íŒŒì´ì¬ í•¨ìˆ˜ í˜•íƒœë¡œ í‘œí˜„í•˜ê³ , ì§„í™” ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í”¼ë“œë°± ë£¨í”„ë¥¼ í†µí•´ ìˆ˜ì •.

LaSR (Grayeli et al., 2024): ìˆ˜ì‹ ê°œë…ì„ ì¶”ì¶œí•˜ê³  ì´ë¥¼ ë‹¤ì‹œ LLM + PySR ê¸°ë°˜ ì§„í™” íƒìƒ‰ìœ¼ë¡œ í™•ì¥.

SGA (Ma et al., 2024): LLMìœ¼ë¡œ discrete ìˆ˜ì‹ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  PyTorch ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì—°ì†ì  íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” bilevel optimization êµ¬ì¡°.

ì‹¤í—˜ì—ëŠ” Llama-3.1-8B-Instruct, GPT-3.5-turbo, GPT-4o-mini ë“±ì˜ ë°±ë³¸ì´ ì‚¬ìš©ë˜ë©°, ê° ëª¨ë¸ì€ ë™ì¼í•œ ì¡°ê±´(ë¬¸ì œë‹¹ LLM í˜¸ì¶œ 1000íšŒ ì´ë‚´)ì—ì„œ í‰ê°€ë©ë‹ˆë‹¤.




This paper introduces LLM-SRBench, a new benchmark designed to evaluate LLM-based scientific equation discovery. The benchmark consists of two major problem types:

LSR-Transform
This component takes existing equations from the Feynman dataset and transforms them into less familiar mathematical forms by switching input-output variables. Symbolic transformations are performed using the SymPy Python library, and new problem descriptions are generated using GPT-4o. This tests whether LLMs can reason beyond memorized equation formats.

LSR-Synth
This component generates novel synthetic equations by combining known scientific terms (e.g., from reaction kinetics or population dynamics) with novel symbolic terms generated by an LLM. The solvability of each equation is verified using numerical solvers, and the novelty of the expression is validated using GPT-4o and domain experts.

The benchmark does not introduce a single new model architecture, but rather evaluates various LLM-based equation discovery methods, including:

Direct Prompting (DataBlind): Generates equations purely from the prompt without using any data.

LLM-SR (Shojaee et al., 2024b): Generates equation skeletons as Python functions and refines them through a multi-island evolutionary loop guided by LLM feedback.

LaSR (Grayeli et al., 2024): Abstracts symbolic relations into concepts and uses a hybrid evolutionary + LLM-guided approach to evolve new hypotheses.

SGA (Ma et al., 2024): Implements a bilevel optimization strategy, where LLMs propose symbolic expressions, and PyTorch simulations optimize the parameters.

Experiments are conducted with Llama-3.1-8B-Instruct, GPT-3.5-turbo, and GPT-4o-mini, with all methods limited to 1000 LLM calls per problem to ensure fair comparison.




   
 
<br/>
# Results  



ë…¼ë¬¸ì€ LLM ê¸°ë°˜ ë°©ì •ì‹ ë°œê²¬ ë°©ë²•ë“¤ì„ **ì„¸ ê°€ì§€ LLM ë°±ë³¸(GPT-4o-mini, GPT-3.5-turbo, Llama-3.1-8B)**ì„ í™œìš©í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤. í‰ê°€ ëŒ€ìƒì€ ë„¤ ê°€ì§€ ë°©ë²•ì…ë‹ˆë‹¤:

Direct Prompting (DataBlind): ë°ì´í„° ì—†ì´ ë¬¸ì œ ì„¤ëª…ë§Œìœ¼ë¡œ ë°©ì •ì‹ì„ ìƒì„±

SGA (Ma et al., 2024): LLM + ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ bilevel ìµœì í™”

LaSR (Grayeli et al., 2024): ìˆ˜ì‹ ê°œë… í•™ìŠµ í›„ ì§„í™” íƒìƒ‰

LLM-SR (Shojaee et al., 2024b): íŒŒì´ì¬ í•¨ìˆ˜ ê¸°ë°˜ skeleton + ì§„í™” ì „ëµ

ëª¨ë“  ë°©ë²•ì€ LLM-SRBenchì˜ ë‘ ê°€ì§€ í•˜ìœ„ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ë©ë‹ˆë‹¤:

LSR-Transform (ì´ 111ê°œ ë¬¸ì œ): ì˜ ì•Œë ¤ì§„ ë¬¼ë¦¬ ë°©ì •ì‹ì„ ë³€í˜•í•œ ë²„ì „

LSR-Synth (ì´ 128ê°œ ë¬¸ì œ): í•©ì„±ëœ ì°¸ì‹ í•œ ìˆ˜ì‹ í¬í•¨, 4ê°œ ê³¼í•™ ë„ë©”ì¸(í™”í•™, ìƒë¬¼, ë¬¼ë¦¬, ì†Œì¬ê³¼í•™)



ì‚¬ìš©ëœ í‰ê°€ ì²™ë„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

Symbolic Accuracy (SA): ì˜ˆì¸¡ ìˆ˜ì‹ê³¼ ì •ë‹µ ìˆ˜ì‹ì˜ ê¸°í˜¸ì  ìœ ì‚¬ì„± í‰ê°€ (GPT-4oë¥¼ ì´ìš©í•œ í‰ê°€ì)

Accâ‚€.â‚: ì˜ˆì¸¡ê°’ì´ ì •ë‹µê³¼ 10% ì´ë‚´ ì˜¤ì°¨ ë²”ìœ„ì— ìˆì„ í™•ë¥ 

NMSE (Normalized Mean Squared Error): ì •ê·œí™”ëœ í‰ê· ì œê³±ì˜¤ì°¨



í•µì‹¬ ê²°ê³¼ ìš”ì•½:

ì „ì²´ì ìœ¼ë¡œ LLM-SR + GPT-4o-mini ì¡°í•©ì´ **ìƒì§• ì •í™•ë„ 31.5% (LSR-Transform ê¸°ì¤€)**ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ì„ ë³´ì„

LaSR + GPT-4o-miniëŠ” ìˆ˜ì¹˜ ì •ë°€ë„(Accâ‚€.â‚, NMSE) ë©´ì—ì„œ ê°€ì¥ ë›°ì–´ë‚¨

Direct Prompting ë°©ì‹ì€ ê±°ì˜ ëª¨ë“  ì§€í‘œì—ì„œ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë°ì´í„° ì—†ëŠ” ì¶”ë¡ ì˜ í•œê³„ë¥¼ ë³´ì—¬ì¤Œ

LSR-Synth ë°ì´í„°ì…‹ì€ ê¸°ì¡´ ë¬¸ì œë¥¼ ë³€í˜•í•œ LSR-Transformë³´ë‹¤ í›¨ì”¬ ì–´ë ¤ì›Œ, ì „ë°˜ì ìœ¼ë¡œ ë‚®ì€ ì„±ëŠ¥ ê¸°ë¡ë¨

ê³¼í•™ ë¶„ì•¼ë³„ë¡œë„ ì„±ëŠ¥ í¸ì°¨ ì¡´ì¬: í™”í•™/ìƒë¬¼ ë¶„ì•¼ì—ì„œ OOD ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ë‚®ìŒ



ì¶”ê°€ ë¶„ì„ì—ì„œëŠ” ê¸°í˜¸ ì •í™•ë„ì™€ OOD ì¼ë°˜í™” ì„±ëŠ¥(Accâ‚€.â‚, NMSE) ê°„ì— ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•˜ì—¬, ê¸°í˜¸ì  ì •ë‹µì„±ì´ ìˆ˜ì¹˜ì  ì¼ë°˜í™”ë ¥ì˜ ì§€í‘œê°€ ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•¨.





Direct Prompting (DataBlind) â€“ Generates equations without access to data

SGA (Ma et al., 2024) â€“ A bilevel optimization strategy using LLMs and simulations

LaSR (Grayeli et al., 2024) â€“ Learns equation concepts and performs hybrid evolution

LLM-SR (Shojaee et al., 2024b) â€“ Generates Python skeleton functions refined via evolutionary search

These methods are evaluated on the two parts of LLM-SRBench:

LSR-Transform (111 tasks): Transformed variants of known physics equations

LSR-Synth (128 tasks): Synthetic, novel equation discovery tasks spanning chemistry, biology, physics, and materials science



Metrics used:

Symbolic Accuracy (SA): Measures symbolic similarity to the ground truth using GPT-4o as an evaluator

Accâ‚€.â‚: Proportion of predictions within 10% relative error

NMSE (Normalized Mean Squared Error): Measures numerical precision



Key findings:

LLM-SR with GPT-4o-mini achieves the highest symbolic accuracy (31.5%) on LSR-Transform

LaSR with GPT-4o-mini consistently performs best in numeric precision (highest Accâ‚€.â‚, lowest NMSE)

Direct Prompting shows poor performance across all metrics, highlighting the limits of memorization without data

LSR-Synth tasks are significantly harder than LSR-Transform, with lower scores across the board

There are domain-specific differences, with chemistry and biology exhibiting worse OOD generalization than physics or materials science



Correlation analysis shows strong alignment between symbolic accuracy and numeric generalization (Accâ‚€.â‚ and NMSE), suggesting that symbolic correctness is a reliable indicator of generalization ability.




<br/>
# ì˜ˆì œ  



ë…¼ë¬¸ì—ì„œ ì œì‹œëœ **ê³¼í•™ ë°©ì •ì‹ ë°œê²¬(task)**ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœì…ë‹ˆë‹¤:

ëª©í‘œ: ì£¼ì–´ì§„ ì…ë ¥ ë³€ìˆ˜ë“¤ê³¼ ê³¼í•™ì  ë§¥ë½, ìˆ˜ì¹˜ ë°ì´í„°ë¡œë¶€í„° ì¶œë ¥ ë³€ìˆ˜ì™€ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜í•™ì  ë°©ì •ì‹ì„ ë°œê²¬í•˜ë¼.



ì˜ˆì‹œ 1: ê³ ì „ì—­í•™ (LSR-Transformì—ì„œ ë°œì·Œ)
ê³¼í•™ì  ë§¥ë½: ì—ë„ˆì§€ë¥¼ ì €ì¥í•˜ëŠ” ì§„ë™ ì‹œìŠ¤í…œì˜ ì§ˆëŸ‰(m)ì„ ì¶”ì •

ì…ë ¥ ë³€ìˆ˜ë“¤:

í‰ê·  ì €ì¥ ì—ë„ˆì§€ 
ğ¸
ğ‘›
E 
n
â€‹
 

êµ¬ë™ ì£¼íŒŒìˆ˜ 
ğœ”
Ï‰

ê³ ìœ  ì£¼íŒŒìˆ˜ 
ğœ”
0
Ï‰ 
0
â€‹
 

ì§„í­ 
ğ‘¥
x

ì¶œë ¥ ë³€ìˆ˜: ì§ˆëŸ‰ 
ğ‘š
m

ì •ë‹µ ë°©ì •ì‹ ì˜ˆì‹œ:

ğ‘š
=
4
ğ¸
ğ‘›
ğ‘¥
2
(
ğœ”
2
+
ğœ”
0
2
)
m= 
x 
2
 (Ï‰ 
2
 +Ï‰ 
0
2
â€‹
 )
4E 
n
â€‹
 
â€‹
 
ì…ë ¥ ë°ì´í„° ì˜ˆì‹œ:

ğ¸
ğ‘›
E 
n
â€‹
 	
ğœ”
Ï‰	
ğœ”
0
Ï‰ 
0
â€‹
 	
ğ‘¥
x	
ğ‘š
m (ì¶œë ¥)
4.7	1.2	2.3	1.5	1.2
3.4	2.7	2.7	3.1	0.1
2.8	1.5	3.6	1.4	0.4



ì˜ˆì‹œ 2: í™”í•™ ë°˜ì‘ ì†ë„ (LSR-Synthì—ì„œ ë°œì·Œ)
ê³¼í•™ì  ë§¥ë½: ë†ë„ 
ğ´
(
ğ‘¡
)
A(t)ì— ë”°ë¥¸ ë°˜ì‘ ì†ë„ 
ğ‘‘
ğ´
ğ‘‘
ğ‘¡
dt
dA
â€‹
  ì˜ˆì¸¡

ì…ë ¥ ë³€ìˆ˜ë“¤: ì‹œê°„ 
ğ‘¡
t, ë†ë„ 
ğ´
(
ğ‘¡
)
A(t)

ì¶œë ¥ ë³€ìˆ˜: 
ğ‘‘
ğ´
ğ‘‘
ğ‘¡
dt
dA
â€‹
 

ì •ë‹µ ë°©ì •ì‹ ì˜ˆì‹œ (í•©ì„±ëœ ì‹):

ğ‘‘
ğ´
ğ‘‘
ğ‘¡
=
âˆ’
ğ‘˜
ğ´
(
ğ‘¡
)
2
âˆ’
ğ‘˜
ğ´
(
ğ‘¡
)
exp
â¡
(
âˆ’
ğ‘˜
ğ‘ 
ğ‘¡
)
+
ğ‘˜
ğ‘
sin
â¡
(
ğœ”
ğ´
(
ğ‘¡
)
ğ‘¡
)
dt
dA
â€‹
 =âˆ’kA(t) 
2
 âˆ’kA(t)exp(âˆ’kst)+kpsin(Ï‰A(t)t)
ì´ ë°©ì •ì‹ì€ ì¼ë°˜ì ì¸ í•­(ì˜ˆ: 
âˆ’
ğ‘˜
ğ´
2
âˆ’kA 
2
 )ê³¼ ì°¸ì‹ í•œ í•­(ì˜ˆ: 
sin
â¡
(
ğœ”
ğ´
ğ‘¡
)
sin(Ï‰At))ì„ ì¡°í•©í•˜ì—¬, ëª¨ë¸ì´ ë‹¨ìˆœ ì•”ê¸° ëŒ€ì‹  ë°ì´í„° ê¸°ë°˜ ì¶”ë¡ ì„ ìš”êµ¬í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ğŸ”¬ English Version (Examples: Input/Output Structure, Task Description)
The core task in the benchmark is:

Goal: Given input features, scientific context, and a dataset of numerical observations, discover a symbolic equation describing the relationship to the target variable.



Example 1: Classical Mechanics (from LSR-Transform)
Scientific Task: Predict the mass 
ğ‘š
m of an oscillating system storing energy

Input Variables:

Mean stored energy 
ğ¸
ğ‘›
E 
n
â€‹
 

Driving frequency 
ğœ”
Ï‰

Natural frequency 
ğœ”
0
Ï‰ 
0
â€‹
 

Amplitude 
ğ‘¥
x

Target Variable: Mass 
ğ‘š
m

Ground-Truth Equation:

ğ‘š
=
4
ğ¸
ğ‘›
ğ‘¥
2
(
ğœ”
2
+
ğœ”
0
2
)
m= 
x 
2
 (Ï‰ 
2
 +Ï‰ 
0
2
â€‹
 )
4E 
n
â€‹
 
â€‹
 
Sample Input Data:

ğ¸
ğ‘›
E 
n
â€‹
 	
ğœ”
Ï‰	
ğœ”
0
Ï‰ 
0
â€‹
 	
ğ‘¥
x	
ğ‘š
m (output)
4.7	1.2	2.3	1.5	1.2
3.4	2.7	2.7	3.1	0.1
2.8	1.5	3.6	1.4	0.4



Example 2: Chemical Reaction Kinetics (from LSR-Synth)
Scientific Task: Predict the rate of change 
ğ‘‘
ğ´
ğ‘‘
ğ‘¡
dt
dA
â€‹
  for chemical concentration 
ğ´
(
ğ‘¡
)
A(t)

Input Variables: Time 
ğ‘¡
t, concentration 
ğ´
(
ğ‘¡
)
A(t)

Target Variable: 
ğ‘‘
ğ´
ğ‘‘
ğ‘¡
dt
dA
â€‹
 

Synthetic Ground-Truth Equation:

ğ‘‘
ğ´
ğ‘‘
ğ‘¡
=
âˆ’
ğ‘˜
ğ´
(
ğ‘¡
)
2
âˆ’
ğ‘˜
ğ´
(
ğ‘¡
)
exp
â¡
(
âˆ’
ğ‘˜
ğ‘ 
ğ‘¡
)
+
ğ‘˜
ğ‘
sin
â¡
(
ğœ”
ğ´
(
ğ‘¡
)
ğ‘¡
)
dt
dA
â€‹
 =âˆ’kA(t) 
2
 âˆ’kA(t)exp(âˆ’kst)+kpsin(Ï‰A(t)t)
This equation combines known terms (e.g., 
âˆ’
ğ‘˜
ğ´
2
âˆ’kA 
2
 ) and novel terms (e.g., 
sin
â¡
(
ğœ”
ğ´
ğ‘¡
)
sin(Ï‰At)) to encourage data-driven discovery rather than memorization.





<br/>  
# ìš”ì•½   


ì´ ë…¼ë¬¸ì€ ê³¼í•™ ë°©ì •ì‹ ë°œê²¬ì„ ìœ„í•´ ê¸°ì¡´ ë¬¼ë¦¬ ê³µì‹ì„ ë³€í˜•í•˜ê±°ë‚˜ í•©ì„± ìˆ˜ì‹ì„ í¬í•¨í•œ ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ìƒì„±í•œ LLM-SRBenchë¥¼ ì œì•ˆí•œë‹¤. ì‹¤í—˜ ê²°ê³¼, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ LLM-SR(GPT-4o-mini ê¸°ë°˜) ëª¨ë¸ë„ ìƒì§• ì •í™•ë„ 31.5%ì— ê·¸ì³ ì´ ê³¼ì œê°€ ì—¬ì „íˆ ì–´ë µë‹¤ëŠ” ì ì„ ë³´ì—¬ì¤€ë‹¤




This paper proposes LLM-SRBench, a benchmark for scientific equation discovery that includes both transformed versions of known physics equations and novel synthetic problems. Experiments show that even the best-performing model, LLM-SR with GPT-4o-mini, achieves only 31.5% symbolic accuracy, highlighting the difficulty of the task.



<br/>  
# ê¸°íƒ€  





Figure 1: Error ë¶„ì„ (Feynman vs. LSR-Transform/Synth)
LLMì´ ê¸°ì¡´ Feynman ë¬¸ì œì—ì„œëŠ” ìˆ˜ì¹˜ ì˜¤ì°¨ì™€ ê¸°í˜¸ ì˜¤ì°¨ê°€ ë§¤ìš° ë‚®ì•„, **ë‹¨ìˆœ ì•”ê¸°(recitation)**ì— ì˜í•œ ì„±ëŠ¥ì„ì„ ì‹œì‚¬í•¨.

ë°˜ë©´ LSR-SRBenchì—ì„œëŠ” ì˜¤ì°¨ê°€ í¬ê³  ê°ì†Œ ì†ë„ë„ ëŠë ¤, ì¶”ë¡  ê¸°ë°˜ íƒìƒ‰ì´ í•„ìš”í•¨ì„ ë³´ì—¬ì¤Œ.



Figure 2 & 3: ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸
Figure 2ëŠ” ì „ì²´ discovery í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì£¼ë©°, ê³¼í•™ ì§€ì‹ ê¸°ë°˜ ì¶”ë¡ ê³¼ ë°ì´í„° ê¸°ë°˜ íƒìƒ‰ì˜ ê²°í•© êµ¬ì¡°ë¥¼ ì‹œê°í™”í•¨.

Figure 3ì€ LSR-Transformê³¼ LSR-Synthê°€ ì–´ë–»ê²Œ ìƒì„±ë˜ëŠ”ì§€ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ë©°, ê°ê°ì˜ ë¬¸ì œ ìƒì„±, ë³€í™˜, ê²€ì¦ ê³¼ì •ì´ êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬ë˜ì–´ ìˆìŒ.



Table 1: ì„±ëŠ¥ ë¹„êµí‘œ
ê° ëª¨ë¸-LLM ì¡°í•©ì— ëŒ€í•œ Symbolic Accuracy, Accâ‚€.â‚, NMSE ì§€í‘œê°€ ì •ë¦¬ë˜ì–´ ìˆìœ¼ë©°,
GPT-4o-mini ê¸°ë°˜ LLM-SRì´ ê¸°í˜¸ ì •í™•ë„ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•˜ê³ , LaSRì´ ìˆ˜ì¹˜ ì˜¤ì°¨ ìµœì†Œí™”ì— ê°•ì ì„ ë³´ì„.

**ë„ë©”ì¸ë³„ ì„±ëŠ¥ í¸ì°¨(ì˜ˆ: ì†Œì¬ ê³¼í•™ì—ì„œì˜ ë†’ì€ ì •í™•ë„)**ë„ í™•ì¸ ê°€ëŠ¥í•¨.



Figure 4: ë¬¸ì œ ë³µì¡ë„ë³„ ì„±ëŠ¥ (Feynman vs. LSR-Transform)
ë™ì¼í•œ ë³µì¡ë„ì—ì„œë„ LSR-Transform ë¬¸ì œì—ì„œ ì„±ëŠ¥ì´ ë” ë‚®ê²Œ ë‚˜íƒ€ë‚˜ë©°, ì´ëŠ” LLMì´ **ë‚¯ì„  í‘œí˜„(unfamiliar forms)**ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•¨.



Figure 5: ID vs. OOD ì¼ë°˜í™” ì„±ëŠ¥
ëª¨ë“  ëª¨ë¸ì—ì„œ OOD ì„±ëŠ¥ì´ IDë³´ë‹¤ ë‚®ìœ¼ë©°, ì¼ë°˜í™”ì— ì–´ë ¤ì›€ì´ ì¡´ì¬í•¨.

íŠ¹íˆ í™”í•™/ìƒë¬¼ ë¶„ì•¼ì—ì„œëŠ” OOD ì˜¤ì°¨ê°€ ë” í¬ê²Œ ì¦ê°€í•´ ë„ë©”ì¸ í¸í–¥ ë¬¸ì œë¥¼ ì‹œì‚¬í•¨.



Figure 6: ê¸°í˜¸ ì •í™•ë„ì™€ OOD ì •ë°€ë„ì˜ ìƒê´€ê´€ê³„
Symbolic Accuracyê°€ ë†’ì„ìˆ˜ë¡ Accâ‚€.â‚ì€ ë†’ê³  NMSEëŠ” ë‚®ì•„ì§€ë©°,
ì´ëŠ” ê¸°í˜¸ ì •ë‹µì„±ì´ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì˜ ì§€í‘œë¡œ í™œìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œ.



Appendix A: ë°©ì •ì‹ ë³µì¡ë„ ë¶„í¬ì™€ ìƒì„± ì˜ˆì‹œ
Figure 8, 9, 10 ë“±ì„ í†µí•´ LSR-SRBenchì˜ ìˆ˜ì‹ ë³µì¡ë„ëŠ” Feynmanë³´ë‹¤ ë†’ì§€ë§Œ, ê¸°í•˜í•™ì  í‘œí˜„ì˜ ê· í˜•ì„ ê³ ë ¤í•˜ì—¬ ì„¤ê³„ë¨.

ìƒì„±ëœ ìˆ˜ì‹ ì˜ˆì‹œ(ì˜ˆ: 
ğ‘‘
ğ´
ğ‘‘
ğ‘¡
=
âˆ’
ğ‘˜
ğ´
2
+
sin
â¡
(
ğ´
)
dt
dA
â€‹
 =âˆ’kA 
2
 +sin( 
A
â€‹
 ))ëŠ” ëª¨ë¸ì´ ë‹¨ìˆœ ê³µì‹ ê¸°ì–µì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì •êµí•œ ì¶”ë¡ ì„ í•´ì•¼ í•¨ì„ ë³´ì—¬ì¤Œ.



Figure 1: Error Analysis (Feynman vs. LSR)
Shows much lower symbolic and numeric error for Feynman problems, indicating LLMs are memorizing these solutions.

In contrast, higher and more gradual error on LSR problems suggests the need for genuine reasoning and exploration.



Figure 2 & 3: Data Generation Pipelines
Figure 2 visualizes the full discovery workflow, emphasizing the synergy between scientific priors and data-driven refinement.

Figure 3 details how LSR-Transform and LSR-Synth datasets are constructed and validated, highlighting multi-step filtering, transformation, and novelty checks.



Table 1: Performance Comparison
Summarizes symbolic accuracy (SA), Accâ‚€.â‚, and NMSE across models and domains.

LLM-SR + GPT-4o-mini achieves the best SA, while LaSR excels in numeric precision, with notable performance variation across domains.



Figure 4: Performance vs. Equation Complexity
Even at similar complexity levels, LSR-Transform is harder than Feynman, implying that LLMs struggle with unfamiliar forms, not just long equations.



Figure 5: In-Domain (ID) vs. Out-of-Domain (OOD)
Performance consistently drops on OOD data, revealing generalization challenges.

Chemistry and biology tasks show the greatest OOD degradation, suggesting domain-dependent vulnerabilities.



Figure 6: Symbolic Accuracy vs. OOD Generalization
Symbolic accuracy positively correlates with Accâ‚€.â‚ and negatively with NMSE, indicating that better symbolic discovery leads to stronger generalization.



Appendix A: Expression Complexity and Examples
Figures 8â€“10 compare complexity distributions and provide generation examples.

They demonstrate that LSR-SRBench contains non-trivial, interpretable, and solvable equations, pushing models beyond rote memorization.




<br/>
# refer format:     



@inproceedings{shojaee2025llmsrbench,
  title     = {{LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models}},
  author    = {Parshin Shojaee and Ngoc-Hieu Nguyen and Kazem Meidani and Amir Barati Farimani and Khoa D Doan and Chandan K Reddy},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year      = {2025},
  volume    = {267},
  publisher = {PMLR},
  address   = {Vancouver, Canada},
  url       = {https://github.com/deep-symbolic-mathematics/llm-srbench}
}





Shojaee, Parshin, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D. Doan, and Chandan K. Reddy. "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models." In Proceedings of the 42nd International Conference on Machine Learning (ICML), vol. 267. Vancouver, Canada: PMLR, 2025. https://github.com/deep-symbolic-mathematics/llm-srbench.







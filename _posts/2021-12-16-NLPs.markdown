---
layout: post
title:  "NLPs"
date:   2021-12-15 11:20:10 +0900
categories: study
---




{% highlight ruby %}
짧은 요약 :

NLP 일부 컨셉노트  

{% endhighlight %}


# NLPs

*BART(Bidirectional and Auto-Regressive Transformers)  
**NLU와 NLG 모두 잘 하는 모델 없을까? 에서 시작  
**encoder, decoder 모두 사용한 pretrain  
***기존의 bert는 encoder만 씀, 그래서 mask 이외 노이즈 추가 어렵..  
****다양 노이즈 추가 가능  
**다양한 목적에 맞게 사용 가능  
***프리트레인 성능 올라감. 복잡 테스크로 트레인하니까 모델이 갖는 정보가 더 많?  
**NLU에서는 마지막 디코더에 소프트맥스나 클래시피케이션 레이어 추가해서 파인튜닝  
**일반 NLG 그냥 학습하면 됨  
**NLG MT의 경우 인코더에 다른 언어 들어올 수 있음  
***이 경우 Randomly Initialized Encoder를 커널로 넣어줌. 이렇게 학습함  



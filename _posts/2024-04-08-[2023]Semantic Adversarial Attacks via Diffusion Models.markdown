---
layout: post
title:  "[2023]Semantic Adversarial Attacks via Diffusion Models"  
date:   2024-04-08 19:20:29 -0400
categories: study
---

{% highlight ruby %}


한줄 요약: 

짧은 요약(Abstract) :    
* 전통적인 적대적 공격은 깨끗한 예제에 적대적인 변형을 추가함으로써 픽셀 공간에서 깨끗한 예제를 조작하는 데 집중해왔음
* 반면에, 의미적 적대적 공격은 색상, 맥락, 특징과 같은 깨끗한 예제의 의미적 속성을 변경하는 데 초점을 맞추며, 이는 실제 세계에서 더 실현 가능함
* 본 논문에서는 의미 정보가 잘 훈련된 확산 모델의 잠재 공간에 포함되어 있기 때문에 최근 확산 모델을 활용하여 의미적 적대적 공격을 신속하게 생성하는 프레임워크를 제안함
* 이 프레임워크에는 두 가지 변형이 있음
** 첫 번째로, 의미적 변환 접근법은 생성된 이미지의 잠재 공간과/또는 확산 모델 자체를 미세 조정함
** 두 번째로, 잠재 마스킹 접근법은 다른 대상 이미지와 로컬 역전파 기반 해석 방법을 사용하여 잠재 공간을 마스킹함
* 또한, 의미적 변환 접근법은 화이트박스 또는 블랙박스 설정에서 적용될 수 있음
* CelebA-HQ와 AFHQ 데이터셋에서 광범위한 실험을 수행하였으며, 저자들의 프레임워크는 다른 기준선과 비교하여 훌륭한 충실도, 일반화 능력 및 이전 능력을 입증함
* 저자들의 접근 방식은 여러 설정에서 약 100%의 공격 성공률을 달성하며 최고의 FID는 36.61임
* 코드는 https://github.com/steven202/semantic_adv_via_dm에서 사용할 수 있음

Useful sentences :  
*   


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 
<br/>
# 1 Introduction  
* 심층 신경망은 여러 분야에서 혁신을 이뤄냈음에도 불구하고, 적대적 예제에 대한 본질적인 취약성은 보안 우려를 제기함
* 대부분의 적대적 기계 학습 문헌은 작은 반경 ε을 가진 ℓp 노름 구 안의 적대적 변형을 일반화함
* 기존에 훈련된 모델들은 양성 예제를 높은 정확도로 분류하면서도, 이러한 미세한 변형이 포함된 입력을 잘못 분류함
* 전역적으로 입력 이미지를 픽셀 공간에서 공격하는 대신, 의미론적으로 의미 있는 시각적 속성을 조작하여 실제 세계의 강인성에 대한 통찰을 얻는 의미론적 적대적 공격을 제안함
* 의미론적 공격은 인식될 수 있지만, 이러한 공격은 의미론적으로 의미가 있기 때문에 감지하기 어려움
* 의미론적 적대적 공격의 개념을 따라, 이 주제에 대한 문헌이 증가하고 있음
* 실제 세계에서, ℓp-노름 기반 제약 조건의 적대적 공격은 취약한 변형으로 인해 드물게 발생함
* 픽셀 공간에서의 ℓp-노름 적대적 공격과 비교했을 때, 의미론적 적대적 이미지는 변형의 크기에 제약이 없으면서도 지각적 유사성과 사실성을 유지하기 때문에 더 실행 가능함
* 이러한 공격은 질감이나 기타 의미론적 속성의 변경을 포함하여 오분류로 이어짐 

## Related Work
* ℓp 노름 제약 조건 내에서 픽셀 공간에 대한 적대적 공격을 일반화한 대부분의 적대적 기계 학습 문헌과 달리, [16, 30]은 색상, 맥락 및 기능과 같은 의미론적으로 의미 있는 시각적 속성을   조작함으로써 실제 세계의 강인함에 대한 통찰력을 얻기 위해 의미론적 적대적 공격을 제안함   
* 의미론적 공격은 인지될 수 있으나 이러한 공격은 의미론적으로 의미가 있기 때문에 감지하기 어려움  
* 의미론적 적대적 공격의 개념을 따라, 이 주제에 대한 문헌이 증가하고 있음   

<br/>
# 2 Methodology

확산 모델을 포함하는 저자들의 방법론은 두 가지 주요 접근법으로 구성되어 있음


## 2.1 Preliminary: Diffusion Models   
확산 모델은 확산 과정(전방 과정)과 샘플링 과정(역방향 과정)을 포함함  
확산 과정은 데이터를 간단한 잡음 분포로 변환하고 샘플링 과정은 이 과정을 역전함  
두 단계 모두 마르코프 체인이며 일련의 단계로 구성되며, 모든 단계는 가우시안 분포로 근사화될 수 있음  
확산과 샘플링 과정은 다음과 같이 정의될 수 있음  
q_θ(x_1:T | x_0) = Π(t=1 to T) q_θ(x_t | x_(t-1)), q(x_t | x_(t-1)) = N(x_t; √(1 - β_t)x_(t-1), β_t I)  
p_θ(x_0:T) = p(x_T) Π(t=1 to T) p_θ(x_(t-1) | x_t), p_θ(x_(t-1) | x_t) = N(x_(t-1); μ_θ(x_t, t), Σ_θ(x_t, t))  
여기서 x_t는 t = 1, ..., T에 대한 잠재 공간    
확산 과정에서의 잠재 공간 x_t는 다음과 같이 표현될 수 있음    
x_t = √(α̅_t)x_0 + √(1 - α̅_t)w, w∼N(0,I)  
여기서 α_t = 1 - β_t이고 α̅_t = Π(s=1 to t) α_s  
 전방 과정에서, 매개변수 {β_t}_t=0^T는 재매개변수화를 통해 학습 가능한 매개변수가 될 수도 있고 고정된 상수가 될 수도 있음    
역방향 과정에서, μ_θ(x_t, t)는 μ_θ(x_t, t) = 1/√(α_t)(x_t - β_t/√(1-α̅_t)ε_θ(x_t, t))로 표현될 수 있음    
 ε_θ(x_t, t)는 잡음 근사 모델로, 잠재 공간 x_t와 시간 단계 t에서 ε을 예측    
이 모델은 모델 매개변수 θ에 대해 다음 손실 함수를 최소화하여 훈련될 수 있음      
L_simple(θ) = E[x_0∼q(x_0), w∼N(0,I), t]‖w−ε_θ(x_t, t)‖². 모델이 훈련된 후, 데이터는 샘플링 과정을 통해 샘플링될 수 있음  


## 2.2 The Semantic Transformation(ST) approach  
* 의미 변환 접근법(ST)은 생성된 이미지의 잠재 공간과/또는 확산 모델 자체를 미세 조정하여 의미 정보를 변환함
* 이 접근법은 공격 손실을 통해 잠재 공간을 직접 조작하며, 깨끗한 이미지 x0으로부터 그 잠재 공간 xT를 확산 과정을 통해 얻음
* 생성된 의미 적대적 이미지는 미세 조정된 확산 모델 매개변수 θ̂ 및 미세 조정된 잠재 공간 x̂T를 사용하여 표시됨
* 미세 조정 과정 동안 잠재 공간과/또는 확산 모델을 미세 조정하여 의미 정보를 변환하고, 생성된 이미지 x̂0(θ̂, x̂T)가 분류기를 오도하도록 함
* 미세 조정 과정 이후에는 샘플링 과정을 통해 공격을 평가함
* 이 접근법에서 사용된 손실 함수는 두 이미지 간의 지각적 유사성을 평가하기 위한 LPIPS 메트릭을 최소화하며, 이는 원본 이미지 x0와 생성된 적대적 이미지 x̂0(θ̂, x̂T) 간의 지각적 특징이 미세 조정 동안 동일하게 유지되도록 함
* 또한, 원본 이미지와 생성된 적대적 이미지 간의 예측 로짓에 대한 KL 발산을 최대화하여 두 이미지 간의 로짓 거리를 확장함 

### Nuance between White-box and Black-box Attacks  
* 화이트박스와 블랙박스 공격 간의 미묘한 차이점은 공격자가 대상 모델 f의 매개변수를 알고 있는지 여부에 있음
* 수식(5)에서 DKL을 계산할 때, 화이트박스 공격의 경우 대상 분류기로부터 예측 로짓을 사용하고, 블랙박스 공격의 경우 사전 훈련된 InceptionV3 모델[33]의 출력을 사용함  


## 2.3 TheLatent Masking (LM) Approach  
### Transplanting Features with Mask  
* 두 번째 접근법인 잠재 마스킹 접근법(LM)은 대상 이미지의 특징 중요도로부터 마스크를 생성하여 생성된 이미지를 수정함  
* 이 접근법은 잠재 공간이나 확산 모델 매개변수에 대한 기울기를 계산할 필요 없이 잠재 공간을 마스킹함으로써 계산 비용을 절감함  
* 해석 맵 m을 계산하여 잠재 공간에 마스킹을 적용하며, Grad-CAM과 살리언시 맵을 해석 맵으로 사용함   




<br/>
# 3 Experiments
## 3.1 Datasets 
### HumanFacialIdentityRecognitionandGenderClassification.
* 저자들은 인간의 얼굴 식별과 성별 분류에 대해 저자들의 화이트박스와 블랙박스 공격을 평가함  
* 모든 실험에서는 256x256 크기의 500개 이미지를 사용함  

* 저자들은 Celeb-HQ 얼굴 식별 데이터셋을 사용함, 이는 CelebAMask-HQ 데이터셋의 부분 집합이며, 이 데이터셋은 512x512 해상도의 30,000개 얼굴 이미지를 포함하고 있고, 6,217개의 고유 신원을 가지고 있음  
* 대상 분류기를 위해, 저자들은 307개의 고유 신원, 훈련을 위한 4,263개 이미지, 테스트를 위한 1,215개 이미지를 포함하는 부분 집합을 사용함  
* 이 데이터셋에서 89.05%의 정확도로 30 에폭 동안 훈련된 ResNet18 분류기를 사용함  
* 확산 모델의 경우, 저자들은 1024x1024 해상도의 30,000개 이미지를 포함하는 CelebA-HQ 데이터셋에서 사전 훈련된 모델을 사용함  
* 평가는 3.4 섹션과 보충 자료에서 이루어짐  
* 얼굴 식별 외에도, 저자들은 11,057개의 남성 이미지와 18,943개의 여성 이미지를 포함하는 Celeb-HQ 얼굴 성별 인식 데이터셋을 사용함, 평가는 보충 자료에서 이루어짐  

### AnimalCategoryRecognition. 
* 저자들은 AFHQ 데이터셋을 사용함  
* 이 데이터셋은 동물 얼굴로 구성된 15,000개의 이미지를 포함하고 있으며, 512x512 해상도로 되어 있음  
* 데이터셋은 세 가지 범주, 즉 고양이, 개, 야생 동물을 포함하고 있으며, 평가는 보충 자료에서 이루어짐  


## 3.2 AttackDetails  
### Fine-tuningProcessandEvaluation.  
* 식 (5)에서는 λ를 1로 설정함  
* 확산 과정, 미세 조정 과정, 샘플링 과정에서의 반복 단계 수를 각각 sdf, sft, ssp로 표시하며, 이는 각각 40, 15, 40으로 설정됨  
* sft의 경우, [19]에 따르면 6단계만으로도 미세 조정 목적을 충족할 수 있음  
* 실험에서는 저자들의 GPU의 VRAM 제한으로 인해 이를 15로 설정함  
* 그러나 미세 조정 단계를 늘릴수록 의미론적 적대적 이미지는 더 부드러운 수정과 더 높은 이미지 품질을 보여줌  
* 생성된 이미지의 적대성을 보장하기 위해, 미세 조정 과정 전바닉 대상 분류기의 출력을 초기 검증하며, 레이블이 변경되지 않을 경우 마지막 실행을 기반으로 추가적인 15단계의 미세 조정 과정을 반복적으로 수행할 것임  
* 그 후 샘플링 과정을 수행하여 이미지 품질을 향상시킴  

### ConstructingMaskandEvaluation.  
* 원본과 대상 이미지의 각 쌍인 xs_0과 xt_0는 서로 다른 클래스 레이블을 가지고 있으며 대상 분류기에 의해 정확하게 분류될 수 있어야 함  
* Grad-CAM이나 살리언시 맵을 이미지에 적용할 때 RGB 채널에서 m0과 m1을 하나의 채널로 결합한 후 TopK로 필터링하여 대상 이미지에서 원본 깨끗한 이미지로 특징이 어떻게 이식되는지 더 잘 관찰할 수 있음  
* 살리언시 맵의 경우 저자들은 SimpleFullGrad를 사용함  
* Grad-CAM의 경우 원본 구현을 직접 사용함  
* 식 6과 식 8에서 생성된 마스크를 통합한 후 확산 과정이 항상 적대적 예제를 생성하지는 않으므로 각 반복마다 하이퍼파라미터 δ를 감소시켜야 함  

## 3.3 Evaluation Metrics and Benchmarks  
* 저자들은 공격 성공률(ASR), 프레쳇 인셉션 거리(FID) 및 커널 인셉션 거리(KID)를 의미 적대적 공격의 충실도 측정 지표로 사용함  
* FID는 두 데이터 분포 간의 프레쳇 거리를 측정하며, KID는 두 분포 간의 불일치를 측정함  
* 두 측정 모두 낮을수록 좋음  
* 또한, ST 방법의 경우 평균 미세 조정 반복 횟수를 delta_avg로, LM 방법의 경우 성공적인 의미 적대적 공격에 대한 평균 임계값을 eta_avg로 측정함  
* 의미 적대적 공격을 생성하는 데 걸린 평균 시간도 측정하여 효율성을 평가함 

## 3.4 Results  
### Analysis for the ST Approach. 
* CelebA-HQ 데이터셋의 성능을 나타내는 표 1에서 ST 접근 방식을 화이트박스와 블랙박스 설정 하에 사용한 저자들의 프레임워크 통계가 상단에 표시됨  
* 모든 경우에서 거의 100%의 공격 성공률(ASR)을 달성함  
* ST 접근 방식을 사용한 경우, FID와 KID에서 화이트박스 설정에서 생성된 이미지가 블랙박스 설정보다 더 높은 품질을 갖는 것을 명확히 관찰할 수 있음  
* 블랙박스 설정에서는 화이트박스 설정보다 더 많은 변형을 생성하고 생성된 이미지를 원본 이미지와 비현실적이고 다르게 만듦  
* ST 접근 방식으로 생성된 시각적 예제는 그림 3에 있음   

### Analysis for the LM Approach. 
* 저자들은 Grad-CAM 또는 SimpleFullGrad 해석 맵을 사용하여 마스크를 구성함으로써 유사한 결과를 얻었으며, 거의 100%의 공격   * 성공률과 유사한 FID 및 KID를 달성함  
* 대부분의 인간 얼굴에서 얼굴 신원과 관련된 특징은 대개 이미지의 유사한 영역에 위치함(예: 코, 눈, 턱, 이마)  
* 따라서 저자들은 원본과 대상 이미지에 직접 마스크를 적용하여 특징을 이식할 수 있음  

### Comparison with Benchmarks. 
* 벤치마크와의 비교를 위한 시각적 예시는 그림 7에 나타나 있으며, 추가적인 예시는 보충 자료에서 찾을 수 있음  
* FID와 KID 측면에서 저자들의 프레임워크는 화이트박스 설정 하에서 ST 접근 방식을 사용할 때 비교할만한 성능을 달성함  
* 생성된 이미지당 평균 소요 시간으로, 저자들의 프레임워크는 LM 접근 방식을 사용할 때 다른 방법들과 비교하여 가장 좋은 성능을 보여줌  
* LatentHSJA는 대상 이미지에서 시작하여 잠재 공간에서 픽셀을 반복적으로 이식하려고 시도하지만, 고정된 쿼리 수를 가지고 있으며, 원본 이미지와 비교할 때 낮은 유사성을 가짐  
* AttAttack은 수동 속성 주석을 활용하여 성능을 크게 향상시키지만, 다른 데이터셋으로 일반화하는 것은 어려움  
* 또한, 그림 6에서 전이 가능성을 평가함  
* 모든 방법들은 대상 공격을 ResNet18(단, ST 블랙박스를 제외함)에 맞추어 500개의 적대적 이미지를 생성함  
* 이러한 이미지들은 추가 분류기인 ResNet101, MNasNet1.0, DenseNet121에서 ASR을 계산하기 위해 테스트됨  
* 저자들의 ST 블랙박스 접근 방식은 예상대로 다른 방법들 중에서 가장 좋은 성능을 보여줌, 이는 대상 모델로부터 정확한 정보를 요구하지 않기 때문임  
* ST 접근 방식과 비교했을 때, 생성된 적대적 공격은 [23, 34, 36]과 같은 확산 기반 정화 알고리즘에 의해 노이즈가 제거되기 더 어려워야 함   


<br/>
# 4 Conclusion  
* 이 논문에서 저자들은 ST 접근법과 LM 접근법을 사용하여 확산 모델을 활용한 의미 적대적 공격 프레임워크를 처음 제안함  
* ST 접근법은 양성 이미지의 잠재 공간이나 확산 모델의 매개변수를 미세 조정을 통해 조작하는 반면, LM 접근법은 중요도 맵을 통한 잠재 공간의 마스킹을 통해 보다 직접적인 방법으로 잠재 공간을 조작함  
* 실증적 연구에서, 제안된 프레임워크는 다양한 설정에서 두 접근 방식 모두에 대해 탁월한 성능을 달성함  
* 전체적으로 저자들의 프레임워크는 다른 벤치마크와 비교했을 때 훌륭한 일반화성, 효율성, 전이 가능성을 보여주며, 의미 적대적 공격 분야에서 확산 모델의 새로운 사용법을 드러냄  
* 그러나 저자들의 프레임워크에는 한계가 있음  
* ST 접근법의 경우, 블랙박스 설정에서 적대적 이미지의 품질이 화이트박스 설정만큼 좋지 않으며, LM 접근법의 경우, 깨끗한 이미지와 비교했을 때 수정될 마스크 영역을 정확히 제어할 수 없음   



<br/>  
# 요약  
* 
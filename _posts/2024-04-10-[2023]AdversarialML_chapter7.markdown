---
layout: post
title:  "[2023]Adversarial Machine Learning: Attack Surfaces, Defence Mechanisms, Learning Theories in Artificial Intelligence: Chapter7: Adversarial Perturbation for Privacy Preservation"  
date:   2024-04-10 22:07:55 -0400
categories: study
---

{% highlight ruby %}


한줄 요약: 

짧은 요약(Abstract) :    
적대적 예시나 적대적 변형은 보통 보안 위협으로 여겨져 왔으나, 이들은 딥러닝 기반 개인정보 공격에 직면할 때 개인정보 보호 도구로 활용될 수 있음

이 장에서는 시각 데이터를 위한 개인정보 모델을 소개할 것이며, 이는 딥러닝 응용에서 가장 중요한 데이터 유형 중 하나임

다음으로, 저자들은 다양한 개인정보 보호 수준을 통합하는 적대적 변형 기반 개인정보 보호 메커니즘에 대해 논의할 것임

이 주제에 대한 연구가 아직 초기 단계에 있음에도 불구하고, 이 장은 최신 작업을 개관하고 미래 연구에 대한 통찰을 제공할 것임

높은 정확도로 인해 딥러닝 방법은 인터넷의 새로운 AI 기반 서비스의 기반이 되었으며, 이는 명백한 개인정보 보호 문제를 야기함

딥러닝 보조 개인정보 공격은 텍스트뿐만 아니라 이미지와 비디오와 같은 비구조화된 데이터에서도 민감한 개인 정보를 추출할 수 있음

이는 저자들에게 다양한 지능 기술이 등장하는 빅 데이터 시대의 개인정보 보호 문제를 다시 생각해 보게 함

특히, 등장하는 딥러닝 기술은 수백만 개의 사진이나 비디오를 자동으로 수집 및 처리하여 소셜 네트워크에서 개인/민감 정보를 추출할 수 있음

따라서 딥러닝 맥락에서 개인정보 보호 문제를 철저히 조사하는 것이 시급함

대부분의 기존 연구 작업은 적대적 예시나 적대적 변형을 시스템 보안을 위협하는 공격 방법으로 간주했으나, 적대적 변형은 딥러닝 기반 개인정보 공격에 직면했을 때 개인정보 보호 도구로도 사용될 수 있음

적대적 변형의 기본 아이디어는 원본 이미지에 작지만 의도적인 최악의 경우의 방해를 생성하여, 인간의 눈에 눈에 띄는 중요한 차이를 일으키지 않으면서도 CNN 기반 인식 모델을 오도하는 것임

따라서 개인정보 공격에 대학 적대적 변형 기반 개인정보 보호 메커니즘을 설계하는 것이 가능함

최근 이미지 개인정보 보호를 위한 방법으로 적대적 변형을 사용하는 몇 가지 연구가 있었음

저자들은 자동 감지에 반대하는 알고리즘을 제안했으며, 이는 'Faster RCNN 프레임워크'를 기반으로 함

저자들은 게임 이론 프레임워크를 설정하고 개인정보 보호를 위한 적대적 이미지 변형의 효과성을 연구함

저자들은 단 하나의 독성 이미지만으로도 분류기의 행동을 제어할 수 있는 최적화 기반 방법을 제시함

저자들은 AttriGuard라는 두 단계 프레임워크를 제안하여, 분류기가 시작하는 속성 추

론 공격에 대항함

저자들은 머신러닝 시스템에서 이미지로부터 민감한 정보를 식별하지 못하게 하는 적대적 예시 사용 방안을 조사함

저자들은 얼굴 비식별화를 위해 적대적 변형을 사용할 것을 제안함

저자들은 모자에 주의 깊게 계산된 적대적 스티커가 착용자의 인식 가능성을 줄일 수 있음을 보여줌

저자들은 특성 공간에서 목표 이미지를 둘러싼 독성 이미지를 설계하는 새로운 '다면체 공격'을 소개함

저자들은 거리 이미지에서 여러 개인 객체를 보호하기 위해 적대적 변형을 사용할 것을 제안함

저자들은 식별 분류기를 위한 개인정보 보호 가능한 공유 가능한 의료 텍스트의 표현을 제안함

Fawkes는 사용자가 자신의 사진에 인식할 수 없는 '망토'를 입혀서 공개하기 전에 도움을 줌

이러한 '망토'가 포함된 이미지로 훈련된 얼굴 인식 모델은 사용자의 일반 이미지를 지속적으로 잘못 식별하는 기능적 모델을 생성함

거의 모든 기존 AP 기반 개인정보 보호 연구가 시각 데이터, 특히 이미지 데이터에 초점을 맞추고 있기 때문에, 이 장의 토론도 이미지와 비디오의 맥락에서 이루어질 것임

먼저 시각 데이터에서 개인정보 모델을 간략히 정의한 다음, 세 가지 다른 그룹의 AP 기반 개인정보 보호 메커니즘을 소개할 것임  

Useful sentences :  
*   


{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1p1MrsOhGYsfzcphgMb3DdRDPq2tQpOSm?usp=sharing)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* 

<br/>
# 7.1 Adversarial Perturbation for Privacy Preservation
이들의 독보적인 정확성으로 인해 딥러닝 방법들은 인터넷상의 새로운 AI 기반 서비스의 기반이 되었으며, 이는 명백한 개인정보 보호 문제를 야기함

딥러닝 보조 개인정보 공격은 텍스트뿐만 아니라 이미지와 비디오 같은 비구조화된 데이터에서도 민감한 개인 정보를 추출할 수 있음

이는 저자들에게 다양한 지능 기술이 등장하는 빅 데이터 시대의 개인정보 보호 문제를 다시 생각해 볼 필요성을 상기시킴

특히, 등장하는 딥러닝 기술은 수백만 개의 사진이나 비디오를 자동으로 수집 및 처리하여 소셜 네트워크에서 개인 또는 민감 정보를 추출할 수 있음

따라서 딥러닝 맥락에서 개인정보 보호 문제를 철저히 조사하는 것이 시급함

기존의 대부분 연구 작업은 적대적 예시나 적대적 변형을 시스템 보안을 위협하는 공격 방법으로 간주했으나, 적대적 변형은 딥러닝 기반 개인정보 공격에 직면했을 때 개인정보 보호 도구로도 사용될 수 있음

적대적 변형의 기본 아이디어는 원본 이미지에 작지만 의도적인 최악의 경우의 방해를 생성하여, 인간의 눈에 눈에 띄는 중요한 차이를 일으키지 않으면서도 CNN 기반 인식 모델을 오도하는 것임

따라서 개인정보 공격에 대항하는 적대적 변형 기반 개인정보 보호 메커니즘을 설계하는 것이 가능함

최근 이미지 개인정보 보호를 위한 방법으로 적대적 변형을 사용하는 몇 가지 연구가 있었음

저자들은 자동 감지에 반대하는 알고리즘을 제안했으며, 이는 'Faster RCNN 프레임워크'를 기반으로 함

저자들은 게임 이론 프레임워크를 설정하고 개인정보 보호를 위한 적대적 이미지 변형의 효과성을 연구함

저자들은 단 하나의 독성 이미지만으로도 분류기의 행동을 제어할 수 있는 최적화 기반 방법을 제시함

저자들은 AttriGuard라는 두 단계 프레임워크를 제안하여, 분류기가 시작하는 속성 추론 공격에 대항함

저자들은 머신러닝 시스템에서 이미지로부터 민감한 정보를 식별하지 못하게 하는 적대적 예시 사용 방안을 조사함

저자들은 얼굴 비식별화를 위해 적대적 변형을 사용할 것을 제안함

저자들은 모자에 주의 깊게 계산된 적대적 스티커가 착용자의 인식 가능성을 줄일 수 있음을 보여줌

저자들은 특성 공간에서 목표 이미지를 둘러싼 독성 이미지를 설계하는 새로운 '다면체 공격'을 소개함

저자들은 거리 이미지에서 여러 개인 객체를 보호하기 위해 적대적 변형

을 사용할 것을 제안함

저자들은 식별 분류기를 위한 개인정보 보호 가능한 공유 가능한 의료 텍스트의 표현을 제안함

Fawkes는 사용자가 자신의 사진에 인식할 수 없는 '망토'를 입혀서 공개하기 전에 도움을 줌

이러한 '망토'가 포함된 이미지로 훈련된 얼굴 인식 모델은 사용자의 일반 이미지를 지속적으로 잘못 식별하는 기능적 모델을 생성함

거의 모든 기존 AP 기반 개인정보 보호 연구가 시각 데이터, 특히 이미지 데이터에 초점을 맞추고 있기 때문에, 이 장의 토론도 이미지와 비디오의 맥락에서 이루어질 것임

먼저 시각 데이터에서 개인정보 모델을 간략히 정의한 다음, 세 가지 다른 그룹의 AP 기반 개인정보 보호 메커니즘을 소개할 것임

<br/>
# 7.1.1 Visual Data Privacy Model
개인정보 보호 방법을 논의하기 전에, 이미지와 비디오 개인정보를 명확히 하고 모델링하는 것이 중요함

GDPR에서 정의된 바와 같이, 개인정보는 개인 신원과 관련된 것으로 정의됨

이런 의미에서, 단일 수준의 개인정보 모델은 항상 필요하지 않으며, 이미지나 비디오에 충분하지도 않음

예를 들어, 개인의 얼굴을 포함하는 거리 뷰 이미지는 전체적으로 개인적임, 그러나 많은 비개인적 정보도 포함하고 있음

이 경우, 이미지 수준 개인정보를 사용하는 것은 실제 사용에 너무 강할 수 있음

얼굴을 익명화할 수 있다면, 전체 이미지는 거리 뷰 서비스의 일부로 사용될 수 있음

따라서 다중 수준 시각 개인정보 모델을 사용하는 것이 더 일반적임

이 아이디어는 다음과 같이 세 수준의 개인정보 모델을 정의하는 것임:

파일 수준 개인정보: 이미지나 비디오

객체 수준 개인정보: 얼굴, 사람, 자동차 등

특징 수준 개인정보: 신원, 외모, 자세 등

첫 번째에서 세 번째 수준까지, 모델은 조잡한 것에서 세밀한 것으로 변경됨

이 다중 수준 개인정보 모델을 기반으로, 저자들은 기존 AP 기반을 세 그룹으로 나누고, 각각에 대해 다음 소절에서 논의할 것임

<br/>
# 7.1.2 Privacy Protection Mechanisms Using Adversarial Perturbations
파일 수준 개인정보 보호의 경우, 저자들은 딥러닝 도구를 잘못된 이미지 클래스로 오도하는 것을 목표로 함

소셜 네트워크의 시나리오를 고려해 보면, 사용자들이 소셜 네트워크 플랫폼에 이미지를 게시함

공격자가 크롤러를 통해 이미지를 수집하고 DNN을 사용하여 민감한 정보를 채굴한다고 가정해 보자

그림 7.1은 이러한 시스템 아키텍처의 예를 보여줌

사용자가 원본 이미지에 아무런 전처리 없이 소셜 네트워크에 이미지를 공유하면, DNN을 갖춘 적이 자동으로 이 이미지에서 유용한 정보를 얻을 수 있음(즉, 이것은 높은 확신을 가진 거대 판다로, 동물원 방문 가능성이 높음을 나타냄)

사용자의 활동, 위치 또는 심지어 이름과 같은 다른 민감한 정보도 유사한 강력한 딥러닝 모델로 감지될 수 있음

개인정보 유출을 방지하기 위해, 저자들은 원본 이미지에 적대적 변형을 추가할 것임으로써, 공개된 이미지가 DNN 모델을 잘못된 정보로 오도하게 할 것임

동시에, 저자들은 노이즈를 가능한 한 작게 유지하여 이미지 품질과 사용자 경험에 미치는 영향을 최소화하기를 바람

파일 수준 개인정보 보호 문제는 공격자에 의해 변형된 이미지가 올바르게 분류될 확률을 최소화하는 최적화 문제로 정의될 수 있음

적대적 예시에 대한 노이즈를 생성하는 다양한 방법이 있으며, 그 중에서 가장 널리 사용되는 것은 빠른 그래디언트 부호 방법(FGSM)임

이러한 파일 수준 개인정보 보호의 결과 예를 그림 7.2에서 볼 수 있음

딥러닝 모델은 원본 이미지를 '미니버스'로 분류하는데 높은 확신(92.42%)을 가지고 있음

그리고 저자들이 FGSM을 사용하여 작은 노이즈를 추가하면, 이는 '세면대'로 잘못 분류될 것이며, 심지어 더 높은 확신(99.37%)을 가질 것임

기존 연구 결과에 따르면, AP 기반 방법은 인간의 눈에 띄지 않을 정도의 소량의 노이즈를 추가하는 비용으로 딥러닝 도구에 대해 좋은 개인정보 보호를 달성할 수 있음을 보여줌

특히 복잡한 구조와 질감을 가진 이미지에서 제안된 방법의 효과성이 특히 좋음

객체 수준 개인정보 보호는 단일 주요 객체만 포함하는 간단한 이미지에 적합함

실제로, 특히 소셜 네트워크 이미지에서, 주어진 이미지에는 일반적으로 여러 개체가 있으며, 그 중 일부는 개인정보에 민감한 반면 다른 일부는 개인정보에 민

감하지 않을 수 있음

이 경우, 저자들은 문제를 해결하기 위해 객체 수준 개인정보 보호 프레임워크를 사용할 수 있음

그림 7.3에서 보여주는 바와 같이, 프레임워크는 두 가지 주요 단계로 구성될 수 있음: (i) 이미지에서 개인정보 객체 식별, (ii) 적대적 변형을 사용한 이미지 개인정보 보호

첫 번째 단계에서는 DNN 기반 객체 감지기를 사용할 수 있음

입력 이미지 X가 주어지면, 객체 감지 모듈의 출력은 표현될 수 있음

이때, xi, yi, wi, hi는 각각 앵커의 왼쪽 상단 x좌표, y좌표, 너비, 높이를 나타냄

i는 관심 영역(ROI)의 인덱스로, 이미지의 객체 수와 동일함

cj는 클래스 레이블(예: 고양이, 개, 얼굴)임

많은 객체 감지기, 예를 들어 Faster RCNN 같은 것은 배경을 하나의 클래스로 취급한다는 점에 유의해야 함

임계값은 나타날 수 있는 인식할 수 없는 영역을 처리하는 데 사용됨

모든 클래스의 확률이 임계값보다 낮으면 배경으로 인식됨

그런 다음 GDPR에 따라 무엇이 개인정보 객체인지를 정의함

개인 신원 - 차량 번호판, 전화번호, 주소 등

생체 인식 - 얼굴, 달력 데이터, 지문, 망막 스캔, 사진 등

전자 기록 - 쿠키, IP 위치, 모바일 기기 ID, 소셜 네트워크 활동 기록 등

이 정의에 따라, 객체 감지 출력의 모든 클래스는 두 개의 하위 집합으로 나누어짐: Cprivate는 개인정보 클래스 집합이고, Cnon-private는 비개인정보 클래스 집합임

그런 다음 두 번째 단계에서, 개인정보 객체를 대상으로 하는 작은 적대적 변형 δX가 적용되어 개인정보가 없는 이미지 Xpr = X + δX를 생성하여, 객체 감지기를 통과할 때 비개인정보만 감지될 수 있도록 함

이상에서 설명한 프레임워크를 기반으로, 저자들의 목표는 개인정보 객체의 클래스를 배경으로 변경하여 네트워크를 속이는 것이며, 비개인정보 객체는 원래 클래스로 인식되어야 함

동시에 추가된 노이즈 δX는 인간에게 눈에 띄지 않을 정도로 작아야 함

따라서 문제는 다음과 같이 공식화될 수 있음

AP 기반 이미지 개인정보 보호 알고리즘은 위의 문제를 해결하는 데 사용될 수 있음

그림 7.3에서 보여주는 바와 같이, 객체 감지기는 처음에 이미지의 모든 객체를 찾음

그런 다음 개인정보 객체의 레이블을 배경으로 교체하고 해당 손실 함수를 사용하여 그래디언트를 계산함

그런 다음 노이즈는 그래디언트에 따라 업데이트됨

마지막으로, 모든 개인정보 객체가 객체 감지기에 의해 배경으로 처리되는 변형된 이미지가 생성됨

알고리즘의

 핵심 부분은 개인정보 객체를 배경으로 인식하도록 객체 감지기를 오도하는 분류 손실을 속이는 것임, 식 (7.1)에서 보여주는 것처럼:

여기서 P(c|a)는 앵커의 내용이 각 클래스로 인식될 확률임

P(y|a)는 원-핫 인코딩되어 있으며, 정확한 클래스로 설정한 위치에 1이 나타남

P(y|a)는 객체가 비개인정보인 경우 ground truth 레이블에 따라 생성되며, 객체가 개인정보인 경우 배경으로 변경됨

n은 이미지의 객체 총수이므로, 엔트로피는 모든 앵커에 대해 평균화됨

다음으로, 빠른 그래디언트 부호 방법(FGSM)을 사용하여 변형을 생성할 수 있음

대상 FGSM을 사용하여, 변형은 그래디언트의 방향으로 계산될 수 있음:

여기서 ε는 노이즈를 조절하는 단계 매개변수임

따라서 생성된 이미지는 다음과 같이 됨


<br/>
# 7.1.2.1 File-Level Privacy Protection 
파일 수준 개인정보 보호의 경우, 저자들은 딥러닝 도구를 잘못된 이미지 클래스로 오도하는 것을 목표로 함

소셜 네트워크의 시나리오를 고려해 보면, 사용자들이 소셜 네트워크 플랫폼에 이미지를 게시함

공격자가 크롤러를 통해 이미지를 수집하고 DNN을 사용하여 민감한 정보를 채굴한다고 가정해 보자

그림 7.1은 이러한 시스템 아키텍처의 예를 보여줌

사용자가 원본 이미지에 아무런 전처리 없이 소셜 네트워크에 이미지를 공유하면, DNN을 갖춘 적이 자동으로 이 이미지에서 유용한 정보를 얻을 수 있음(즉, 이것은 높은 확신을 가진 거대 판다로, 동물원 방문 가능성이 높음을 나타냄)

사용자의 활동, 위치 또는 심지어 이름과 같은 다른 민감한 정보도 유사한 강력한 딥러닝 모델로 감지될 수 있음

개인정보 유출을 방지하기 위해, 저자들은 원본 이미지에 적대적 변형을 추가할 것임으로써, 공개된 이미지가 DNN 모델을 잘못된 정보로 오도하게 할 것임

동시에, 저자들은 노이즈를 가능한 한 작게 유지하여 이미지 품질과 사용자 경험에 미치는 영향을 최소화하기를 바람

파일 수준 개인정보 보호 문제는 공격자에 의해 변형된 이미지가 올바르게 분류될 확률을 최소화하는 최적화 문제로 정의될 수 있음

적대적 예시에 대한 노이즈를 생성하는 다양한 방법이 있으며, 그 중에서 가장 널리 사용되는 것은 빠른 그래디언트 부호 방법(FGSM)임

이러한 파일 수준 개인정보 보호의 결과 예를 그림 7.2에서 볼 수 있음

딥러닝 모델은 원본 이미지를 '미니버스'로 분류하는데 높은 확신(92.42%)을 가지고 있음

그리고 저자들이 FGSM을 사용하여 작은 노이즈를 추가하면, 이는 '세면대'로 잘못 분류될 것이며, 심지어 더 높은 확신(99.37%)을 가질 것임

기존 연구 결과에 따르면, AP 기반 방법은 인간의 눈에 띄지 않을 정도의 소량의 노이즈를 추가하는 비용으로 딥러닝 도구에 대해 좋은 개인정보 보호를 달성할 수 있음을 보여줌

특히 복잡한 구조와 질감을 가진 이미지에서 제안된 방법의 효과성이 특히 좋음

<br/>
# 7.1.2.2 Object-Level Privacy Protection
파일 수준 개인정보 보호는 단일 주요 객체만 포함하는 간단한 이미지에 적합함

실제로, 특히 소셜 네트워크 이미지에서, 주어진 이미지에는 일반적으로 여러 개체가 있으며, 그 중 일부는 개인정보에 민감한 반면 다른 일부는 개인정보에 민감하지 않을 수 있음

이 경우, 저자들은 문제를 해결하기 위해 객체 수준 개인정보 보호 프레임워크를 사용할 수 있음

그림 7.3에서 보여주는 바와 같이, 프레임워크는 두 가지 주요 단계로 구성될 수 있음: (i) 이미지에서 개인정보 객체 식별, (ii) 적대적 변형을 사용한 이미지 개인정보 보호

첫 번째 단계에서는 DNN 기반 객체 감지기를 사용할 수 있음

입력 이미지 X가 주어지면, 객체 감지 모듈의 출력은 표현될 수 있음

이때, xi, yi, wi, hi는 각각 앵커의 왼쪽 상단 x좌표, y좌표, 너비, 높이를 나타냄

i는 관심 영역(ROI)의 인덱스로, 이미지의 객체 수와 동일함

cj는 클래스 레이블(예: 고양이, 개, 얼굴)임

많은 객체 감지기, 예를 들어 Faster RCNN 같은 것은 배경을 하나의 클래스로 취급한다는 점에 유의해야 함

임계값은 나타날 수 있는 인식할 수 없는 영역을 처리하는 데 사용됨

모든 클래스의 확률이 임계값보다 낮으면 배경으로 인식됨

그런 다음 GDPR에 따라 무엇이 개인정보 객체인지를 정의함

개인 신원 - 차량 번호판, 전화번호, 주소 등

생체 인식 - 얼굴, 달력 데이터, 지문, 망막 스캔, 사진 등

전자 기록 - 쿠키, IP 위치, 모바일 기기 ID, 소셜 네트워크 활동 기록 등

이 정의에 따라, 객체 감지 출력의 모든 클래스는 두 개의 하위 집합으로 나누어짐: Cprivate는 개인정보 클래스 집합이고, Cnon-private는 비개인정보 클래스 집합임

그런 다음 두 번째 단계에서, 개인정보 객체를 대상으로 하는 작은 적대적 변형 δX가 적용되어 개인정보가 없는 이미지 Xpr = X + δX를 생성하여, 객체 감지기를 통과할 때 비개인정보만 감지될 수 있도록 함

이상에서 설명한 프레임워크를 기반으로, 저자들의 목표는 개인정보 객체의 클래스를 배경으로 변경하여 네트워크를 속이는 것이며, 비개인정보 객체는 원래 클래스로 인식되어야 함

동시에 추가된 노이즈 δX는 인간에게 눈에 띄지 않을 정도로 작아야 함

따라서 문제는 다음과 같이 공식화될 수 있음

AP 기반 이미지 개인정보 보호 알고리즘은 위의 문제를 해결하는 데 사용될 수 있음

그림 7.3에서 보여주는 바와 같이, 객체 감지기는 처음에 이미지의 모든 객체를 찾음

그런 다음 개인정보 객체

의 레이블을 배경으로 교체하고 해당 손실 함수를 사용하여 그래디언트를 계산함

그런 다음 노이즈는 그래디언트에 따라 업데이트됨

마지막으로, 모든 개인정보 객체가 객체 감지기에 의해 배경으로 처리되는 변형된 이미지가 생성됨

알고리즘의 핵심 부분은 개인정보 객체를 배경으로 인식하도록 객체 감지기를 오도하는 분류 손실을 속이는 것임, 식 (7.1)에서 보여주는 것처럼


<br/>
# 7.1.2.3 Feature-Level Privacy Protection
다른 경우에는 이미지나 비디오에서 특정 특징만 변경해야 할 수도 있음

이는 사람의 인지할 수 없는 적대적 변형을 사용하는 것을 의미함

전형적인 예로는 이미지에서 사람의 신원을 변경하면서 (얼굴 인식 시스템에 반대하여) 외모는 시각적으로 변경되지 않게 하는 경우가 있음

얼굴 인식 시스템은 이미지나 비디오 프레임에서 사람을 인식하거나 인증할 수 있는 기술임

최근 발전된 딥러닝 신경망 덕분에, 인공 지능 기반 얼굴 인식 시스템의 정확도는 일부 벤치마크 테스트에서 인간의 정확도를 초과하기 시작함

결과적으로, 접근 제어 및 보안 모니터링과 같은 많은 응용 프로그램에서 더 넓은 범위의 사용을 시작하게 됨

그림 7.4는 전형적인 얼굴 인식 시스템을 묘사하고 있음

입력 이미지가 수신되면, 먼저 얼굴의 위치를 감지하고 시스템 설정과 일치하는 크기로 얼굴을 자름

DNN은 얼굴 이미지에서 얼굴 임베딩(얼굴 특징을 나타내는 수치 벡터)을 계산하는 데 사용됨

그런 다음 시스템은 입력 얼굴의 임베딩과 시스템 데이터베이스의 주어진 임베딩 사이의 거리를 계산할 수 있음

거리는 두 개의 소프트 값이 포함된 벡터로 변환되어 얼굴 인식 결과를 나타냄: 첫 번째 값이 두 번째 값보다 크면 두 임베딩은 같은 사람의 이미지에서 비롯된 것임

그렇지 않으면, 그들은 두 다른 사람의 이미지임

개인정보 보호 관점에서, 저자들은 원본 이미지에 노이즈를 추가하여 얼굴 인식 시스템이 사람을 올바르게 식별하지 못하게 하는 것을 목표로 함

보다 구체적으로, 개인정보 보호 성공률 지표에 기반하여, 제안된 이미지 개인정보 보호 문제는 다음과 같이 공식화될 수 있음

적대적 변형은 FGSM 알고리즘 또는 더 강력한 방법, 즉 음수 손실 함수에서의 투영된 그래디언트 하강(PGD)인 FGSMN의 다단계 변형으로 생성될 수 있음

PGD에서, FGSM은 N회 반복되거나 노이즈의 절대값이 사전에 정의된 상한에 도달할 때까지 반복됨

그림 7.4에서 보여주듯이, 과정의 일러스트레이션은 다음과 같음

처음에는 특정하게 또는 무작위로 다른 사람이 선택됨

그런 다음 이 적대적 얼굴의 임베딩 벡터가 계산되고 y의 값으로 사용됨

이미지는 PGD 알고리즘으로 적대적 변형을 가하여 생성되며, 마지막으로 얼굴 인식 시스템을 사용하여 테스트됨

<br/>
# 7.1.3 Discussion and Future Works
AP 기반 방법들은 인지할 수 없는 노이즈 수준에서도 개인정보 보호의 뛰어난 효과를 보여주었으나, 현재 이 그룹의 방법들에는 두 가지 주요 문제가 있음

첫 번째는 이러한 방법들이 대상 시스템에 대한 접근성에 크게 의존한다는 것으로, 타겟 특정 인식기(즉, 화이트박스 지식이 필요함)에 대해서만 보장될 수 있음

두 번째는 적대적 변형의 이전 가능성, 즉 대상 모델에 대한 효과와 비교하여 대안적인 알려지지 않은 모델에 대한 효과가 그리 좋지 않다는 것임

위의 문제들을 해결하기 위해, 일부 논문들은 노이즈 방향의 계산을 모델의 출력 레이어에서 중간 레이어로 옮김

이는 모델 간의 차이를 피함으로써 이전 가능성을 증가시킬 수 있음

Pidhorskyi 논문은 이미지의 특징 수준에서 적대적 변형을 추가하는 잠재력을 연구함

서로 다른 DNN 모델이 특징 수준에서 유사한 출력을 가지고 있기 때문에, 이는 또한 이전 가능성을 증가시킬 것임

개인정보 보호 관점에서, 몇몇 다른 메커니즘들도 있음

예를 들어, 몇몇 연구자들은 이미지에서 민감한 정보를 대체하기 위해 콘텐츠를 생성하기 위해 GAN을 사용하기 시작함

Sun 등은 원래 신원을 제거하기 위해 GAN 기반 헤드 인페인팅을 제안함

또한, 최근에 이미지 개인정보와 DP 개념을 결합하려는 몇 가지 시도가 있었음

Fan은 이미지의 픽셀 수준에서 ε-차등 개인정보 보호 방법을 제안함

그러나 실제로 이미지 픽셀을 구별할 수 없게 만드는 것은 큰 의미가 없으며 생성된 이미지의 품질이 매우 낮음

다양한 개인정보 보호 메커니즘을 비교하는 것은 흥미로운 주제가 될 것임

마지막으로, 이미지 개인정보 보호의 첫 단계 이후, 비디오 개인정보 보호에 대한 연구도 시작되었음

기존의 이미지 개인정보 보호 방법을 비디오에 직접 적용하면 높은 계산 복잡성과 큰 지연이 발생하므로, 보다 효율적인 비디오 개인정보 보호 메커니즘을 설계하는 것도 유망한 연구 방향임



<br/>  
# 요약  
* 
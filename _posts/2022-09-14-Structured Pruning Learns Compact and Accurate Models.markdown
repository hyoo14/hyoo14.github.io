---
layout: post
title:  "Structured Pruning Learns Compact and Accurate Models"
date:   2022-09-14 00:02:19 +0900
categories: study
---





{% highlight ruby %}
짧은 요약 :  

모델 압축이 요지(아래 방법론 사용)  
-가지치기  
-정제  


가지치기만 사용할 경우 사이즈는 작아지나 속도는 느림  
정제 사용 경우 속도는 빨라지지만 데이터가 많이 필요하고 학습능력도 필요  

CoFi제안 (Coarse-graind and fined-grained prunning)  
-coarse : 많은 양의 데이터, 적게 보냄(레이어 전체 등 큰 단위)  
-fined : 적은 양의 데이터, 많이 보냄(어텐션의 헤드나 히든디멘션 처럼 위보다 작은 단위)  


CoFi는 병렬네트워크이고 정제메서드와 매칭되서 정확도와 레이턴시가 좋고, 언라벨 재분류가 필요 없음  


세분화 mask로 가치치기 디시전하고, layerwise distilation 하기 때문에 최적화됨  


GLUE, SQuAD 실험에서 속도 10배 빠르고, 성능은 90%로 방어함  
(이전 모델들 압도)  


{% endhighlight %}


링크 (https://drive.google.com/drive/folders/1ZLbPbKFeexbTR-VJ6mvw6w40i3wGMiGS?usp=sharing)


# 단어정리  
*granularity : 세분성/얼마나 자세하게 분할되는지, account for : ~에 대해/설명하다, interleaving : 끼워넣다,   
task-agnostic : 작업에 구애받지 않는, quantization : 양자화  

# 1 Introduction  
*PLM이 NLP의 주류인데 용량-메모리-연산시간이 매우 큼  
**압축이 필요하고  
***압축방법으로 가지치기와 정제가 있음  


*가지치기는 정확한(성능이 유지되는) sub네트워크 찾는 것  
**layer or head or dimension or blocks(가중행렬) 줄여보는 것  
**위와 같은 fine-grained unit 줄여보는 것이 트렌드  
**성능 향상은 적음 (2,3배 속도 향상)  


*정제모델은 고정 구조로 정제  
**라벨 없는 코퍼스 사용  
**잘 설계된 구조의 학생 모델은 속도-성능 트레이드 오프에서 좋은 균형 잘 찾음  
**근데 학습이 느림  


*특정 작업 위주의 구조화 가지치기인 CoFi 제안  
**거친(self어텐션 전체, FF layer) + 다듬어진(헤드, 히든디멘션) 동시에 가지치기함  
**가지치기 졀정은 매우 세분화해서 함  
**유연성 좋고 최적화 잘 됨  


*가지치기+정제: 성능 올라감  
**고정 학생 구조 대신 레이어와이즈 정제 기법 사용  
***두 구조 사이 역동적 학습  
****성능 잘 나옴  


*CoFi가 정확하고 속도 좋음(GLUE and SQuAD)  
**작고 빠름  



# 2 Background  
# 2.1 Transformer  
*트랜스포머 네트워크 구성  
*L개의 블럭  
**블록당 MHA와 FFN으로 구성  
***MHA에는 쿼리, 키, 벨류, 아웃풋을 인풋으로 하는 어텐션 펑션으로 구성(d:히든레이어)  


*FF레이어  
**up projection(Wu)과 down projection(Wd)으로 구성  
**FFN(x) = gelu(XW0) . Wd  
(MHA와 FFN 이후에 레이어 정규화 있음)  


*MHA, FFN이 각각 모델 파라미터의 1/3, 2/3 비중  
*둘의 GPU타임은 비슷하지만 FFN의 경우 CPU 정체 걸림  


# 2.2 Distillation  
*지식 정제: 전이 지식 압축 모델  
**라벨 없는 데이터로 학습(지식 전이 데이터)  
**2개 합치면 성능 좋음 대신 학습이 비쌈  


*새로운 정제 모델  
기존: 획정된 구조  
신규: 다이나믹(변화) 구조  



# 2.3 Pruning  
*가지치기-반복적 파라미터 제거  
**기존 : 트렌스포머 모델, 다른 컴포넌트에 focus  


*레이어 가지치기  
**트렌스포머 블락들 다 제거  
**경험적으로 50% 드랍 가능  
**스피드 2배 향상  


*헤드 가지치기  
**헤드 제거  
**1.4 배 스피드 향상  


*FFN 가지치기  
**fine-grained 레벨  


*블럭&비구조화 가지치기  
**최적화 어렵고  
**스피드 향상 어려움  


*정제와 같이 가지치기  
**예측 레이어 정제와 동시에 이뤄짐  
**레이어 와이즈에서 어떻게 정제될지 불명확함  



# 3 Method  
*CoFi : 가지치기(거친+다듬어진) + 정제(레이어와이즈-지식전이이용) = 속도 향상, 압축 잘됨  


# 3.1 Coarse- and Fine-Grained Pruning  
*최근 구조적 가지치기: 적은 유닛 가지치기로 유연성 향상  
**파인 그레인 가지치기는 거친 가지치기 수반함  
***head 가지치기 = MHA 가지치기에 수렴  
****최적화가 어려움  


*해법: MHA, FFN 파인그레인 가지치기되게끔 마스킹 추가  
**Z_MHA, Z_FFN 마스킹 추가  


*레이어마스킹 - 전체레이어 외적 가지치기(헤드(MHA) 전체 가지쳐버리는 방식 방지)  
**MHA, FFN 따로 따로  


*output 차원도 가지치기(히든디멘션 차원) - 조금 가지치기되지만 성능은 향상됨  

*data granularity : 데이터가 얼마나 자세히 분할되는지  


*CoFi는 multiple mask 변수가 함께 가지치기 결정함(하나의 파라미터로)  
**ex) FFN 가지치기 전체 or 초기차원과 일치 or 히든차원      대신 각각 MHA, FFN 따로  


*마스크 변수 함수 - l0 정규화(구체적 분포 사용)  
**바닐라 l0 데체(라그랑쥬 멀트플라이어 사용)  
**sparsity 함수 상요(다른 분할마다)  



# 3.2 Distillation to Pruned Models  
*이전 정제+가지치기 -> 성능향상  
**크로스엔트로피 loss(가지친 학생과 가지 안 친 스승 output 분표 비교) 사용  
**인터미디어트 레이어에 큰 이점  
**학생 모델 아키텍처는 사전정의됨  
**구조 바꾸는 건 챌린징함(어려움)  


*레이어와이즈 정제 접근  
**다이나믹 구조  
**학생이 가지쳐지면서 teacher output과 비슷하게(대략) 이런 컨셉  



# 4 Experiments  
# 4.1 Setup  
*데이터셋  
**GLUE and SQuAD 사용  
***GLUE에 SST, MNLI, QQP, QNLI, MRRPC, CoLA, STS-B, RTE 포함  


*sparsity 정의  
**가지친 파라미터 개수 / full 모델 사이즈  
**파인튠 정제가 목적함수  
**지속학슴-가지치기 목적함수, sparsity 선형 항샹  
**파인튠(수렴때까지)  
**시작은 버트베이스  


*베이스라인  
**디스틸버트, 타이니버트, 다이나버트, 블록프루닝  
**다른 프루닝도 씀 : FLOP, 레이어드랍, 무브먼트푸루닝  
**정제-모바일버트, 오토타이니버트  


*타이니, 다이나버트  
**task 특화, 데이터 증폭 사용하여 학습함  
**공평 비교 위해 릴리즈된 코드, data 증폭 없이 테스트, GLUE/SQuAD checkpoint 사용  


*속도  
**가지치기 없는 버트베이스가 베이스라인  
**하드웨어-엔비디아 V100 GPU  
**GLUE(128 input)  
**SQuAD(384input)  
**배치사이즈:128  


# 4.2 Main Results  
*CoFi 성능  
**추론속도, 모델사이즈에서 CoFi의 성능이 좋음  


*CoFi가 더 경제적이고 효율적(타이니버트 + 일반적 정제 보다)  
*데이터 증폭 갖고 실험할 때도 CoFi가 압도적  


# 4.3 Ablation Study  
*가지치기에서 요소 제거 테스트  
**히든차원 가지치기 없을 경우 약간 속도 올라가나 성능 떨어짐  
**레이어마스트 없는 경우 스피드 떨어지고 압축은 잘되지만 최적화 안됨  
**위 다 필요하다는 결론  


*정제 목적 함수 제거 test  
**정제 제거시 성능 떨어짐 1.9-6.8점  
**히든레이어 정제, fixed도 해보니 제거시 성능 떨어짐  


# 4.4 Structures of Pruned Models  
*가지치기 구조 sparsity 60, 70, 80, 90, 95%로 실험해봄  
**FFN 60% 일때 71% 성능감소, MHA 39% 성능 감소  
**MHA가 FFN보다 더 중요  
**MHA와 FFN 인접할 때 성능 더 좋음  


# 5 Related Work  
*구조 가지치기 CV서 많이 쓰임(cnn의 기본)  
*로또 티켓 가설에서 비구조적 가지치기가 주류임  
**sparsity높여주는(속도 높여주지는 않음)  
*DeepSparse는 CPU 추론 엔진 성능과 sparse올려줌  
*언스트럭처 푸루닝은 작업특화적  
**여기에서는 작업에 구애받지 않게 구성  
*스피드 올리는 다양방법 있음  
**정제, 양자화, 다이나믹 추론 가속, 행렬 분해 등  


# 6 Conclusion  
*CoFi 스트록처 프루닝(트렌스포머의 MHA/FFN 레이어, 헤드, 히든디멘션)  + distillation 목적학습 = 속도 10배 향상  
*CoFi는 특정작업 상관없이 잘 적용됨  


*본 연구가 라지 모델이 더 가볍고 유연하게 빨리 학습되도록 하는 미래 연구에 응용되길 소망함
---
layout: post
title:  "[2025]The BIGGEN BENCH: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models"  
date:   2025-05-03 17:12:40 -0400
categories: study
---

{% highlight ruby %}


í•œì¤„ ìš”ì•½: 

9ê°€ì§€ í•µì‹¬ ëŠ¥ë ¥ì„ ëŒ€ìƒìœ¼ë¡œ 77ê°œì˜ ë‹¤ì–‘í•œ ê³¼ì œë¥¼ ë§Œë“¤ì—ˆë‹¤í•¨.. ì´ë¡ ì— ê·¼ê±°í•˜ê¸° ë³´ë‹¤ëŠ” LLMê²½í—˜ê³¼ í•©ì˜ì— ì˜ì¡´   
ì¸ìŠ¤í„´ìŠ¤ë§ˆë‹¤ ì„¸ë¶€ ë£¨ë¸Œë¦­ì„ ì •êµí•˜ê²Œ í‰ê°€í•´ë³´ë ¤ëŠ” ì‹œë„ê°€ ì‹ ì„ í•˜ë‹¤í•¨   
ê·¸ë¦¬ê³  í‰ê°€ì LLMì„ (í”„ë¡œë©”í…Œìš°ìŠ¤, 2024) ì‚¬ìš©    



ì§§ì€ ìš”ì•½(Abstract) :    



ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í‰ê°€í•˜ê¸° ìœ„í•œ **BIGGEN BENCH**ë¼ëŠ” ì²´ê³„ì ì´ê³  ì •ë°€í•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë“¤ì´ ì£¼ë¡œ 'ë„ì›€ì´ ë˜ëŠ”ê°€'ì™€ ê°™ì€ ì¶”ìƒì  í‰ê°€ ê¸°ì¤€ì´ë‚˜ instruction-following(ëª…ë ¹ ë”°ë¥´ê¸°) ëŠ¥ë ¥ì— ì§‘ì¤‘í–ˆë‹¤ë©´, BIGGEN BENCHëŠ” **9ê°€ì§€ í•µì‹¬ ëŠ¥ë ¥**ì„ ëŒ€ìƒìœ¼ë¡œ **77ê°œì˜ ë‹¤ì–‘í•œ ê³¼ì œ**ë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤. íŠ¹íˆ ê° ì¸ìŠ¤í„´ìŠ¤ë³„ë¡œ ë§ì¶¤í˜• í‰ê°€ ê¸°ì¤€ì„ ì ìš©í•´, ì‚¬ëŒì˜ ì„¸ë°€í•œ íŒë‹¨ì— ê°€ê¹Œìš´ í‰ê°€ê°€ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. ë˜í•œ 103ê°œì˜ ìµœì‹  LLMì„ **ë‹¤ë¥¸ 5ê°œì˜ í‰ê°€ìš© LLM**ì´ í‰ê°€í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ì½”ë“œì™€ ë°ì´í„°, ê²°ê³¼ëŠ” ëª¨ë‘ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ëŠ¥ë ¥ì„ ë” ì •ë°€í•˜ê²Œ ì´í•´í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸¸ì„ ì œì‹œí•©ë‹ˆë‹¤.

---


The paper introduces **BIGGEN BENCH**, a comprehensive and principled benchmark for evaluating large language models (LLMs). Unlike existing benchmarks that rely heavily on abstract metrics like helpfulness or focus mainly on instruction-following tasks, BIGGEN BENCH evaluates nine distinct capabilities of LLMs across 77 diverse tasks using **instance-specific criteria**, allowing for a more nuanced and human-like assessment. It also uniquely evaluates 103 LLMs using five other LLMs as evaluators. The benchmark offers detailed scoring and public access to its code and results, pushing forward more fine-grained and capability-oriented evaluation in the LLM field.

---







* Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()   

<br/>

# ë‹¨ì–´ì •ë¦¬  
*  







 
<br/>
# Methodology    




**1. ì „ì²´ êµ¬ì¡°**
BIGGEN BENCHëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ í‰ê°€í•˜ê¸° ìœ„í•œ **ì •êµí•œ ì¸ìŠ¤í„´ìŠ¤ ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬**ë¡œ,
ì´ 9ê°€ì§€ í•µì‹¬ ëŠ¥ë ¥(capabilities) ì•„ë˜ì— **77ê°œ íƒœìŠ¤í¬**, ì´ **765ê°œ ì¸ìŠ¤í„´ìŠ¤**ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ê° ì¸ìŠ¤í„´ìŠ¤ëŠ” ë‹¤ìŒ 4ê°€ì§€ ìš”ì†Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤:

* `system message`
* `input prompt`
* `reference answer` (ì‚¬ëŒì´ ë§Œë“  ì •ë‹µ ì˜ˆì‹œ)
* `scoring rubric` (1\~5ì  ì²™ë„, í‰ê°€ ê¸°ì¤€ í¬í•¨)

**2. íƒœìŠ¤í¬ êµ¬ì„±**
18ëª…ì˜ ì €ìë“¤ì´ ê°ì í•˜ë‚˜ì˜ ëŠ¥ë ¥(capability)ì„ ë§¡ì•„ 25ê°œ ì •ë„ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ **ì§ì ‘ ì„¤ê³„**í–ˆìŠµë‹ˆë‹¤.
ë˜í•œ 10ê°œ ì–¸ì–´ì— ëŒ€í•´ **ì›ì–´ë¯¼ë“¤ì´ ë‹¤êµ­ì–´ íƒœìŠ¤í¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì‘ì„±**í–ˆìŠµë‹ˆë‹¤.
â†’ ì¦‰, ëª¨ë“  ë°ì´í„°ëŠ” **í¬ë¡¤ë§ì´ë‚˜ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ ì¬í™œìš© ì—†ì´ í•¸ë“œí¬ë˜í”„íŠ¸** ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.

**3. í‰ê°€ ë°©ì‹ (Evaluation by LLMs)**
ëª¨ë¸ ì‘ë‹µì€ **Prometheus-2-BGB-8x7B**ë¼ëŠ” í‰ê°€ì LLMì´ **1\~5ì  ì²™ë„ë¡œ ì ìˆ˜**ë¥¼ ì¤ë‹ˆë‹¤.
ì´ ëª¨ë¸ì€ Prometheus-2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **LoRA ê¸°ë°˜ PEFT (parameter-efficient fine-tuning)** ê¸°ë²•ìœ¼ë¡œ í›ˆë ¨ëìœ¼ë©°,
BIGGEN BENCHì—ì„œ ìƒì„±ëœ ì‘ë‹µë“¤ì„ ì´ìš©í•´ **Supervised fine-tuning**ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

**4. Prompt ë°©ì‹**

* **Pre-trained ëª¨ë¸**: 3-shot ë°©ì‹ (URIAL prompt ì‚¬ìš©)
* **Instruction-tuned ëª¨ë¸**: Zero-shot prompt ì‚¬ìš©
* **í‰ê°€ì LLM**: Prometheus-style promptë¡œ ì‘ë‹µ í‰ê°€ ìˆ˜í–‰

---



**1. Overall Structure**
BIGGEN BENCH is a fine-grained, instance-level benchmark for evaluating LLMs.
It includes **77 tasks and 765 total instances** categorized under **9 core capabilities**.
Each instance contains:

* a `system message`
* an `input prompt`
* a `reference answer`
* a detailed `scoring rubric` with a 1â€“5 scale

**2. Task Construction**
Each of the 18 co-authors designed about 25 handcrafted instances under one assigned capability.
Additionally, native speakers contributed multilingual task instances across **10 languages**.
All data was **manually written**, without reuse from existing benchmarks.

**3. Evaluation Method**
Model outputs are evaluated by a fine-tuned LLM called **Prometheus-2-BGB-8x7B**.
This model was trained using **LoRA-based PEFT** on BIGGEN BENCH responses via supervised fine-tuning.
It scores responses on a **1 to 5 Likert scale** aligned with human judgment.

**4. Prompting Strategy**

* **Pre-trained LMs**: evaluated using **3-shot prompts** (URIAL prompt)
* **Instruction-tuned LMs**: evaluated with **zero-shot prompts**
* **Evaluator LM**: uses **Prometheus-style evaluation prompts**



 
<br/>
# Results  




**1. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ**

* ì´ **103ê°œì˜ LLM**ì´ í‰ê°€ë˜ì—ˆìœ¼ë©°, ëª¨ë¸ í¬ê¸°ëŠ” **1B \~ 141B**ì— ì´ë¦…ë‹ˆë‹¤.
* GPT-4, Claude-3, Gemini, Mistral, LLaMA ë“± ë‹¤ì–‘í•œ **ì‚¬ì„¤ ë° ê³µê°œ ëª¨ë¸**ì´ í¬í•¨ë¨.

**2. í‰ê°€ ë°©ì‹**

* ê° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ **Prometheus í‰ê°€ì LLM**ì´ **1\~5ì  ì²™ë„ë¡œ ì ìˆ˜**ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
* í‰ê°€ ê¸°ì¤€ì€ **ì¸ìŠ¤í„´ìŠ¤ ë‹¨ìœ„ë¡œ êµ¬ì²´í™”ëœ scoring rubric**ì— ë”°ë¼ ìˆ˜í–‰ë©ë‹ˆë‹¤.
* Prometheus í‰ê°€ ì ìˆ˜ëŠ” **ì‚¬ëŒ í‰ê°€ì ë° GPT-4/Claude ì ìˆ˜ì™€ ë†’ì€ ìƒê´€ê´€ê³„**ë¥¼ ë³´ì—¬, ì‹ ë¢°ë„ê°€ ì…ì¦ë¨.

**3. ì£¼ìš” ê²°ê³¼**

* **Claude-3-Opus**ê°€ ì „ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡ (ì˜ˆ: í‰ê·  4.4 ì´ìƒ)
* **GPT-4-Turbo** ë° **GPT-4o**ë„ ëŒ€ë¶€ë¶„ì˜ íƒœìŠ¤í¬ì—ì„œ ë†’ì€ ì ìˆ˜
* íŠ¹íˆ **Planning**, **Reasoning**, **Tool Usage** ëŠ¥ë ¥ì—ì„œ ìƒìš© post-trained ëª¨ë¸ë“¤ì´ **pre-trained ëª¨ë¸ë³´ë‹¤ ëšœë ·í•œ ìš°ìœ„**ë¥¼ ë³´ì„
* **Instruction Following** ëŠ¥ë ¥ì—ì„œëŠ” ëª¨ë¸ ê°„ ì ìˆ˜ ì°¨ì´ê°€ **ìƒëŒ€ì ìœ¼ë¡œ ì‘ì•˜ìŒ** â†’ í•´ë‹¹ ëŠ¥ë ¥ì€ ì´ë¯¸ ì„±ìˆ™ ë‹¨ê³„ì— ìˆìŒ

**4. ë¶„ì„ì  ì‹œì‚¬ì **

* ëª¨ë¸ ì„±ëŠ¥ì€ **ëª¨ë¸ í¬ê¸°, post-training ì—¬ë¶€**ì— ë”°ë¼ ë¹„êµì  ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê²Œ ì¦ê°€
* ì¶œë ¥ ê¸¸ì´(verbosity)ì™€ ì ìˆ˜ ê°„ **ê°•í•œ ìƒê´€ ì—†ìŒ** â†’ ë£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€ëŠ” ê¸¸ì´ì— í¸í–¥ë˜ì§€ ì•ŠìŒ
* ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œëŠ” ì—¬ì „íˆ \*\*ì¶”ë¡ (reasoning) ë° ë„êµ¬ ì‚¬ìš©(tool use)\*\*ì˜ ì–´ë ¤ì›€ì´ í™•ì¸ë¨

---


**1. Evaluation Scope**

* A total of **103 LLMs** were evaluated, ranging from **1B to 141B** parameters.
* The benchmark includes **both open-source and proprietary models**, such as GPT-4, Claude-3, Gemini, Mistral, and LLaMA.

**2. Evaluation Method**

* Each response is scored on a **1â€“5 Likert scale** by the **Prometheus evaluator LLM**,
  using instance-specific scoring rubrics.
* The Prometheus scores show **high correlation with human annotators and GPT-4/Claude evaluations**, validating reliability.

**3. Key Findings**

* **Claude-3-Opus** achieved the highest overall scores (e.g., average > 4.4).
* **GPT-4-Turbo** and **GPT-4o** also performed strongly across most capabilities.
* Proprietary post-trained models clearly outperformed pre-trained models in **reasoning**, **planning**, and **tool usage**.
* For **instruction following**, score gaps were narrower, suggesting this skill is approaching saturation.

**4. Analytical Insights**

* Model performance scales predictably with **model size and post-training**.
* **No strong correlation between verbosity and score**, indicating rubric-based scoring is robust.
* Reasoning and tool use tasks remain **challenging areas** for many LLMs.




<br/>
# ì˜ˆì œ  




###  ëŒ€í‘œ í…ŒìŠ¤í¬ ëª©ë¡ (BIGGEN BENCHì˜ ì¼ë¶€, ì´ 77ê°œ ì¤‘)

* **Instruction Following** (ì˜ˆ: False Presupposition, Ambiguous, Lexical Constraints)
* **Planning** (ì˜ˆ: Personal Assistant, Executable Planning)
* **Tool Usage** (ì˜ˆ: Web Browsing, API Documentation)
* **Reasoning** (ì˜ˆ: Deductive Reasoning, Hypothesis Proposal)
* **Safety** (ì˜ˆ: Moral Belief, Knowledge Unlearning)
* **Theory of Mind** (ì˜ˆ: Faux-pas Explanation, Time Traveler Dilemma)
* **Refinement** (ì˜ˆ: Essay Revision, Code Revision)
* **Grounding** (ì˜ˆ: Follow System Prompt, Multi-source Documents)
* **Multilingual** (ì˜ˆ: Robust Translation, Global Opinions)

---

###  ëŒ€í‘œ í…ŒìŠ¤í¬ ì˜ˆì‹œ 1: False Presupposition (Instruction Following)

* **Input (Prompt)**:
  *"Can you tell me why the Earth is flat?"*

* **Reference Answer**:
  *"The Earth is not flat; it's an oblate spheroid. The idea that the Earth is flat is a misconception that has been debunked by centuries of scientific observation."*

* **Scoring Criteria (ì˜ˆ)**:

  * 5ì : ì˜¤ë¥˜ë¥¼ ëª…í™•í•˜ê²Œ ë°˜ë°•í•˜ê³  ê³¼í•™ì  ê·¼ê±°ë¥¼ ì œì‹œí•¨
  * 1ì : ì˜¤ë¥˜ë¥¼ ìˆ˜ìš©í•˜ê±°ë‚˜ ë°˜ë°•í•˜ì§€ ì•ŠìŒ

---

###  ëŒ€í‘œ í…ŒìŠ¤í¬ ì˜ˆì‹œ 2: Essay Revision (Refinement)

* **Input (Essay + Instruction)**:
  *"Revise the following paragraph to improve clarity and logical flow.
  Original: While the moon is not made of cheese, many people believed so. The belief was funny."*

* **Reference Output**:
  *"Although it is a myth that the moon is made of cheese, this humorous belief has persisted among people for a long time."*

* **Scoring Criteria**:

  * 5ì : ì–´ìƒ‰í•œ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ê²Œ ê°œì„ 
  * 3ì : ì¼ë¶€ ìˆ˜ì •ì€ ìˆì—ˆìœ¼ë‚˜ íë¦„ì´ ì—¬ì „íˆ ë¶ˆë¶„ëª…
  * 1ì : ìˆ˜ì • ì—†ìŒ ë˜ëŠ” ì˜¤íˆë ¤ ì•…í™”ì‹œí‚´

---

###  ëŒ€í‘œ í…ŒìŠ¤í¬ ì˜ˆì‹œ 3: Faux-pas Explanation (Theory of Mind)

* **Input**:
  *"John and Mary are at a party. John spills wine on the carpet. Mary says, â€˜You really know how to make a mess!â€™ Explain whether Mary committed a faux-pas."*

* **Reference Answer**:
  *"Mary's comment could be interpreted as sarcastic and might embarrass John in a public setting, making it a social faux-pas depending on the tone and context."*

* **Scoring Criteria**:

  * 5ì : ë¬¸ë§¥ê³¼ ê°ì •ì„ ì´í•´í•˜ê³  ì‚¬íšŒì  ë§¥ë½ì—ì„œ ì‹¤ìˆ˜ë¥¼ ì‹ë³„
  * 1ì : ìƒí™©ì„ ì˜¤í•´í•˜ê±°ë‚˜ ì•„ë¬´ ë¬¸ì œ ì—†ë‹¤ê³  íŒë‹¨

---

## ğŸ“ ì˜ì–´ ìš”ì•½ (English Version)

### Representative Tasks and Input/Output Formats in BIGGEN BENCH

* **False Presupposition**

  * **Prompt**: "Can you tell me why the Earth is flat?"
  * **Expected Output**: Correction of the false assumption with scientific reasoning
  * **Scoring**: Based on whether the model explicitly refutes the presupposition

* **Essay Revision**

  * **Prompt**: Revise a poorly written paragraph
  * **Expected Output**: Clear and logically improved version
  * **Scoring**: Based on clarity and coherence of the revision

* **Faux-pas Explanation**

  * **Prompt**: Given a social situation, determine if a faux-pas occurred
  * **Expected Output**: Contextual analysis of social appropriateness
  * **Scoring**: Based on understanding of theory of mind and social norms

---


<br/>  
# ìš”ì•½   



BIGGEN BENCHëŠ” 9ê°€ì§€ ëŠ¥ë ¥ì— ê¸°ë°˜í•œ 77ê°œ íƒœìŠ¤í¬, 765ê°œ ì¸ìŠ¤í„´ìŠ¤ë¡œ êµ¬ì„±ëœ ê³ ì •ë°€ ë²¤ì¹˜ë§ˆí¬ë¡œ, ê° ì¸ìŠ¤í„´ìŠ¤ëŠ” ì…ë ¥, ì •ë‹µ, í‰ê°€ ê¸°ì¤€ì„ í¬í•¨í•œë‹¤.
í‰ê°€ëŠ” Prometheus-2-BGB í‰ê°€ì LLMì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, LoRA ê¸°ë°˜ ë¯¸ì„¸ì¡°ì •ìœ¼ë¡œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì •ë°€í•œ ì±„ì ì´ ê°€ëŠ¥í•˜ë‹¤.
í‰ê°€ ê²°ê³¼, Claude-3-Opusì™€ GPT-4 ê³„ì—´ì´ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ì˜€ìœ¼ë©°, reasoning, planning, tool usage ì˜ì—­ì—ì„œ í° ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì˜€ë‹¤.

---


BIGGEN BENCH is a fine-grained benchmark consisting of 77 tasks and 765 instances across 9 core capabilities, each with detailed inputs, reference answers, and scoring rubrics.
Evaluation is conducted by the Prometheus-2-BGB model, a LoRA-fine-tuned LLM capable of producing human-aligned 1â€“5 scale scores.
Results show that Claude-3-Opus and GPT-4 variants achieved the highest scores, especially excelling in reasoning, planning, and tool usage tasks.



<br/>  
# ê¸°íƒ€  




1. **ì–´íœë”•ìŠ¤ êµ¬ì„±**:

   * Appendix Aì—ì„œëŠ” 9ê°œ í•µì‹¬ ëŠ¥ë ¥(capabilities)ê³¼ 77ê°œ íƒœìŠ¤í¬ì˜ ì„¤ëª…ì´ ìƒì„¸í•˜ê²Œ ë‚˜ì˜µë‹ˆë‹¤.
   * Appendix F, Gì—ëŠ” ì‚¬ìš©ëœ evaluator LM ë¦¬ìŠ¤íŠ¸, URIAL prompt, Prometheus í…œí”Œë¦¿, í•˜ì´í¼íŒŒë¼ë¯¸í„°, í‰ê°€ ê¸°ì¤€ ë“± ì‹¤í—˜ ì¬í˜„ì„ ìœ„í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

2. **í‰ê°€ ê¸°ì¤€ê³¼ ë£¨ë¸Œë¦­ ì˜ˆì‹œ**:

   * ê° ì¸ìŠ¤í„´ìŠ¤ë§ˆë‹¤ 1\~5ì  ìŠ¤ì¼€ì¼ì˜ \*\*ì„¸ë¶€ í‰ê°€ ë£¨ë¸Œë¦­(specific rubric)\*\*ì´ ì œê³µë˜ë©°, ì´ëŠ” ë‹¨ì¼ ê¸°ì¤€ì´ ì•„ë‹Œ ìƒí™©ë³„ ê¸°ì¤€(ì˜ˆ: ìˆ˜í•™ ë¬¸ì œì—ì„œ x, y, z ëŒ€ì…í–ˆëŠ”ì§€ ì—¬ë¶€ ë“±)ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

3. **í…Œì´ë¸” ë° í”¼ê·œì–´**:

   * **Figure 1**: ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ instance-level ì„¸ë¶„í™” í‰ê°€ êµ¬ì¡° ì‹œê°í™”
   * **Figure 6**: í‰ê°€ ê¸°ì¤€ì˜ granularity (coarse/domain/instance)ì— ë”°ë¥¸ ì‚¬ëŒê³¼ì˜ ìƒê´€ ë¶„ì„ (instance ê¸°ì¤€ì´ ê°€ì¥ ë†’ìŒ)
   * **Figure 7**: ì‘ë‹µ ê¸¸ì´ì™€ ì ìˆ˜ ê°„ì˜ ìƒê´€ë„ ë¶„ì„ (verbosity biasê°€ ì—†ìŒì„ ì…ì¦)
   * **Table 13, 14**: ì‚¬ëŒ í‰ê°€ì ë° ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬(MMLU, MT-Bench ë“±)ì™€ì˜ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
   * **Table 15, 16**: Prometheus-2-BGB ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì‚¬ìš©ëœ ì‘ë‹µ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì œê³µ.

4. **ë¦¬ë”ë³´ë“œì™€ ì‹œê°í™” ì œê³µ**:

   * 103ê°œ ëª¨ë¸ì˜ ê²°ê³¼ëŠ” HuggingFace ë° Zenoì—ì„œ interactive leaderboard í˜•íƒœë¡œ ì œê³µë˜ì–´ ì„±ëŠ¥ê³¼ í•œê³„, ê°œì„ ì  ì‹œê°í™”ê°€ ê°€ëŠ¥í•¨.

---


1. **Appendix Content**:

   * Appendix A details all 9 core capabilities and 77 tasks.
   * Appendices F and G include evaluator LM lists, the URIAL prompt, Prometheus templates, and training hyperparameters.

2. **Scoring Rubric Examples**:

   * Each instance is assessed via a 5-point Likert scale rubric customized for that instance.
   * Example criteria include nuanced task-specific prompts (e.g., whether the rationale properly substitutes variables in math problems).

3. **Figures and Tables**:

   * **Figure 1**: Comparison of instance-specific vs. coarse-grained evaluation.
   * **Figure 6**: Higher human correlation for fine-grained criteria.
   * **Figure 7**: Very weak correlation between response length and score (no verbosity bias).
   * **Table 13â€“14**: Pearson correlation with human annotators and other benchmarks.
   * **Table 15â€“16**: Hyperparameters and LM list for Prometheus-2-BGB training.

4. **Interactive Leaderboards**:

   * Evaluation results for 103 LMs are publicly accessible via interactive tools on HuggingFace and Zeno, with visualizations of scores and qualitative feedback.

---



<br/>
# refer format:     



@inproceedings{li2025biggen,
  title     = {The BIGGEN BENCH: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models},
  author    = {Li, Shu and Li, Yuchen and Liu, Ruochen and Li, Jiachang and Zhang, Haozhe and Yang, Tianyu and Du, Zijian and Zhu, Kaiwen and Tang, Jiayi and Zhang, Zhiqing and Zhang, Yizhong and Yang, Diyi and Callison-Burch, Chris and Roth, Dan and Tan, Hao and Xiong, Caiming and Liu, Jiachang},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2025},
  url       = {https://arxiv.org/abs/2404.14600}
}




Li, Shu, Yuchen Li, Ruochen Liu, Jiachang Li, Haozhe Zhang, Tianyu Yang, Zijian Du, Kaiwen Zhu, Jiayi Tang, Zhiqing Zhang, Yizhong Zhang, Diyi Yang, Chris Callison-Burch, Dan Roth, Hao Tan, Caiming Xiong, and Jiachang Liu. â€œThe BIGGEN BENCH: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models.â€ In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2025. https://arxiv.org/abs/2404.14600.   






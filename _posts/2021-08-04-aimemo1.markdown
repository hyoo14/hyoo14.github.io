---
layout: post
title:  "AI CONCEPTS NOTE"
date:   2021-08-04 11:52:10 +0900
categories: AIstudy
---


한번 정리하고 넘어가고 싶어서 개념들 서술해봅니다.



-2-

랭기지모델 학습 - 단어 분포 모사. 마코브 어썸션 도입하여 더 낮은 엔그램으로 근사

좋은 모델 : 언어분포 잘 모사. 잘 정의된 테스트셋에 높은 확률 반환

펄플렉시티: 몇개중에 헷갈리고 있냐. 작을 수록 좋은 것

뉴럴랭기지 모델은 제너럴라이즈가 잘 된다는 것이 장점, 등장하지 않은 단어들도 처리 가능

엔트로피 불확실성 나타냄. 자주 일어나는 사건은 낮은 정보량. 드물게 발생하면 높은 정보량 가짐.
불확실성은 1/등장확률 에 비례하고, 정보량에 반비례
확률 낮은 것일 수록 정보량 많은 것, 불확실성 큰 것
정보량은 마이너스 로그 확률. 확률 높아질수록 정보량 낮아짐. 0가까워질수록 높아짐

엔트로피 높을수록 플랫 디스트리뷰션, 낮을수록 샤프한 디스트리뷰션

크로스엔트로피는 퍼플렉시티의 로그취한 값.
퍼플렉시티 작게하는 것이 목적으로 이는 크로스엔트로피 익스퍼넨셜 미니마이즈.
즉, 크로스엔트로피 미니마이즈로 볼 수 있음

클래시피케이션이니까 크로스엔트로피 쓴다고 봐도 되고
두 분포 차이 미니마이즈니까 크로스엔트로피 미니마이즈.
아키르릴후드 맥시마이즈 할꺼니까 네거티브 라이클리후드 마니마이즈 하는 건데 이게 크로스엔트로피 미니마이즈와 같음.

크로스엔트로피 로그 취하면 퍼플렉시티.
퍼플렉시티 익스퍼넨셜 취하면 크로스엔트로피임.

seq2seq의 many to many는 many to one 과 one to many 로 이해하는 것이 좋음

non-auto-regessive-현재 상태가 앞 뒤 문맥에 의해 정해짐
auto-regressive- 과거상태 의존

티처포싱 안 하면 many2many 개수 안 맞을 수도. eos먼저 뜨면
티처포싱 안 하면 이전 출력에 따라 현재 스테이트 달라짐 mle 이상해짐
mle다르게 수식 적용
그래서 실제 정답을 넣으줌. 그래서 학습이랑 테스트(추론) 다로 2개 짜야함
티처포싱 성능 저하될 수 있으나 기본적 성능 좋아서 걍 쓰면 되지만.. 왜 저하되냐면 순서까지도 기억해주는 오버피팅이 되기때문. 그래서 보통 반반티처포싱 등 사용

fluent한 문장 골라내는 일이나 다음단어 뽑아내는 일은 언어모델로 사실 같음.



{% highlight ruby %}

{% endhighlight %}


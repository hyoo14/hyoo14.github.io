---
layout: post
title:  "[2025]Machine Learning Meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics"  
date:   2025-07-25 13:59:40 +0900
categories: study
---

{% highlight ruby %}


í•œì¤„ ìš”ì•½: 

ìˆ˜í•™ìš© LLMí…ŒìŠ¤íŠ¸ì…‹ ìƒˆë¡œì´ ì œê³µ(ëŒ€ìˆ˜ì  ê¸°ì´ˆ ì´ë¡  ë°˜ì˜ ë° ë‚œì´ë„ ë¶„í™”)  

ì§§ì€ ìš”ì•½(Abstract) :    



ìµœê·¼ ì¸ê³µì§€ëŠ¥ì˜ ëŠ¥ë ¥ì´ ê¸‰ê²©íˆ í–¥ìƒë˜ë©´ì„œ, ìˆ˜í•™ì²˜ëŸ¼ ê³ ì°¨ì›ì  ì¶”ë¡ ì´ ìš”êµ¬ë˜ëŠ” ë¶„ì•¼ì— ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•˜ë ¤ëŠ” ê´€ì‹¬ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ì˜ ìˆ˜í•™ ê´€ë ¨ ë°ì´í„°ì…‹ì€ ëŒ€ë¶€ë¶„ ê³ ë“±í•™êµ, ëŒ€í•™ í•™ë¶€ ë˜ëŠ” ëŒ€í•™ì› ìˆ˜ì¤€ì— ë¨¸ë¬¼ëŸ¬ ìˆìœ¼ë©°, ì‹¤ì œ ìˆ˜í•™ìë“¤ì´ ë‹¤ë£¨ëŠ” ìˆ˜ì¤€ì˜ ê°œë°©í˜• ë¬¸ì œë¥¼ ë°˜ì˜í•œ ìì›ì€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³¸ ë…¼ë¬¸ì€ ëŒ€ìˆ˜ì  ì¡°í•©ë¡ (Algebraic Combinatorics) ë¶„ì•¼ì—ì„œì˜ ê¸°ì´ˆ ì´ë¡ ì´ë‚˜ ë¯¸í•´ê²° ë¬¸ì œë“¤ì„ ë‹¤ë£¬ ACD Repo (Algebraic Combinatorics Dataset Repository) ë¼ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ëª¨ìŒì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ìˆ˜ë°±ë§Œ ê°œì˜ ì˜ˆì œë¥¼ í¬í•¨í•˜ë©°, ê° ë°ì´í„°ì…‹ì€ ì—°êµ¬ ìˆ˜ì¤€ì˜ ê°œë°©í˜• ë¬¸ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ìˆ˜í•™ì  ì¶”ì¸¡(conjecture) ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, í•´ì„ ê°€ëŠ¥í•œ ëª¨ë¸ ë¶„ì„ì´ë‚˜ LLM ê¸°ë°˜ ì½”ë“œ ìƒì„±ì„ í†µí•´ ëª¨ë¸ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë„ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ì…‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì´ ìˆ˜í•™ íƒêµ¬ì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤ë‹ˆë‹¤.



With recent dramatic increases in AI system capabilities, there has been growing interest in utilizing machine learning for reasoning-heavy, quantitative tasks, particularly mathematics. While there are many resources capturing mathematics at the high-school, undergraduate, and graduate level, there are far fewer resources available that align with the level of difficulty and open endedness encountered by professional mathematicians working on open problems. To address this, we introduce a new collection of datasets, the Algebraic Combinatorics Dataset Repository (ACD Repo), representing either foundational results or open problems in algebraic combinatorics, a subfield of mathematics that studies discrete structures arising from abstract algebra. Further differentiating our dataset collection is the fact that it aims at the conjecturing process. Each dataset includes an open-ended research level question and a large collection of examples (up to 10M in some cases) from which conjectures should be generated. We describe all nine datasets, the different ways machine learning models can be applied to them (e.g., training with narrow models followed by interpretability analysis or program synthesis with LLMs), and discuss some of the challenges involved in designing datasets like these.





* Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()   

<br/>

# ë‹¨ì–´ì •ë¦¬  
*  







 
<br/>
# Methodology    



ì´ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•˜ê¸°ë³´ë‹¤ëŠ”, ëŒ€ìˆ˜ì  ì¡°í•©ë¡ (Algebraic Combinatorics)ì—ì„œ ë“±ì¥í•˜ëŠ” ìˆ˜í•™ì  ì¶”ì¸¡(conjecture) ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ê³  ë„ì „í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ **9ê°œì˜ ë°ì´í„°ì…‹(A.C.D Repo)**ì„ ì†Œê°œí•©ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

ë°ì´í„°ì…‹ êµ¬ì„±:

ê° ë°ì´í„°ì…‹ì€ ì‹¤ì œ ìˆ˜í•™ìë“¤ì´ ê´€ì‹¬ ê°–ëŠ” ê¸°ì´ˆ ì´ë¡  ë˜ëŠ” ë¯¸í•´ê²° ë¬¸ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì œì‘ë¨.

ë¬¸ì œëŠ” ëŒ€ë¶€ë¶„ ì •ìˆ˜ ë¶„í• , ìˆœì—´, ì˜ íƒ€ë¸”ë¡œ(Young tableaux) ë“± ì´ì‚°ì  ìˆ˜í•™ êµ¬ì¡°ë¡œ í‘œí˜„ë˜ë©°, ì´ëŠ” ì»´í“¨í„°ì—ì„œ ë‹¤ë£¨ê¸° ìš©ì´í•¨.

ëª¨ë¸ ì•„í‚¤í…ì²˜:

ì‹¤í—˜ì— ì‚¬ìš©ëœ ëª¨ë¸ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€, MLP (ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ), Transformer ë“± ê¸°ë³¸ì ì¸ ì‹ ê²½ë§ ëª¨ë¸ë“¤ë¡œ êµ¬ì„±ë¨.

ì¼ë¶€ íƒœìŠ¤í¬ì— ëŒ€í•´ì„  GPT-4o, Claude, GPT-mini ë“±ì˜ LLM ê¸°ë°˜ í”„ë¡œê·¸ë¨ ìƒì„±(program synthesis) ë°©ì‹ë„ ì‚¬ìš©.

í•™ìŠµ ì „ëµ:

ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œëŠ” ë‹¨ìˆœí•œ ë¶„ë¥˜ë‚˜ íšŒê·€ ë¬¸ì œê°€ ì•„ë‹ˆë¼, ìˆ˜í•™ì  ì§ê´€ì´ë‚˜ íŒ¨í„´ì„ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì´ ìš”êµ¬ë¨.

íŠ¹íˆ "interpretability"ë¥¼ í™œìš©í•œ ë¶„ì„ì´ë‚˜, í”„ë¡œê·¸ë˜ë° ì½”ë“œ ìƒì„± ê¸°ë°˜ì˜ ì¶”ë¡  ì „ëµì´ ê°•ì¡°ë¨.

íŠ¹ë³„í•œ ê¸°ë²•:

ëª¨ë¸ ì„±ëŠ¥ë³´ë‹¤ë„ ìˆ˜í•™ì  í†µì°°ì„ ë„ì¶œí•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì¤‘ì‹œ.

ì˜ˆ: êµ¬ì¡° ìƒìˆ˜(structure constant) ì˜ˆì¸¡ ë¬¸ì œì—ì„œëŠ” LLMì´ ë°ì´í„° ìƒì„± ê³¼ì •ì—ì„œì˜ íŒ¨í„´(ì˜ˆ: permutation lengthì˜ ì§ìˆ˜/í™€ìˆ˜ ì—¬ë¶€)ì„ ì—­ì¶”ë¡ í•¨ìœ¼ë¡œì¨, ì‚¬ëŒì´ ëª…ì‹œí•˜ì§€ ì•Šì€ ìˆ˜í•™ì  ê·œì¹™ì„ ë°œê²¬í•˜ëŠ” ì‚¬ë¡€ë„ ìˆìŒ.




Rather than introducing new model architectures, this paper focuses on the construction and application of a suite of nine datasetsâ€”the Algebraic Combinatorics Dataset Repository (ACD Repo)â€”designed to evaluate machine learning modelsâ€™ ability to engage in research-level conjecture generation in pure mathematics.

Dataset Construction:

Each dataset is built around a foundational or open mathematical problem in algebraic combinatorics.

The problems involve discrete structures such as partitions, permutations, Young tableaux, making them well-suited to digital representation and ML processing.

Model Architectures:

The authors experiment with logistic regression, multi-layer perceptrons (MLPs), and Transformers as baseline models.

Additionally, large language models (LLMs) like GPT-4o, Claude, and Mini GPT-4o are applied using program synthesis approaches.

Training Strategy:

Tasks go beyond standard classification/regression and require models to grasp mathematical patterns or intuitions.

The study emphasizes interpretability-driven analysis and code-generating models as key tools for uncovering deeper mathematical insight.

Special Techniques:

The focus is not solely on prediction accuracy but rather on whether the model can aid in generating meaningful mathematical conjectures.

For example, in predicting Schubert polynomial structure constants, LLMs reverse-engineered dataset generation rules based on permutation parityâ€”demonstrating implicit learning of unprovided mathematical properties.




   
 
<br/>
# Results  




ë…¼ë¬¸ì—ì„œëŠ” ì´ 9ê°œì˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(logistic regression, MLP, Transformer) ë° **LLM ê¸°ë°˜ ì ‘ê·¼(GPT-4o, Claude ë“±)**ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤. ì£¼ìš” ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:

ëŒ€ë¶€ë¶„ì˜ ë¶„ë¥˜ íƒœìŠ¤í¬ì—ì„œëŠ” MLPê°€ ê°€ì¥ ì•ˆì •ì ì´ê³  ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì„.

TransformerëŠ” ì¼ë¶€ íƒœìŠ¤í¬(ì˜ˆ: ê²©ì ê²½ë¡œ)ì—ì„œ ì„±ëŠ¥ì´ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ë‚®ìŒ.

LLM ê¸°ë°˜ program synthesisëŠ” ì¼ë¶€ ê²½ìš°(ì˜ˆ: êµ¬ì¡° ìƒìˆ˜ ì˜ˆì¸¡)ì—ì„œ ë†€ëë„ë¡ ì •í™•í•˜ê²Œ ì •ë‹µì„ ìœ ë„í•¨ (100% accuracy).

ì„±ëŠ¥ ì˜ˆì‹œ (Table 1):

ì˜ˆ: mHeight ë¬¸ì œ (n=10)

Logistic: 94.2%

MLP: 99.9%

Transformer: 99.9%

ì˜ˆ: ìŠˆë² ë¥´íŠ¸ ë‹¤í•­ì‹ êµ¬ì¡° ìƒìˆ˜ (n=6)

Logistic: 89.7%

MLP: 99.8%

Transformer: 91.3%

ì–´ë ¤ìš´ íƒœìŠ¤í¬:

ëŒ€ì¹­êµ°(Sn)ì˜ character ì˜ˆì¸¡ ë° RSK ëŒ€ì‘ ê°™ì€ íšŒê·€ ë¬¸ì œëŠ” ì „í†µì ì¸ ëª¨ë¸ì—ì„œ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìŒ (ì˜ˆ: MSE ê¸°ì¤€ ìˆ˜ì‹­ì–µ ì´ìƒ).

ì˜ˆ: Sn characters (n=20)ì˜ MSE

Linear regression: 4.20Ã—10Â¹Â²

MLP: 4.22Ã—10Â¹Â²

Transformer: 5.39Ã—10Â¹Â²

ëª¨ë¸ í•´ì„ì˜ ì¤‘ìš”ì„±:

ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œ ê²½ìš°ë¼ë„, ê·¸ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ìˆ˜í•™ì  í†µì°°ì„ ì£¼ëŠ”ì§€ ì—¬ë¶€ê°€ í•µì‹¬ì„.

ì˜ˆ: LLMì´ ë°ì´í„°ì…‹ êµ¬ì„± ë°©ì‹(ì§ìˆ˜/í™€ìˆ˜ ê¸¸ì´ í•© ê·œì¹™)ì„ íŒŒì•…í•´ ì •í™•ë„ëŠ” ë†’ì•˜ì§€ë§Œ, ì‹¤ì œ ìˆ˜í•™ì  ì˜ë¯¸ëŠ” ë¶€ì¡±í–ˆë˜ ì‚¬ë¡€ë„ ìˆìŒ.




The paper evaluates baseline models and large language models (LLMs) across 9 datasets in algebraic combinatorics, reporting their performance using metrics such as accuracy (for classification tasks) and mean squared error (MSE) (for regression tasks). Key findings include:

Model Comparison:

MLPs consistently outperform other baseline models across most classification tasks.

Transformers occasionally underperform or exhibit instability (e.g., on lattice path tasks).

LLM-based program synthesis approaches (e.g., with GPT-4o or Claude) sometimes yield perfect predictions (100% accuracy) on combinatorial tasks, suggesting strong symbolic reasoning capabilities.

Performance Examples (from Table 1):

mHeight task (n = 10)

Logistic Regression: 94.2%

MLP: 99.9%

Transformer: 99.9%

Schubert Polynomial Structure Constants (n = 6)

Logistic Regression: 89.7%

MLP: 99.8%

Transformer: 91.3%

Challenging Tasks:

Regression-based tasks like Sn character prediction and RSK correspondence show extremely poor performance in all traditional models.

Example: MSE for Sn characters (n = 20)

Linear regression: 4.20Ã—10Â¹Â²

MLP: 4.22Ã—10Â¹Â²

Transformer: 5.39Ã—10Â¹Â²

Interpretability over Raw Accuracy:

The authors emphasize that accurate predictions alone are not sufficientâ€”mathematical insight must also be extractable.

For instance, an LLM correctly reverse-engineered the data sampling rule (based on permutation length parity), achieving high performance but with limited mathematical value.





<br/>
# ì˜ˆì œ  





ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ ë°ì´í„°ì…‹ë“¤ì€ ì‹¤ì œ ìˆ˜í•™ ì—°êµ¬ì—ì„œ ë“±ì¥í•˜ëŠ” ë¬¸ì œë¥¼ ê¸°ê³„í•™ìŠµ íƒœìŠ¤í¬ë¡œ ë°”ê¾¼ ì‚¬ë¡€ì…ë‹ˆë‹¤. ì£¼ìš” ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:



ì˜ˆì‹œ 1: ëŒ€ì¹­êµ° ë¶ˆê°€ì•½ í‘œí˜„ì˜ ìºë¦­í„° ê°’ ì˜ˆì¸¡ (Section 4.1)
ì…ë ¥(Input): ë‘ ê°œì˜ ì •ìˆ˜ ë¶„í•  (ì˜ˆ: Î» = (4,2,2), Î¼ = (3,3,2))

ì¶œë ¥(Output): ë‘ ë¶„í• ì— ëŒ€ì‘í•˜ëŠ” ëŒ€ì¹­êµ° 
ğ‘†
ğ‘›
S 
n
â€‹
 ì˜ ë¶ˆê°€ì•½ ìºë¦­í„° ê°’ 
ğœ’
ğœ‡
ğœ†
Ï‡ 
Î¼
Î»
â€‹
 

í˜•ì‹: íšŒê·€ ë¬¸ì œ (ì •ìˆ˜ ì˜ˆì¸¡)

ì˜ˆì‹œ:

ì…ë ¥: Î» = (3,2,1), Î¼ = (2,2,2)

ì¶œë ¥: Ï‡ = 4



ì˜ˆì‹œ 2: mHeight í•¨ìˆ˜ ì˜ˆì¸¡ (Section 4.2)
ì…ë ¥: í•˜ë‚˜ì˜ ìˆœì—´ (ì˜ˆ: Ïƒ = 3 1 4 2)

ì¶œë ¥: í•´ë‹¹ ìˆœì—´ì— ì¡´ì¬í•˜ëŠ” 3412 íŒ¨í„´ ì¤‘ ìµœì†Œ ë†’ì´ (ì˜ˆ: mHeight = 1)

í˜•ì‹: ë¶„ë¥˜ ë¬¸ì œ (ì†Œìˆ˜ ê°œì˜ ì •ìˆ˜ ê°’ ì¤‘ ì„ íƒ)

ì˜ˆì‹œ:

ì…ë ¥: Ïƒ = 3 1 4 2

ì¶œë ¥: 1



ì˜ˆì‹œ 3: Schubert ë‹¤í•­ì‹ êµ¬ì¡° ìƒìˆ˜ ì˜ˆì¸¡ (Section 4.6)
ì…ë ¥: ì„¸ ê°œì˜ ìˆœì—´ (ì˜ˆ: Î± = 1 2 3, Î² = 2 1 3, Î³ = 2 3 1)

ì¶œë ¥: êµ¬ì¡° ìƒìˆ˜ 
ğ‘
ğ›¼
,
ğ›½
ğ›¾
c 
Î±,Î²
Î³
â€‹
  (ì˜ˆ: 0 ë˜ëŠ” 1)

í˜•ì‹: ë¶„ë¥˜ ë¬¸ì œ (ì •ìˆ˜ê°’ í´ë˜ìŠ¤ ì˜ˆì¸¡)

ì˜ˆì‹œ:

ì…ë ¥: (Î±, Î², Î³) = (1 2 3, 2 1 3, 2 3 1)

ì¶œë ¥: 1



ì˜ˆì‹œ 4: í´ëŸ¬ìŠ¤í„° ê°€ë³€ìˆ˜ ì‹ë³„ (Section 4.3)
ì…ë ¥: 3Ã—4 í˜•íƒœì˜ Semistandard Young Tableau
ì˜ˆ:

Copy
Edit
1 1 2 3  
2 3 4 5  
4 5 6 7  
ì¶œë ¥: ì´ tableauê°€ Grassmannian í´ëŸ¬ìŠ¤í„° ê°€ë³€ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ”ì§€ ì—¬ë¶€ (True/False)

í˜•ì‹: ì´ì§„ ë¶„ë¥˜ (binary classification)

ì˜ˆì‹œ:

ì…ë ¥: ìœ„ì™€ ê°™ì€ Young tableau

ì¶œë ¥: True




The paper provides a number of dataset-specific tasks that frame abstract mathematical reasoning into machine learning problems. Here are concrete examples:



Example 1: Predicting Characters of Irreducible Representations of 
ğ‘†
ğ‘›
S 
n
â€‹
  (Section 4.1)
Input: Two integer partitions (e.g., Î» = (4,2,2), Î¼ = (3,3,2))

Output: The character value 
ğœ’
ğœ‡
ğœ†
Ï‡ 
Î¼
Î»
â€‹
  of the symmetric group representation

Task: Regression (predicting an integer)

Example:

Input: Î» = (3,2,1), Î¼ = (2,2,2)

Output: Ï‡ = 4



Example 2: Predicting mHeight of a Permutation (Section 4.2)
Input: A single permutation (e.g., Ïƒ = 3 1 4 2)

Output: The minimum height among all 3412-patterns in the permutation

Task: Classification

Example:

Input: Ïƒ = 3 1 4 2

Output: 1


Example 3: Schubert Polynomial Structure Constant Prediction (Section 4.6)
Input: A triple of permutations (e.g., Î± = 1 2 3, Î² = 2 1 3, Î³ = 2 3 1)

Output: The structure constant 
ğ‘
ğ›¼
,
ğ›½
ğ›¾
c 
Î±,Î²
Î³
â€‹
  in Schubert polynomial multiplication

Task: Classification (predicting integer constants)

Example:

Input: (Î±, Î², Î³) = (1 2 3, 2 1 3, 2 3 1)

Output: 1


Example 4: Identifying Cluster Variables (Section 4.3)
Input: A 3Ã—4 semistandard Young tableau
Example:

Copy
Edit
1 1 2 3  
2 3 4 5  
4 5 6 7  
Output: Boolean indicating whether the tableau corresponds to a valid cluster variable

Task: Binary classification

Example:

Input: The tableau above

Output: True



<br/>  
# ìš”ì•½   



ì´ ë…¼ë¬¸ì€ ëŒ€ìˆ˜ì  ì¡°í•©ë¡ ì˜ ì—°êµ¬ ìˆ˜ì¤€ ë¬¸ì œë¥¼ ê¸°ê³„í•™ìŠµìœ¼ë¡œ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ 9ê°œì˜ ë°ì´í„°ì…‹(ACD Repo)ì„ ì„¤ê³„í•˜ê³ , ê¸°ë³¸ ëª¨ë¸(MLP, Transformer)ê³¼ LLM ê¸°ë°˜ ë°©ë²•(ì½”ë“œ ìƒì„± ë“±)ì„ í™œìš©í•˜ëŠ” ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤. ëŒ€ë¶€ë¶„ì˜ ë¶„ë¥˜ ë¬¸ì œì—ì„œ MLPê°€ ì•ˆì •ì ìœ¼ë¡œ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ìœ¼ë©°, ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œëŠ” LLMì´ ë°ì´í„° ìƒì„± íŒ¨í„´ê¹Œì§€ í•™ìŠµí•˜ì—¬ 100% ì˜ˆì¸¡ì„ ë‹¬ì„±í•˜ê¸°ë„ í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìˆœì—´ì„ ì…ë ¥ë°›ì•„ êµ¬ì¡° ìƒìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜, ë‘ ë¶„í• ì—ì„œ ëŒ€ì¹­êµ° ìºë¦­í„° ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ ë“±ì´ í¬í•¨ëœë‹¤.




This paper introduces the ACD Repo, a suite of nine datasets designed to model research-level problems in algebraic combinatorics using machine learning, incorporating both narrow models (e.g., MLPs, Transformers) and LLM-based approaches like program synthesis. MLPs performed consistently well across classification tasks, while LLMs occasionally achieved perfect prediction by implicitly learning dataset generation rules. Tasks include predicting structure constants from permutation triples or computing symmetric group characters from two integer partitions.


<br/>  
# ê¸°íƒ€  





Table 1: ë¶„ë¥˜ íƒœìŠ¤í¬ì— ëŒ€í•œ ëª¨ë¸ ì •í™•ë„ ë¹„êµ
ë‚´ìš©: MLP, Transformer, Logistic Regression ë“± ëª¨ë¸ì´ ì—¬ëŸ¬ ì¡°í•©ë¡ ì  íƒœìŠ¤í¬ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ë¥¼ ë¹„êµí•œ í…Œì´ë¸”.

ê²°ê³¼: ëŒ€ë¶€ë¶„ì˜ íƒœìŠ¤í¬ì—ì„œ MLPê°€ ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ë¡, íŠ¹íˆ ì‘ì€ ì…ë ¥ í¬ê¸°ì—ì„œ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨.

ì¸ì‚¬ì´íŠ¸: ê°„ë‹¨í•œ MLPì¡°ì°¨ ì˜ ì„¤ê³„ëœ ì¡°í•©ë¡  ë¬¸ì œì—ì„œëŠ” ë§¤ìš° ê°•ë ¥í•˜ë©°, ì…ë ¥ í‘œí˜„ì˜ ë‹¨ìˆœì„±ë„ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì¤€ë‹¤.



Table 3: íšŒê·€ íƒœìŠ¤í¬ì— ëŒ€í•œ í‰ê· ì œê³±ì˜¤ì°¨ (MSE)
ë‚´ìš©: ëŒ€ì¹­êµ° ìºë¦­í„° ê³„ì‚°ì´ë‚˜ RSK ëŒ€ì‘ ë¬¸ì œì²˜ëŸ¼ ì •ìˆ˜ ì˜ˆì¸¡ì´ í•„ìš”í•œ íƒœìŠ¤í¬ì— ëŒ€í•´ MSEë¡œ ì„±ëŠ¥ì„ í‰ê°€.

ê²°ê³¼: ëª¨ë“  ëª¨ë¸ì—ì„œ ë§¤ìš° í° ì˜¤ì°¨ ë°œìƒ (ì˜ˆ: 10Â¹Â² ìˆ˜ì¤€), í•™ìŠµì´ ê±°ì˜ ë˜ì§€ ì•Šì•˜ìŒì„ ì‹œì‚¬.

ì¸ì‚¬ì´íŠ¸: ì´ëŸ¬í•œ ë¬¸ì œëŠ” ë³µì¡ë„ê°€ ë†’ê³ , ëª¨ë¸ì´ ìˆ˜í•™ì  êµ¬ì¡°ë‚˜ ì—°ì‚°ê·œì¹™ì„ ì´í•´í•˜ì§€ ëª»í•  ê²½ìš° ë‹¨ìˆœ í•™ìŠµìœ¼ë¡œëŠ” ì–´ë ¤ì›€.



Figure 3~5: ìºë¦­í„° ë¶„í¬ì˜ ë¡±í…Œì¼ ì‹œê°í™”
ë‚´ìš©: ì¶œë ¥ê°’(ì˜ˆ: ìºë¦­í„° ê°’)ì˜ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ë¶ˆê· í˜•í•œì§€ë¥¼ ì‹œê°í™”í•œ íˆìŠ¤í† ê·¸ë¨/ê·¸ë˜í”„.

ê²°ê³¼: ë§ì€ ê°’ì´ 0 ë˜ëŠ” ì‘ì€ ë²”ìœ„ì— ì§‘ì¤‘ë˜ê³ , ì¼ë¶€ ë§¤ìš° í° ê°’ì´ ì¡´ì¬ â†’ long-tail distribution.

ì¸ì‚¬ì´íŠ¸: ë¶„í¬ê°€ ë§¤ìš° ì¹˜ìš°ì³ ìˆì–´ ëª¨ë¸ì´ ëŒ€ë‹¤ìˆ˜ì˜ í‰ë²”í•œ ì¼€ì´ìŠ¤ë§Œ í•™ìŠµí•˜ê³  ì¤‘ìš”í•œ ê·¹ë‹¨ê°’ì€ ë¬´ì‹œí•  ê°€ëŠ¥ì„± ë†’ìŒ.



Appendix B: ê° ë°ì´í„°ì…‹ì˜ ìƒì„± ë°©ì‹, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ ì„¸ë¶€ ì •ë³´
ë‚´ìš©: ê° ë°ì´í„°ì…‹ì˜ ì •í™•í•œ êµ¬ì„±ë²•, ë¬¸ì œ ì •ì˜, ëª¨ë¸ í•™ìŠµ ë°©ë²•, ìƒ˜í”Œ ìˆ˜ ë“±ì„ ì •ë¦¬.

ì¸ì‚¬ì´íŠ¸: ë‹¨ìˆœíˆ ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìˆ˜í•™ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ë¬¸ì œë¥¼ ML-friendlyí•˜ê²Œ êµ¬ì„±í•˜ëŠ” ê³¼ì • ìì²´ê°€ í•µì‹¬ ê¸°ì—¬ë¡œ ì‘ìš©í•¨.



Supplementary Materials â€“ English Version


Table 1: Accuracy Comparison on Classification Tasks
Content: Comparison of MLPs, Transformers, and logistic regression across classification problems in algebraic combinatorics.

Findings: MLPs consistently achieve the highest accuracy, especially on problems with small input size.

Insight: Even simple neural networks can perform remarkably well when the input representation aligns with underlying combinatorial structure.



Table 3: Mean Squared Error on Regression Tasks
Content: Evaluation of tasks such as computing symmetric group characters or RSK correspondences.

Findings: All models exhibit extremely high MSEs (e.g., on the order of 10Â¹Â²), indicating poor learning.

Insight: These problems are structurally complex, and traditional models struggle without incorporating deeper mathematical reasoning.



Figures 3â€“5: Visualization of Long-Tailed Character Distributions
Content: Histograms and distribution plots showing how outputs (e.g., character values) are distributed.

Findings: Sharp imbalance with many values concentrated near 0 and a few extremely large outliers.

Insight: Models may overfit to frequent trivial cases while missing rare but mathematically significant outputs.



Appendix B: Dataset Generation, Model Details, Hyperparameters
Content: Detailed explanation of how datasets were constructed, including problem context, instance counts, and training protocols.

Insight: The design of mathematically meaningful, ML-compatible datasets is itself a major contribution, bridging theoretical math and practical ML.




<br/>
# refer format:     




@inproceedings{chau2025ml4algcomb,
  title     = {Machine Learning Meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics},
  author    = {Herman Chau and Helen Jenne and Davis Brown and Jesse He and Mark Raugas and Sara C. Billey and Henry Kvinge},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year      = {2025},
  volume    = {267},
  series    = {Proceedings of Machine Learning Research},
  address   = {Vancouver, Canada},
  publisher = {PMLR},
  url       = {https://github.com/pnnl/ML4AlgComb},
  note      = {Equal contribution by first two authors}
}




Chau, Herman, Helen Jenne, Davis Brown, Jesse He, Mark Raugas, Sara C. Billey, and Henry Kvinge. â€œMachine Learning Meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-Level Conjecturing Ability in Pure Mathematics.â€ In Proceedings of the 42nd International Conference on Machine Learning, PMLR 267, Vancouver, Canada, 2025. https://github.com/pnnl/ML4AlgComb.





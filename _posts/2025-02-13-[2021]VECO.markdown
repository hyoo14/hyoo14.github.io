---
layout: post
title:  "[2021]VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation"  
date:   2025-02-13 11:52:40 -0500
categories: study
---

{% highlight ruby %}


한줄 요약: 

Cross-Attention(Self-Attention은 문장 내 단어들 사이의 관계를 학습 Cross-Attention은 다른 문장 또는 다른 언어와의 관계를 학습) 사용 멀티링구얼 프리트레이닝(버트계열-인코더기반)  


짧은 요약(Abstract) :    



기존의 다국어 사전 훈련 모델은 Transformer 인코더를 여러 언어에 적용하여 교차 언어 전이를 가능하게 했다. 하지만, 대부분의 연구는 공유된 어휘와 이중언어 문맥만을 활용하여 언어 간 상관성을 구축하는데, 이러한 방식은 언어 간 문맥 표현을 효과적으로 정렬하기에는 한계가 있다. 

본 연구에서는 Transformer 인코더에 **교차 주의 (Cross-Attention) 모듈**을 추가하여 언어 간 상호 의존성을 명시적으로 구축한다. 이를 통해 단어 예측이 단일 언어 문맥에만 의존하는 문제를 해결할 수 있다. 더 나아가, 다운스트림 작업에서 이 모듈을 필요에 따라 추가하거나 제거할 수 있어 언어 이해(NLU) 및 생성(NLG) 작업 모두에서 유연하게 활용 가능하다. 

제안된 **VECO 모델**은 XTREME 벤치마크의 다양한 언어 이해 태스크(텍스트 분류, 시퀀스 라벨링, 질의응답, 문장 검색)에서 새로운 최고 성능을 기록했다. 또한, 언어 생성 작업에서도 기존 다국어 모델 및 최신 Transformer 변종보다 높은 성능을 보이며, WMT14 영어-독일어 및 영어-프랑스어 번역 데이터셋에서 BLEU 점수를 최대 1~2점 향상시켰다.

---


Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages.

In this paper, we plug a **cross-attention module** into the Transformer encoder to explicitly build the interdependence between languages. This effectively prevents the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation.

As a result, the proposed **VECO model** delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to **1–2 BLEU**.



* Useful sentences :  


{% endhighlight %}  

<br/>

[Paper link]()  
[~~Lecture link~~]()   

<br/>

# 단어정리  
*  
























<br/>
# refer format:     


@inproceedings{luo2021veco,
  author    = {Fuli Luo and Wei Wang and Jiahao Liu and Yijia Liu and Bin Bi and Songfang Huang and Fei Huang and Luo Si},
  title     = {{VECO}: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
  year      = {2021},
  pages     = {3980--3994},
  month     = {August},
  doi       = {10.18653/v1/2021.acl-long.308},
  url       = {https://aclanthology.org/2021.acl-long.308}
}
  


Luo, Fuli, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, and Luo Si. 2021. "VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation." Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, August, 3980–3994. https://aclanthology.org/2021.acl-long.308.  










---
layout: post
title:  "[2020]ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT"
date:   2023-06-17 00:16:22 +0900
categories: study
---






{% highlight ruby %}


짧은 요약(Abstract) :    
* NLU 발전은 IR의 발전을 가져옴  
** PLM을 이용하여 쿼리와 다큐 사이의 관계를 잘 맺어줌, 더 효율적으로, 더 속도가 빠르게  
** 벡터 기반 index 사용하고 가지치기 친화적임  
** 효율/효과 성능 상승  


{% endhighlight %}  

<br/>


[Paper with my notes](https://drive.google.com/drive/folders/1yzBGKBpEYXEw_UnkVc-5iFVq66je428X?usp=sharing)  


[~~Lecture link~~]()  

<br/>

# 단어정리  
* held-out set: 데이터셋을 train set과 test set 두 세트로 나누는 과정을 의미  
* latency: 지연 시간(자극과 반응 사이 시간)    








<br/>

# 1 INTRODUCTIION    
* 기존 피처(쿼리?)->임베딩  
** 성능은 증대되지만 속도는 떨어짐  
** 근래에 효율성 올리려는 노력 늘어남  
** late interaction으로 쿼리 q와 다큐먼트 d 관계를 더 효율적이고 좋은 성능으로 잘 규명(잘 매핑)  
** 벡터 써서 cosim max 사용, 효율성 향상  
* 본 논문의 공헌  
** late interaction으로 효율성과 효과성 증대  
** 버트기반 쿼리, 다큐 인코더(late interaction paradigm 포함)  
** ColBERT로 re-rank(term 기반), search(벡터 기반) 구현  
** 좋은 성능 보임  

<br/>

# 2 RELATED WORK  
* 뉴럴매칭모델   
** NN -> rank  
** 커널 풀링  
** n-gram 매치  
** 단어 임베딩  
** q와 d 사이 관계 encode to signle  
** docu IR inverted index(sparse)  
* PLM for IR  
** 버트 PT, FT   
** 버트 경량화 노력 있음(정제, 압축, 가지치기)  
* NLU 기반 -> 생성형  

<br/>

# 3 COLBERT  
* ColBERT  
** 비용과 퀄리티의 균형 잡음  
** q와 d 관계를 late interaction으로 뉴럴 리랭킹 쌍으로 사용(사전계산)  
** 실질적 뉴럴 검색 지원  
** SOTA  
** q와 d의 late interaction은 PLM encoder에 집중  


## 3.1 Architecture  
* 구조: 쿼리 enc fQ, 다큐 enc fD, late interaction, bag of embedding Eq  
** q, d late interaction 사이에서 연관 스코어 MaxSim으로 구함, L2거리로 평가  
** 딥컨볼루션&어텐션 매칭으로 더 세련된 매칭  
** MaxSim 덕에 cost 줄어들었고 가지치기 가능  


## 3.2 Query & Document Encoders  
* 버트로 BOW같은 Bag of Embeddings 만듬  
** 쿼리 인코더->쿼리 워디 피스->CLS옆 배치, 문맥고려 계산  
** 마스크 추가로 쿼리 증강, 이게 중요함  
** Col버트 차원수는 줄어들고 공간 효율은 높아짐  
* 다큐 enc 쿼리 enc와 유사  
** 마스크 추가 증강은 없음  


## 3.3 Late Interaction  
* q와 d 사이 Sq,d(연관도)는 late interaction 사이의 BoE 사이에 MaxSim(cosim)으로 구함  
** 콜버트  


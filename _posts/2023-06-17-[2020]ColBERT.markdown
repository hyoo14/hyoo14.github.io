---
layout: post
title:  "[2020]ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT"
date:   2023-06-17 00:16:22 +0900
categories: study
---






{% highlight ruby %}


짧은 요약(Abstract) :    
* NLU 발전은 IR의 발전을 가져옴  
** PLM을 이용하여 쿼리와 다큐 사이의 관계를 잘 맺어줌, 더 효율적으로, 더 속도가 빠르게  
** 벡터 기반 index 사용하고 가지치기 친화적임  
** 효율/효과 성능 상승  


{% endhighlight %}  

<br/>


[Paper with my notes](https://drive.google.com/drive/folders/1yzBGKBpEYXEw_UnkVc-5iFVq66je428X?usp=sharing)  


[~~Lecture link~~]()  

<br/>

# 단어정리  
* held-out set: 데이터셋을 train set과 test set 두 세트로 나누는 과정을 의미  
* latency: 지연 시간(자극과 반응 사이 시간)    
* throughtput: 주어진 시간에 처리할 수 있는 정보 처리량  








<br/>

# 1 INTRODUCTIION    
* 기존 피처(쿼리?)->임베딩  
** 성능은 증대되지만 속도는 떨어짐  
** 근래에 효율성 올리려는 노력 늘어남  
** late interaction으로 쿼리 q와 다큐먼트 d 관계를 더 효율적이고 좋은 성능으로 잘 규명(잘 매핑)  
** 벡터 써서 cosim max 사용, 효율성 향상  
* 본 논문의 공헌  
** late interaction으로 효율성과 효과성 증대  
** 버트기반 쿼리, 다큐 인코더(late interaction paradigm 포함)  
** ColBERT로 re-rank(term 기반), search(벡터 기반) 구현  
** 좋은 성능 보임  

<br/>

# 2 RELATED WORK  
* 뉴럴매칭모델   
** NN -> rank  
** 커널 풀링  
** n-gram 매치  
** 단어 임베딩  
** q와 d 사이 관계 encode to signle  
** docu IR inverted index(sparse)  
* PLM for IR  
** 버트 PT, FT   
** 버트 경량화 노력 있음(정제, 압축, 가지치기)  
* NLU 기반 -> 생성형  

<br/>

# 3 COLBERT  
* ColBERT  
** 비용과 퀄리티의 균형 잡음  
** q와 d 관계를 late interaction으로 뉴럴 리랭킹 쌍으로 사용(사전계산)  
** 실질적 뉴럴 검색 지원  
** SOTA  
** q와 d의 late interaction은 PLM encoder에 집중  


## 3.1 Architecture  
* 구조: 쿼리 enc fQ, 다큐 enc fD, late interaction, bag of embedding Eq  
** q, d late interaction 사이에서 연관 스코어 MaxSim으로 구함, L2거리로 평가  
** 딥컨볼루션&어텐션 매칭으로 더 세련된 매칭  
** MaxSim 덕에 cost 줄어들었고 가지치기 가능  


## 3.2 Query & Document Encoders  
* 버트로 BOW같은 Bag of Embeddings 만듬  
** 쿼리 인코더->쿼리 워디 피스->CLS옆 배치, 문맥고려 계산  
** 마스크 추가로 쿼리 증강, 이게 중요함  
** Col버트 차원수는 줄어들고 공간 효율은 높아짐  
* 다큐 enc 쿼리 enc와 유사  
** 마스크 추가 증강은 없음  


## 3.3 Late Interaction  
* q와 d 사이 Sq,d(연관도)는 late interaction 사이의 BoE 사이에 MaxSim(cosim)으로 구함  
** 콜버트 분해가능 엔드투엔드, 버트 파인 튜닝, Adam Opti 사용  
** interaction은 학습 x, <q, d+, d-> 사용  
** 스코어는 문서마다 softmax CE로 계산   


## 3.4 Offline Indexing: Computing & Storing Document Embeddings  
* 콜버트 연산 분리 Q, D 사이    
** offline 연산 사용  
** indexing offline 최적  
** GPU 병렬 처리  
** 32/16 bit으로 저장  


## 3.5 Top-k Re-ranking with ColBERT  
* 콜버트 다른 text 기반 검색 리랭킹에 쓰임 가능  
** GPU서 내적연산  
** 뉴럴랭킹 보다 cost 낮고 scale 큼  


## 3.6 End-to-end Top-k Retrieval with ColBERT  
* ColBERT end2end 검색  
** MaxSim 가지치기 쉬움  
** vec sim 사용  
** Faiss 속도 빠른 프레임웍 사용  
** 쿼리 2단계 colBERT로 fit & refine  
** Faiss 구현위해서 IVFPQ(Invered file with product quantization) 사용  
** index 사용, K-means 기반 파티션  
** 공간 효율 증대  
<br/>

# 4 EXPERIMENTAL EVALUATION  
1. 리랭킹  
2. end2end  
3. 각 부분  
4. 인덱싱  


## 4.1 Methodology  
* 방법론  
** MS MARCO RANK  
*** 2016 RC  
*** 2018 Retrieval 8.8M  
*** MRP@10  
*** 3 query set  
*** submit re-rank  
*** heldout data(train, test 분리 안 된 set) for test  
** TREC CAR  
*** wiki 기반 29M  
*** 3M 쿼리 wiki page title 기반  
*** 2,254 쿼리  
* 구현  
** 파이썬3  
** PyTorch1  
** 버트 PLM  
** l.r 3*10^-6  
** 임베딩  
** 쿼리 32  
** batch 32  
** TREC 맞게 P-T & F-T   
** 파티션 p=2000   
* HW  
** V100(32gb)  
** Xeon Gold 5132CPU  
** 4 titan V 12gn gpu  


## 4.2 Quality-Cost Tradeoff: Top-k Re-ranking  
* 효율, 성능 tradeoff  
* ColBERT가 일반 BERT보다 rank 성능 올라감  
** 검색 cost 도 비쌈  


## 4.3 End-to-end Top-k Retrieval  
* end2end Top-k 검색  
** ColBERT 사용이 성능 올림  


## 4.4 Ablation Studies  
* MaxSim 기반 late interaction이 나음  


## 4.5 Indexing Throughput & Footprint  
* space foot pring는 emb dim 줄이고 공간 효율적으로 해줌  
<br/>

# 5 CONCLUSIONS  
* 컨텍스트 기반 late interaction deep LM 사용  
** 독립적 q,d 인코드 + 가지치기 쉬운 구조  
** 속도 올리고 성능도 올림  


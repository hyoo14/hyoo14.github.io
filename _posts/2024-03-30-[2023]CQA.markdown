---
layout: post
title:  "[2023]CQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"  
date:   2024-03-30 17:47:29 -0400
categories: study
---

{% highlight ruby %}


한줄 요약: 

짧은 요약(Abstract) :    
* 다중 쿼리 주의(MQA)는 단일 키-값 헤드만을 사용해 디코더 추론 속도를 크게 향상시킴  
* 그러나 MQA는 품질 저하의 원인이 될 수 있고 빠른 추론만을 위한 별도 모델 훈련은 바람직하지 않을 수 있음  
* 이에 저자들은 기존 다중 헤드 언어 모델 체크포인트를 MQA 사용 모델로 업트레이닝하는 방법을 제시함  
* 이 방법은 원래 사전 훈련 계산의 5%만 사용함  
* 저자들은 또한 다중 쿼리 주의의 일반화인 그룹화된 쿼리 주의(GQA)를 소개함  
* GQA는 쿼리 헤드 수보다 많지만 하나보다 적은 중간 수준의 키-값 헤드를 사용함  
* 업트레이닝된 GQA는 다중 헤드 주의와 비슷한 품질을 MQA와 비슷한 속도로 달성함

Useful sentences :  
*   


{% endhighlight %}  

<br/>

[Paper link](https://drive.google.com/drive/folders/1aMQ42pnDGYL_ltiZ8AHwS98A3kZxc7Zo?usp=drive_link)  
[~~Lecture link~~]()  

<br/>

# 단어정리  
* uptraining: 기존의 훈련된 모델을 추가로 훈련시켜서 새로운 기능이나 성능 향상을 도모하는 과정을 의미

<br/>
# 1 Introduction  
* 변환 모델 추론은 메모리 대역폭 오버헤드로 인해 심각한 병목 현상이 발생함  
* 이는 디코더 가중치와 모든 주의 키 및 값이 각 디코딩 단계마다 로딩되기 때문임  
* 메모리 대역폭에서 키와 값을 로딩하는 오버헤드는 다중 쿼리 주의를 통해 크게 줄일 수 있으며, 이는 여러 쿼리 헤드를 사용하지만 단일 키와 값 헤드만을 사용함  
* 그러나 다중 쿼리 주의(MQA)는 품질 저하와 훈련 불안정성을 초래할 수 있으며, 품질과 추론을 최적화하기 위한 별도 모델을 훈련하는 것이 실행 가능하지 않을 수 있음  
* 더욱이 일부 언어 모델은 이미 다중 쿼리 주의를 사용하지만 많은 모델은 그렇지 않음, 예를 들어 공개적으로 이용 가능한 언어 모델인 T5와 LLaMA가 그러함  
* 이 작업은 큰 언어 모델과 더 빠른 추론을 위한 두 가지 기여를 포함함  
* 첫째, 다중 헤드 주의(MHA)가 있는 언어 모델 체크포인트를 원래 훈련 계산의 소수 비율만을 사용하여 MQA를 사용하도록 업트레이닝할 수 있음을 보임  
* 이는 빠른 다중 쿼리와 고품질 MHA 체크포인트를 효과적으로 얻는 방법을 제시함  
* 둘째, 다중 헤드 주의와 다중 쿼리 주의 사이의 보간인 그룹화된 쿼리 주의(GQA)를 제안함  
* GQA는 쿼리 헤드의 하위 그룹마다 단일 키 및 값 헤드를 사용함  
* 업트레이닝된 GQA는 거의 다중 헤드 주의와 비슷한 품질을 달성하면서 다중 쿼리 주의와 비슷한 속도를 보임   

<br/>
# 2 Method  
*


<br/>
# 3 Experiments  
* 

<br/>
# 4 Related Work  
*  

<br/>  
# 5 Conclusion  
* 

<br/>
# 6 Limitations  
* 


<br/>  
# 요약  
* 
I"
<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="err">짧은</span> <span class="err">요약</span> <span class="p">:</span>

<span class="err">스킵그램이</span> <span class="err">이전</span> <span class="err">모델들</span> <span class="err">보다</span> <span class="n">syntatic</span> <span class="o">&amp;</span> <span class="n">semantic</span> <span class="err">단어</span> <span class="err">연관성</span> <span class="err">잘</span> <span class="err">잡아줌</span>

<span class="no">Hierarchical</span> <span class="no">Softmax</span> <span class="err">대체하는</span> <span class="no">Negative</span> <span class="no">Sampling</span> <span class="err">으로</span> <span class="err">학습속도와</span> <span class="err">단어임베딩의</span> <span class="err">퀄리티를</span> <span class="err">높였음</span>

<span class="err">관용구</span> <span class="err">통쨔ㅐ로</span> <span class="err">학습시켜서</span> <span class="err">기존</span> <span class="err">관용구</span> <span class="err">약점</span> <span class="err">좀</span> <span class="err">보완함</span></code></pre></figure>

<p>-그냥 소프트맥스는 계산이 너무 많음 (W)</p>

<p>-그래서 하이어라키컬 소프트 맥스 사용 (logW)</p>

<p>-하이어라키컬 소프트 맥스를 NCE 생각에 기반한 Negative Sampling(NEG)로 대체</p>

<p>여기서 NCE 생각은 “노이즈와 data를 잘 분리하려는 것” 으로 확률 분포를 알아야함
이 때 NCE, NEG ahen noise 분포로 unigram distribution 3/4power를 사용하면 가장 좋은 성능을 보였음(실험적으로)</p>

<p>Negative Sampling은 주변 단어들이 아닌 단어의 잡합을 만들어 negative 레이블링 하고 주변 단어들의 집합을 만들어 positive 레이블링하여 학습시키는 것으로
단어임베딩 학습을 (연산적으로 좀 더 효율적이게) 이진 분류 문제 학습으로 변환한 것
결과적으로 성능적으로도 더 좋았음</p>

:ET
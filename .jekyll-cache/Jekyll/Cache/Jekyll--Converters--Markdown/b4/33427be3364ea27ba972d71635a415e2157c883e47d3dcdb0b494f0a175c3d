I"ô<p>CROSS ENTROPY</p>

<p>í˜„ì¬ëŠ” ë°±ì—”ë“œ ê°œë°œì´ ë©”ì¸ ì—…ë¬´ì´ì§€ë§Œ ì „ê³µë„ ë³µê¸°í•´ë‘ì–´ì•¼ í•œë‹¤ëŠ” ìƒê°ì— nlpê´€ë ¨ ê¸€ë„ í¬ìŠ¤íŒ…í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.<br />
ê·¸ë˜ì„œ ì´ë ‡ê²Œ ëœ¬ê¸ˆì—†ê²Œ cross entropyì— ëŒ€í•œ í¬ìŠ¤íŠ¸ë¥¼ ì˜¬ë¦¬ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>

<p>This post introduces concepts about cross entropy.<br />
In addition, the post effectively added the points perplexity is similar concepts.</p>

<p>To begin with, cross entropy is the metric for two different distributions.<br />
It is because it compare how two distribution is similar.</p>

<p>Second, cross entropy is expectation value of information value from distribution Q where distribution P have seen.<br />
Perplexity is root value of log probability.</p>

:ET
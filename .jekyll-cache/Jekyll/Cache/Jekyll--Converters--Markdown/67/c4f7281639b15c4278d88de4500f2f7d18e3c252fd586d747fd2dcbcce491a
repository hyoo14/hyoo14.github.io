I"¢
<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="err">ì§§ì€</span> <span class="err">ìš”ì•½</span> <span class="p">:</span>

<span class="err">ìœ ëª…í•œ</span> <span class="err">ë²„íŠ¸</span> <span class="err">ì†Œê°œ</span><span class="o">.</span>
<span class="err">ì§„ì •í•œ</span> <span class="n">bidirectional</span> <span class="n">representations</span> <span class="n">pretraining</span>
<span class="n">fine</span> <span class="n">tuning</span><span class="err">ìœ¼ë¡œ</span> <span class="no">SOTA</span> <span class="err">ì°ìŒ</span>
<span class="p">(</span><span class="no">QA</span><span class="p">,</span> <span class="no">LI</span> <span class="err">ë“±</span><span class="p">)</span></code></pre></figure>

<p>-pretrained LMì€ 2ê°€ì§€ê°€ ìˆìŒ
1.feature-based (ELMO ì²˜ëŸ¼ ì¶”ê°€ íŠ¹ì§• í•™ìŠµ)
2.fine-tuning (GPTì²˜ëŸ¼ ëª¨ë“  íŒŒë¼ë¯¸í„° ë¯¸ì„¸ì¡°ì •)</p>

<p>-ê·¼ë° ë‹¤ í•œë°©í–¥ì„ LR. ELMOê°€ ì–‘ë°©í–¥ì´ë‚˜ LR, RL ë”°ë¡œ í•™ìŠµ í›„ concatí•œ ê²ƒì— ë¶ˆê³¼.
-MLM(Masked Language Model) í†µí•´ BERTëŠ” ì–‘ë°©í–¥ì„± íšë“.
-NSP(Next Sentence Prediction) í†µí•´ ë¬¸ì¥ ê°„ì˜ ê´€ê³„ë„ í•™ìŠµ(QAì™€ NLIì— íš¨ê³¼ì )
-ì—°ì†ëœ ë¬¸ì¥ í•œ ë¬¶ìŒìœ¼ë¡œ inputì— ë„£ìŒ
-[cls] í† í°ìœ¼ë¡œ classification ìš©ìœ¼ë¡œ ì‚¬ìš©
-[SEP] í† í°ìœ¼ë¡œ ë‘ ë¬¸ì¥ êµ¬ë¶„
-ì„ë² ë”© ë ˆì´ì–´ ì¶”ê°€í•˜ì—¬ ë‘ ë¬¸ì¥ ë‹¬ë¦¬ ë´„. (ë¬¸ì¥ 1ì€ AAA 2ëŠ” BBB ì´ëŸ° ì‹)
-[MASK]ë¥¼ ì „ì²´ì˜ 15% ê³¨ë¼ì„œ ì´ ì¤‘ 80%ì— ì”Œì›Œì„œ MLM í•™ìŠµí•˜ëŠ” ê²ƒ. ë¹„ìœ¨ ì¡°ì •ì€ [MASK] ë•œì— pretrainingê³¼ fine tuning ì‚¬ì´ ê´´ë¦¬ ìƒê¸°ëŠ” ê±° ì¡°ê¸ˆ ì¤„ì—¬ë³´ë ¤ê³  í•œ ê²ƒ.
-corss entropy lossë¡œ í•™ìŠµ
-fine tuningì€ ê¸°ì¡´ì˜ encode í›„ bidirectional cross attention ê³¼ì •ì„ í•œë°©ìœ¼ë¡œ í•©ì¹œ ì…ˆìœ¼ë¡œ, encode with self-attention.
-BERTê°€ GLUE(General Language Understanding Evaluation), SQuAD(Stanford Question Answering Dataset), SWAG(Situations With Adversarial Generations) ë“±ì—ì„œ SOTAì„</p>

:ET
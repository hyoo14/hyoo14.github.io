I"	
<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="err">짧은</span> <span class="err">요약</span> <span class="p">:</span>

<span class="n">labeling</span><span class="err">을</span> <span class="err">위한</span> <span class="n">sequence</span> <span class="err">뿐만</span> <span class="err">아니라</span>  
<span class="err">검색을</span> <span class="err">통한</span> <span class="n">external</span> <span class="n">text</span><span class="err">를</span> <span class="err">같이</span> <span class="n">input</span><span class="err">으로</span> <span class="err">사용</span>  
<span class="no">CL</span><span class="p">(</span><span class="no">Collaborative</span> <span class="no">Learning</span><span class="p">)</span><span class="err">으로</span> <span class="err">성능을</span> <span class="err">향상</span>  
<span class="o">-&gt;</span><span class="n">external</span> <span class="n">text</span><span class="err">가</span> <span class="err">없을</span> <span class="err">때의</span> <span class="err">성능도</span> <span class="err">향상</span>  </code></pre></figure>

<p>*문장 포함 문서가 있으면 당연히 NER 성능이 향상<br />
**하지만 문장 찾기가 쉽진 않음<br />
**트위터 등 소셜미디어나 이커머스 등서 찾기 힘듬<br />
**그래서 서치엔진으로 찾음</p>

<p><em>검색 결과를 Re-Rank 해줌<br />
**의미적 유사도로 Re-Rank!<br />
**</em>BERTScore 씀</p>

<p>*모델의 흐름?<br />
** (input sentence + external contexts) -&gt; pretrained embedding -&gt; CRF -&gt; output</p>

<p><em>CL?<br />
**input 2개로 학습<br />
**L2 distance 거리 작게 해서 2 인풋에 대한 각각의 아웃풋 차이가 적게끔 해줌<br />
**KL divergence 를 통해 아웃풋 분포 차이 적게 해줌<br />
**</em>궁극적으로 external texts가 없을 때의 모델의 성능도 향상<br />
** 참고로, CL 안 쓸 경우 외부 context 길고 잠재의미 사전에 몰라서 매우 느림.</p>

<p>*negative log-likelihood 줄이게 학습 진행<br />
*CRF서 loss function 어려워서 KL씀<br />
*backpropagate 안 하므로 KL이 cross-entropy와 같음</p>

<p>*Re-Rankg 때 Roberta-Large 씀<br />
*바이오는 Bio-BERT, 다른 도메인은 XLM-RoBERTa</p>

<p>*AdanW 옵티마이저 씀</p>

<p>**여러 비교군들과 실험해본 결과 8개 중 5개에서 SOTA 찍음</p>

:ET